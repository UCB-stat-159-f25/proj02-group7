{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eda7c73d-adac-4a73-a03a-2e25cfe903e3",
   "metadata": {},
   "source": [
    "## Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)\n",
    "\n",
    "Now we will start working on simple text processing using the `SpaCy` package and the same dataset as Part 1. The package should already be included in the `environment.yml`. However, we will also need to download `en_core_web_sm`, an English language text processing model. To do this, while having your `sotu` environment activated, run the following:\n",
    "\n",
    "```\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "Now, you should be good to go!\n",
    "\n",
    "Some important definitions:\n",
    "\n",
    "- *Token*: a single word or piece of a word\n",
    "- *Lemma*: the core component of a word, e.g., \"complete\" is the lemma for \"completed\" and \"completely\"\n",
    "- *Stop Word*: a common word that does not add semantic value, such as \"a\", \"and\", \"the\", etc.\n",
    "- *Vectorization*: representing a document as a vector where each index in the vector corresponds to a token or word and each entry is the count.\n",
    "\n",
    "In this section, we will explore the most common tokens and lemmas throughout different slices of the speech data. We will also develop vectorization representations of the speeches. \n",
    "\n",
    " The core steps are:\n",
    "\n",
    "1. Process speeches using the SpaCy nlp module\n",
    "2. Analyze Tokens vs Lemmas:\n",
    "- Create a list of all tokens across all speeches that are not stop words, punctuation, or spaces.\n",
    "- Create a second list of the lemmas for these same tokens.\n",
    "- Display the top 25 for each of these and compare.\n",
    "3. Analyze common word distributions over different years:\n",
    "- Create a function that takes the dataset and a year as an input and outputs the top n lemmas for that year's speeches\n",
    "- Compare the top 10 words for 2023 versus 2019\n",
    "4. Document Vectorization:\n",
    "- Train a Term Frequency-Inverse Document Frequency (TF-IDF) vectorization model using your processed dataset and scikit learn\n",
    "- Output the feature vectors \n",
    "\n",
    "**Helpful Resources:**\n",
    "- https://realpython.com/natural-language-processing-spacy-python/\n",
    "- https://www.statology.org/text-preprocessing-feature-engineering-spacy/ \n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html# \n",
    "- https://www.geeksforgeeks.org/nlp/how-to-store-a-tfidfvectorizer-for-future-use-in-scikit-learn/ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947edaf0-1597-47f4-a0c6-9f217fcdb0fd",
   "metadata": {},
   "source": [
    "### Processing Speeches with SpaCy\n",
    "\n",
    "Lets study just speeches from 2000 and onwards to begin with. So, be sure to subset your DataFrame to just these speeches before continuing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3067b26-501d-466f-a802-c1095c8988e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /srv/conda/envs/notebook/lib/python3.12/site-packages (3.8.11)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (0.20.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.12.3)\n",
      "Requirement already satisfied: jinja2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (70.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (24.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /srv/conda/envs/notebook/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (1.17.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47d5ab9d-c174-4d37-a986-e9c73546089b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sou = pd.read_csv('data/SOTU.csv')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da2bdf9f-d9f3-45e7-bc43-2a019e1b0ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>President</th>\n",
       "      <th>Year</th>\n",
       "      <th>Text</th>\n",
       "      <th>Word Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joseph R. Biden</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>\\n[Before speaking, the President presented hi...</td>\n",
       "      <td>8003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joseph R. Biden</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>\\nThe President. Mr. Speaker——\\n[At this point...</td>\n",
       "      <td>8978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Joseph R. Biden</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>\\nThe President. Thank you all very, very much...</td>\n",
       "      <td>7539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Joseph R. Biden</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>\\nThe President. Thank you. Thank you. Thank y...</td>\n",
       "      <td>7734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>\\nThe President. Thank you very much. Thank yo...</td>\n",
       "      <td>6169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>1791.0</td>\n",
       "      <td>\\nFellow-Citizens of the Senate and House of R...</td>\n",
       "      <td>2264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>1790.0</td>\n",
       "      <td>\\nFellow-Citizens of the Senate and House of R...</td>\n",
       "      <td>1069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>1790.0</td>\n",
       "      <td>\\nFellow-Citizens of the Senate and House of R...</td>\n",
       "      <td>1069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>1790.0</td>\n",
       "      <td>\\nFellow-Citizens of the Senate and House of R...</td>\n",
       "      <td>1069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>1790.0</td>\n",
       "      <td>\\nFellow-Citizens of the Senate and House of R...</td>\n",
       "      <td>1069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>246 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             President    Year  \\\n",
       "0      Joseph R. Biden  2024.0   \n",
       "1      Joseph R. Biden  2023.0   \n",
       "2      Joseph R. Biden  2022.0   \n",
       "3      Joseph R. Biden  2021.0   \n",
       "4      Donald J. Trump  2020.0   \n",
       "..                 ...     ...   \n",
       "241  George Washington  1791.0   \n",
       "242  George Washington  1790.0   \n",
       "243  George Washington  1790.0   \n",
       "244  George Washington  1790.0   \n",
       "245  George Washington  1790.0   \n",
       "\n",
       "                                                  Text  Word Count  \n",
       "0    \\n[Before speaking, the President presented hi...        8003  \n",
       "1    \\nThe President. Mr. Speaker——\\n[At this point...        8978  \n",
       "2    \\nThe President. Thank you all very, very much...        7539  \n",
       "3    \\nThe President. Thank you. Thank you. Thank y...        7734  \n",
       "4    \\nThe President. Thank you very much. Thank yo...        6169  \n",
       "..                                                 ...         ...  \n",
       "241  \\nFellow-Citizens of the Senate and House of R...        2264  \n",
       "242  \\nFellow-Citizens of the Senate and House of R...        1069  \n",
       "243  \\nFellow-Citizens of the Senate and House of R...        1069  \n",
       "244  \\nFellow-Citizens of the Senate and House of R...        1069  \n",
       "245  \\nFellow-Citizens of the Senate and House of R...        1069  \n",
       "\n",
       "[246 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e55eb261-c1e8-4120-9a20-ae34b7630e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>President</th>\n",
       "      <th>Year</th>\n",
       "      <th>Text</th>\n",
       "      <th>Word Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joseph R. Biden</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>\\n[Before speaking, the President presented hi...</td>\n",
       "      <td>8003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joseph R. Biden</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>\\nThe President. Mr. Speaker——\\n[At this point...</td>\n",
       "      <td>8978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Joseph R. Biden</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>\\nThe President. Thank you all very, very much...</td>\n",
       "      <td>7539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Joseph R. Biden</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>\\nThe President. Thank you. Thank you. Thank y...</td>\n",
       "      <td>7734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>\\nThe President. Thank you very much. Thank yo...</td>\n",
       "      <td>6169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>\\nThe President. Madam Speaker, Mr. Vice Presi...</td>\n",
       "      <td>5519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>\\nThe President. Mr. Speaker, Mr. Vice Preside...</td>\n",
       "      <td>5755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>\\nThank you very much. Mr. Speaker, Mr. Vice P...</td>\n",
       "      <td>4903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>\\nThank you. Mr. Speaker, Mr. Vice President, ...</td>\n",
       "      <td>5956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>\\nThe President. Mr. Speaker, Mr. Vice Preside...</td>\n",
       "      <td>6659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>\\nThe President. Mr. Speaker, Mr. Vice Preside...</td>\n",
       "      <td>6904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>\\nPlease, everybody, have a seat. Mr. Speaker,...</td>\n",
       "      <td>6692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>\\nMr. Speaker, Mr. Vice President, Members of ...</td>\n",
       "      <td>6915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>\\nMr. Speaker, Mr. Vice President, Members of ...</td>\n",
       "      <td>6758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>\\nMadam Speaker, Vice President Biden, Members...</td>\n",
       "      <td>7129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>\\nMadam Speaker, Mr. Vice President, Members o...</td>\n",
       "      <td>5979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>George W. Bush</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>\\nThank you all. Madam Speaker, Vice President...</td>\n",
       "      <td>5635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>George W. Bush</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>\\nThank you very much. And tonight I have the ...</td>\n",
       "      <td>5491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>George W. Bush</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>\\nThank you all. Mr. Speaker, Vice President C...</td>\n",
       "      <td>5218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>George W. Bush</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>\\nMr. Speaker, Vice President Cheney, Members ...</td>\n",
       "      <td>4987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>George W. Bush</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>\\nMr. Speaker, Vice President Cheney, Members ...</td>\n",
       "      <td>5100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>George W. Bush</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>\\nMr. Speaker, Vice President Cheney, Members ...</td>\n",
       "      <td>5288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>George W. Bush</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>\\nThank you very much. Mr. Speaker, Vice Presi...</td>\n",
       "      <td>3762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>George W. Bush</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>\\nMr. Speaker, Mr. Vice President, Members of ...</td>\n",
       "      <td>4293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>William J. Clinton</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>\\nMr. Speaker, Mr. Vice President, Members of ...</td>\n",
       "      <td>8925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             President    Year  \\\n",
       "0      Joseph R. Biden  2024.0   \n",
       "1      Joseph R. Biden  2023.0   \n",
       "2      Joseph R. Biden  2022.0   \n",
       "3      Joseph R. Biden  2021.0   \n",
       "4      Donald J. Trump  2020.0   \n",
       "5      Donald J. Trump  2019.0   \n",
       "6      Donald J. Trump  2018.0   \n",
       "7      Donald J. Trump  2017.0   \n",
       "8         Barack Obama  2016.0   \n",
       "9         Barack Obama  2015.0   \n",
       "10        Barack Obama  2014.0   \n",
       "11        Barack Obama  2013.0   \n",
       "12        Barack Obama  2012.0   \n",
       "13        Barack Obama  2011.0   \n",
       "14        Barack Obama  2010.0   \n",
       "15        Barack Obama  2009.0   \n",
       "16      George W. Bush  2008.0   \n",
       "17      George W. Bush  2007.0   \n",
       "18      George W. Bush  2006.0   \n",
       "19      George W. Bush  2005.0   \n",
       "20      George W. Bush  2004.0   \n",
       "21      George W. Bush  2003.0   \n",
       "22      George W. Bush  2002.0   \n",
       "23      George W. Bush  2001.0   \n",
       "24  William J. Clinton  2000.0   \n",
       "\n",
       "                                                 Text  Word Count  \n",
       "0   \\n[Before speaking, the President presented hi...        8003  \n",
       "1   \\nThe President. Mr. Speaker——\\n[At this point...        8978  \n",
       "2   \\nThe President. Thank you all very, very much...        7539  \n",
       "3   \\nThe President. Thank you. Thank you. Thank y...        7734  \n",
       "4   \\nThe President. Thank you very much. Thank yo...        6169  \n",
       "5   \\nThe President. Madam Speaker, Mr. Vice Presi...        5519  \n",
       "6   \\nThe President. Mr. Speaker, Mr. Vice Preside...        5755  \n",
       "7   \\nThank you very much. Mr. Speaker, Mr. Vice P...        4903  \n",
       "8   \\nThank you. Mr. Speaker, Mr. Vice President, ...        5956  \n",
       "9   \\nThe President. Mr. Speaker, Mr. Vice Preside...        6659  \n",
       "10  \\nThe President. Mr. Speaker, Mr. Vice Preside...        6904  \n",
       "11  \\nPlease, everybody, have a seat. Mr. Speaker,...        6692  \n",
       "12  \\nMr. Speaker, Mr. Vice President, Members of ...        6915  \n",
       "13  \\nMr. Speaker, Mr. Vice President, Members of ...        6758  \n",
       "14  \\nMadam Speaker, Vice President Biden, Members...        7129  \n",
       "15  \\nMadam Speaker, Mr. Vice President, Members o...        5979  \n",
       "16  \\nThank you all. Madam Speaker, Vice President...        5635  \n",
       "17  \\nThank you very much. And tonight I have the ...        5491  \n",
       "18  \\nThank you all. Mr. Speaker, Vice President C...        5218  \n",
       "19  \\nMr. Speaker, Vice President Cheney, Members ...        4987  \n",
       "20  \\nMr. Speaker, Vice President Cheney, Members ...        5100  \n",
       "21  \\nMr. Speaker, Vice President Cheney, Members ...        5288  \n",
       "22  \\nThank you very much. Mr. Speaker, Vice Presi...        3762  \n",
       "23  \\nMr. Speaker, Mr. Vice President, Members of ...        4293  \n",
       "24  \\nMr. Speaker, Mr. Vice President, Members of ...        8925  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subset the speech dataframe for speeches from 2000 and onwards\n",
    "sou_new = sou[sou['Year'] >= 2000]\n",
    "sou_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b63b5f3-2957-48f8-8684-5249fe59a5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each speeches using the 'nlp' function\n",
    "processed_speeches = []\n",
    "\n",
    "for i in range(len(sou_new)):\n",
    "    speech_text = sou_new.loc[i, 'Text'] \n",
    "    processed = nlp(speech_text)    \n",
    "    processed_speeches.append(processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734ee6ca-0f96-45ac-8220-02853a47aafb",
   "metadata": {},
   "source": [
    "### Analyze Tokens vs Lemmas\n",
    "\n",
    "#### Token List\n",
    "\n",
    "Create a list of tokens across all speeches that are not spaces, stopwords, or punctuation. Make each token lowercase as well. *Hint: each element of the list we just created are themselves lists of tokens. Token objects have attributes `is_stop`, `is_punct`, and `is_space`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77731edc-40fa-4b46-94c3-947a4ed41382",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "\n",
    "for speech in processed_speeches:\n",
    "    for token in speech:\n",
    "        if not token.is_punct and not token.is_space and not token.is_stop:\n",
    "            tokens.append(token.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff33e97b-e894-4bb1-8604-2b136c2badc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('America', 816), ('people', 622), ('American', 582), ('new', 495), ('years', 439), ('Americans', 437), ('world', 409), ('year', 401), ('country', 369), ('jobs', 325), ('work', 324), ('know', 323), ('Congress', 317), ('time', 297), ('help', 278), ('need', 266), ('tonight', 253), ('President', 246), ('economy', 243), ('want', 237)]\n"
     ]
    }
   ],
   "source": [
    "# print top 20 tokens\n",
    "# Hint - use Counter, and one of the Counter object's methods to display the top 20\n",
    "token_count = Counter(tokens).most_common(20)\n",
    "print(token_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cccfb41-0af1-4dce-91ae-66b06f57bc67",
   "metadata": {},
   "source": [
    "#### Lemma List\n",
    "\n",
    "Do the same as above, but for lemmas. *Hint: recall lemmas are components of words. Each token should have an attribute to extract the lemma.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f8f21e-a099-4777-97b7-44d9c0c6bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = []\n",
    "\n",
    "for token in tokens:\n",
    "    word = nlp(token) # run the tokens through nlp to make sure it has the lemma_ attribute\n",
    "    for token in word:\n",
    "        lemma=token.lemma_\n",
    "        lemmas.append(lemma.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa81c91-9ac3-4319-a520-9991ae1a5cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print top 20 lemmas\n",
    "lemma_counts = Counter(lemmas).most_common(20)\n",
    "lemma_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbca5ab-c5d8-4dff-bdd2-31b198f4d191",
   "metadata": {},
   "source": [
    "#### Token versus Lemma Comparison\n",
    "\n",
    "What do you notice about the top tokens versus the top lemmas? \n",
    "Consider two tokens - \"year\" and \"years\" - how do their counts compare to the lemma \"year\"?\n",
    "What about the lemma \"child\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd47cc5f-92d4-4d3e-80a9-5d0a3d648420",
   "metadata": {},
   "source": [
    "### Common Words\n",
    "\n",
    "#### Common Words per Year Function\n",
    "\n",
    "Fill in the below function to obtain the n-most common words in speeches for a given year.\n",
    "\n",
    "inputs: \n",
    "- df raw unprocessed sou dataframe\n",
    "- year\n",
    "- n\n",
    "outputs: \n",
    "- top n words for that years\n",
    "\n",
    "steps:\n",
    "- subset the dataframe for the year of interest - note the years might not be in int type\n",
    "- process the subsetted dataframe with spacy\n",
    "- get the lemmas across all those speeches\n",
    "- count the top n lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885c316e-e120-4921-8eff-b654289db291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common_words(df, year, n=25):\n",
    "    \"\"\"\n",
    "    Processes the SOTU speech for a given year and returns\n",
    "    the most common non-stopword/punctuation lemmas.\n",
    "\n",
    "    INPUTS\n",
    "    - raw unprocessed sou data frame\n",
    "    - year\n",
    "\n",
    "    OUTPUTS\n",
    "    - top n words for that year\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Subset df\n",
    "    new_df = df[df['Year'] == year]\n",
    "    \n",
    "    # Step 2: Process the text with spaCy\n",
    "    new_df = df[df['Year'] == year].reset_index(drop=True) \n",
    "    processed_speeches = []\n",
    "\n",
    "    for i in range(len(new_df)):\n",
    "        speech_text = new_df.loc[i, 'Text'] \n",
    "        processed = nlp(speech_text)    \n",
    "        processed_speeches.append(processed)\n",
    "    \n",
    "    # Step 3: Get lemmas\n",
    "    lemmas = []\n",
    "\n",
    "    for doc in processed_speeches:\n",
    "        for token in doc:\n",
    "            if not token.is_punct and not token.is_space and not token.is_stop:\n",
    "                lemmas.append(token.lemma_.lower())\n",
    "    \n",
    "    lemma_counts = Counter(lemmas).most_common(n)\n",
    "    \n",
    "    return lemma_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd41316b-059d-4955-81e6-b71b87891321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it on 2024\n",
    "get_most_common_words(sou, 2024, n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922adcde-e34d-4879-bde4-0a1d39a73b5b",
   "metadata": {},
   "source": [
    "#### Compare 2023 to 2017\n",
    "\n",
    "Run your function from the previous step to get the top 20 words for 2017 and 2023. Plot the words and their frequencies in a barchart and replicate the figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee121d6-7f7e-4173-96f5-c31ac9f90003",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_2023 = get_most_common_words(sou, 2023, n=20)\n",
    "words_2017 = get_most_common_words(sou, 2017, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e8936a-a5a7-4d5a-8d84-98294cd5798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a651d464-9279-4a29-ac7e-a4cfc7a0c2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8736be57-32a6-4401-8562-8f6a8a0a3614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint - put the words and counts into a pd Dataframe for better structure\n",
    "# and to make plotting easier\n",
    "df_2017 = pd.DataFrame(words_2017, columns=['lemma', 'count'])\n",
    "df_2023 = pd.DataFrame(words_2023, columns=['lemma', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65094b4a-676c-4ab9-be34-822472acfc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(12, 10)) \n",
    "# Plot 2017\n",
    "sns.barplot(\n",
    "    x = 'lemma',\n",
    "    y = 'count',\n",
    "    data = df_2017,\n",
    "    ax = axes[0],\n",
    "     color='#2C7BA1'\n",
    "    \n",
    ")\n",
    "axes[0].set_title('2017 State of the Union Most Frequent Words')\n",
    "axes[0].set_xlabel('Word')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45) \n",
    "\n",
    "# Plot 2023\n",
    "sns.barplot(\n",
    "    x='lemma',\n",
    "    y='count',\n",
    "    data=df_2023,\n",
    "    ax=axes[1],\n",
    "    color='#2C7BA1'\n",
    ")\n",
    "\n",
    "axes[1].set_title('2023 State of the Union Most Frequent Words')\n",
    "axes[1].set_xlabel('Word')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#saving figure\n",
    "plt.savefig('outputs/sotu_word_freq2017_2023.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90971782-59d0-4247-8582-41214f950231",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization\n",
    "\n",
    "To use statsitical alorithms on documents, we need to transform them into vectors, where each element of the vector corresponds to a particular word in a document or corpus of documents. One common way is via TF-IDF embeddings. LLMs work similarly - they typically use transformer models to generate text embeddings before sending text through a deep neural network. \n",
    "\n",
    "Here we will fit a TF-IDF vectorizer, plot all the speeches on a 2-D grid using PCA and also using a heatmap, and examine TF-IDF scores for the top 10 most common words in the first speech. This is a good resource here: https://medium.com/GeoffreyGordonAshbrook/vector-visualization-2d-plot-your-tf-idf-with-pca-83fa9fccb1d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b54c198-0b90-418d-b645-53262c11ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install nlk to current kernel only once, can do from terminal \n",
    "#(has been added to environment.yml too so if using this, do not have to install in notebook)\n",
    "\n",
    "import sys, subprocess\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nltk\"])\n",
    "\n",
    "# confirm the version that was just installed\n",
    "import nltk\n",
    "print(\"NLTK version:\", nltk.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef170d48-ea7e-4614-98af-bdb9a740f5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.tokenize.casual import casual_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5021cae-0e33-42da-87db-463cc1d9d3ae",
   "metadata": {},
   "source": [
    "#### Train the Vectorizer and Transform the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f851cf8a-760a-4839-ba79-9e3409a7021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may use this as input to fit the TF-IDF vectorizer\n",
    "raw_docs = sou[\"Text\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386b1757-b9b6-43e4-8fe9-17680885f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Minimal TF-IDF Vectors (Reference: Vector Visualization by Ashbrook)\n",
    "##########################\n",
    "\n",
    "# Select Model\n",
    "tfidf_model = TfidfVectorizer()\n",
    "\n",
    "# Fit Model\n",
    "tfidf_vectors = tfidf_model.fit_transform(raw_docs).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25946e30-ea2b-4f9b-96d1-b2768985d0c5",
   "metadata": {},
   "source": [
    "The output of `fit_transform()` will be a matrix where each row corresponds to a speech, each column corresponds to a word in the corpus of speeches, and the value is the TF-IDF score which measures the importance of that word in that speech, relative to the rest of the speeches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f906cdd-2468-4a3f-87a0-6c2d05762673",
   "metadata": {},
   "source": [
    "#### Plot Speeches\n",
    "\n",
    "- First used PCA to generate the first chart\n",
    "- Second use seaborn heatmap with a log-scaled color axis to generate the second chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e623bc7-514b-4e9a-ac27-07d92f116f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "# Step 1: Set PCA to find first 2 principal components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Step 2: Create a new dataframe where each row is a speech, and each column is a projection onto\n",
    "# one of the two principal components\n",
    "speeches_array = tfidf_vectors\n",
    "speeches_df2d = pd.DataFrame(pca.fit_transform(speeches_array), columns=list('xy'))\n",
    "\n",
    "# Plot Data Visualization (Matplotlib)\n",
    "\n",
    "# Plot (matplotlib only)\n",
    "plt.style.use('seaborn-v0_8')           # just a style; still matplotlibplt.figure(figsize=(6,4))\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(speeches_df2d['x'], speeches_df2d['y'], s=20)\n",
    "plt.xlabel('Principle Component 1'); plt.ylabel('Principle Component 2'); plt.title('Plot of Vectorized Speeches Principle Components')\n",
    "plt.grid(False)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a009a3b0-7784-475c-be72-157206e40d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
    "\n",
    "# vectorized_docs: scipy sparse matrix (rows=speeches, cols=tokens)\n",
    "A = tfidf_vectors.astype(np.float32)   # densify\n",
    "\n",
    "# Log scale needs vmin > 0; pick the smallest positive value\n",
    "pos_min = A[A > 0].min() if (A > 0).any() else 1e-6\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.figure(figsize=(5.8, 4.7))\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    A,\n",
    "    cmap=\"magma\",\n",
    "    norm=LogNorm(vmin=pos_min, vmax=A.max()),\n",
    "    cbar_kws={\"format\": mticker.LogFormatterMathtext()}\n",
    ")\n",
    "\n",
    "ax.set_title(\"Vectorized Speeches\", pad=8)\n",
    "ax.set_xlabel(\"Vector Index\")\n",
    "ax.set_ylabel(\"Speech Index\")\n",
    "\n",
    "n_cols = A.shape[1]\n",
    "step = 928         \n",
    "xticks = np.arange(0, n_cols, step)\n",
    "\n",
    "ax.xaxis.set_major_locator(FixedLocator(xticks))\n",
    "ax.xaxis.set_major_formatter(FixedFormatter([str(v) for v in xticks]))\n",
    "plt.setp(ax.get_xticklabels(), rotation=90)\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f9b735-3f9c-4bf3-a465-d623ff93d8ee",
   "metadata": {},
   "source": [
    "#### Get the TF-IDF value for certain words and documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261609d7-5f16-4ffd-b99c-2430d002e4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['year',\n",
    " 'america',\n",
    " 'people',\n",
    " 'american',\n",
    " 'work',\n",
    " 'new',\n",
    " 'job',\n",
    " 'country',\n",
    " 'americans',\n",
    " 'world'] # top ten most common words through whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd457fde-02a2-41f3-b274-f9bb39e449b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_nums = [tfidf_model.vocabulary_[w] for w in word_list] # get each word's index number using the .vocabular_ attributed of vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfa3b4f-ecd3-4458-89b9-06a1a0e97643",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_score = tfidf_model.idf_[word_nums] # get their IDF score by using .idf_ at the indices from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84317ea8-dd12-4ae1-bad9-2b81e7411b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = tfidf_vectors[0, word_nums] # get the tf_idf score for the first speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39876e83-8b92-4705-b16e-b6a6af874a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Word\": word_list, \"IDF Score\": idf_score, \"TF-IDF Score\": tf_idf})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
