<!DOCTYPE html><html lang="en" class="" style="scroll-padding:60px"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width,initial-scale=1"/><title>Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts) - Stat159 Project 2 - Reproducibility in Natural Language Processing</title><meta property="og:title" content="Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts) - Stat159 Project 2 - Reproducibility in Natural Language Processing"/><meta name="generator" content="mystmd"/><meta name="keywords" content=""/><meta name="image" content="/user/jcollins36855/myst-build/proj02-group7/build/b77199e99a54e59b2e3c037c2cc90f21.svg"/><meta property="og:image" content="/user/jcollins36855/myst-build/proj02-group7/build/b77199e99a54e59b2e3c037c2cc90f21.svg"/><link rel="stylesheet" href="/user/jcollins36855/myst-build/proj02-group7/build/_assets/app-IZWEOBHI.css"/><link rel="stylesheet" href="/user/jcollins36855/myst-build/proj02-group7/build/_assets/thebe-core-VKVHG5VY.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jupyter-matplotlib@0.11.3/css/mpl_widget.css"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous"/><link rel="icon" href="/user/jcollins36855/myst-build/proj02-group7/favicon.ico"/><link rel="stylesheet" href="/user/jcollins36855/myst-build/proj02-group7/myst-theme.css"/><script>
  const savedTheme = localStorage.getItem("myst:theme");
  const theme = window.matchMedia("(prefers-color-scheme: light)").matches ? 'light' : 'dark';
  const classes = document.documentElement.classList;
  const hasAnyTheme = classes.contains('light') || classes.contains('dark');
  if (!hasAnyTheme) classes.add(savedTheme ?? theme);
</script></head><body class="m-0 transition-colors duration-500 bg-white dark:bg-stone-900"><div class="fixed top-1 left-1 h-[0px] w-[0px] focus-within:z-40 focus-within:h-auto focus-within:w-auto bg-white overflow-hidden focus-within:p-2 focus-within:ring-1" aria-label="skip to content options"><a href="#skip-to-frontmatter" class="block px-2 py-1 text-black underline">Skip to article frontmatter</a><a href="#skip-to-article" class="block px-2 py-1 text-black underline">Skip to article content</a></div><div class="bg-white/80 backdrop-blur dark:bg-stone-900/80 shadow dark:shadow-stone-700 p-3 md:px-8 sticky w-screen top-0 z-30 h-[60px]"><nav class="flex items-center justify-between flex-nowrap max-w-[1440px] mx-auto"><div class="flex flex-row xl:min-w-[19.5rem] mr-2 sm:mr-7 justify-start items-center shrink-0"><div class="block xl:hidden"><button class="flex items-center border-stone-400 text-stone-800 hover:text-stone-900 dark:text-stone-200 hover:dark:text-stone-100"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" width="2rem" height="2rem" class="m-1"><path fill-rule="evenodd" d="M3 6.75A.75.75 0 0 1 3.75 6h16.5a.75.75 0 0 1 0 1.5H3.75A.75.75 0 0 1 3 6.75ZM3 12a.75.75 0 0 1 .75-.75h16.5a.75.75 0 0 1 0 1.5H3.75A.75.75 0 0 1 3 12Zm0 5.25a.75.75 0 0 1 .75-.75h16.5a.75.75 0 0 1 0 1.5H3.75a.75.75 0 0 1-.75-.75Z" clip-rule="evenodd"></path></svg><span class="sr-only">Open Menu</span></button></div><a class="flex items-center ml-3 dark:text-white w-fit md:ml-5 xl:ml-7" href="/user/jcollins36855/myst-build/proj02-group7/"><span class="text-md sm:text-xl tracking-tight sm:mr-5">Made with MyST</span></a></div><div class="flex items-center flex-grow w-auto"><div class="flex-grow hidden text-md lg:block"></div><div class="flex-grow block"></div><button type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:R74op:" data-state="closed" class="flex items-center h-10 aspect-square sm:w-64 text-left text-gray-400 border border-gray-300 dark:border-gray-600 rounded-lg bg-gray-50 dark:bg-gray-700 hover:ring-blue-500 dark:hover:ring-blue-500 hover:border-blue-500 dark:hover:border-blue-500"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="p-2.5 h-10 w-10 aspect-square"><path fill-rule="evenodd" d="M10.5 3.75a6.75 6.75 0 1 0 0 13.5 6.75 6.75 0 0 0 0-13.5ZM2.25 10.5a8.25 8.25 0 1 1 14.59 5.28l4.69 4.69a.75.75 0 1 1-1.06 1.06l-4.69-4.69A8.25 8.25 0 0 1 2.25 10.5Z" clip-rule="evenodd"></path></svg><span class="hidden sm:block grow">Search</span><div aria-hidden="true" class="items-center hidden mx-1 font-mono text-sm text-gray-400 sm:flex gap-x-1"><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none hide-mac">CTRL</kbd><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none show-mac">⌘</kbd><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none ">K</kbd><script>
;(() => {
const script = document.currentScript;
const root = script.parentElement;

const isMac = /mac/i.test(
      window.navigator.userAgentData?.platform ?? window.navigator.userAgent,
    );
root.querySelectorAll(".hide-mac").forEach(node => {node.classList.add(isMac ? "hidden" : "block")});
root.querySelectorAll(".show-mac").forEach(node => {node.classList.add(!isMac ? "hidden" : "block")});
})()</script></div></button><button class="theme rounded-full aspect-square border border-stone-700 dark:border-white hover:bg-neutral-100 border-solid overflow-hidden text-stone-700 dark:text-white hover:text-stone-500 dark:hover:text-neutral-800 w-8 h-8 mx-3" title="Toggle theme between light and dark mode" aria-label="Toggle theme between light and dark mode"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="h-full w-full p-0.5 hidden dark:block"><path fill-rule="evenodd" d="M9.528 1.718a.75.75 0 0 1 .162.819A8.97 8.97 0 0 0 9 6a9 9 0 0 0 9 9 8.97 8.97 0 0 0 3.463-.69.75.75 0 0 1 .981.98 10.503 10.503 0 0 1-9.694 6.46c-5.799 0-10.5-4.7-10.5-10.5 0-4.368 2.667-8.112 6.46-9.694a.75.75 0 0 1 .818.162Z" clip-rule="evenodd"></path></svg><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-full w-full p-0.5 dark:hidden"><path stroke-linecap="round" stroke-linejoin="round" d="M12 3v2.25m6.364.386-1.591 1.591M21 12h-2.25m-.386 6.364-1.591-1.591M12 18.75V21m-4.773-4.227-1.591 1.591M5.25 12H3m4.227-4.773L5.636 5.636M15.75 12a3.75 3.75 0 1 1-7.5 0 3.75 3.75 0 0 1 7.5 0Z"></path></svg></button><div class="block sm:hidden"></div><div class="hidden sm:block"></div></div></nav></div><div class="fixed xl:article-grid grid-gap xl:w-screen xl:pointer-events-none overflow-auto max-xl:min-w-[300px] hidden z-10" style="top:60px"><div class="pointer-events-auto xl:col-margin-left flex-col overflow-hidden hidden xl:flex"><div class="flex-grow py-6 overflow-y-auto primary-scrollbar"><nav aria-label="Navigation" class="overflow-y-hidden transition-opacity ml-3 xl:ml-0 mr-3 max-w-[350px] lg:hidden"><div class="w-full px-1 dark:text-white font-medium"></div></nav><div class="my-3 border-b-2 lg:hidden"></div><nav aria-label="Table of Contents" class="flex-grow overflow-y-hidden transition-opacity ml-3 xl:ml-0 mr-3 max-w-[350px]"><div class="w-full px-1 dark:text-white"><a title="Stat159 Project 2 - Reproducibility in Natural Language Processing" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30 font-bold" href="/user/jcollins36855/myst-build/proj02-group7/">Stat159 Project 2 - Reproducibility in Natural Language Processing</a><a title="Contributions" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/user/jcollins36855/myst-build/proj02-group7/contributions">Contributions</a><a title="Project 2: Reproducibility in Natural Language Processing" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/user/jcollins36855/myst-build/proj02-group7/nlp-p01">Project 2: Reproducibility in Natural Language Processing</a><a title="Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/user/jcollins36855/myst-build/proj02-group7/nlp-p02">Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)</a><a title="Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/user/jcollins36855/myst-build/proj02-group7/nlp-p03">Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)</a><a title="Part 4" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/user/jcollins36855/myst-build/proj02-group7/nlp-p04">Part 4</a><a title="Project 2: Reproducibility in Natural Language Processing" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/user/jcollins36855/myst-build/proj02-group7/proj02-nlp">Project 2: Reproducibility in Natural Language Processing</a><a title="Project 2: Reproducibility in Natural Langauge Processing" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/user/jcollins36855/myst-build/proj02-group7/project-description">Project 2: Reproducibility in Natural Langauge Processing</a></div></nav></div><div class="flex-none py-6 transition-all duration-700 translate-y-6 opacity-0"><a class="flex mx-auto text-gray-700 w-fit hover:text-blue-700 dark:text-gray-200 dark:hover:text-blue-400" href="https://mystmd.org/made-with-myst" target="_blank" rel="noreferrer"><svg style="width:24px;height:24px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100" stroke="none"><g id="icon"><path fill="currentColor" d="M23.8,54.8v-3.6l4.7-0.8V17.5l-4.7-0.8V13H36l13.4,31.7h0.2l13-31.7h12.6v3.6l-4.7,0.8v32.9l4.7,0.8v3.6h-15
          v-3.6l4.9-0.8V20.8H65L51.4,53.3h-3.8l-14-32.5h-0.1l0.2,17.4v12.1l5,0.8v3.6H23.8z"></path><path fill="#F37726" d="M47,86.9c0-5.9-3.4-8.8-10.1-8.8h-8.4c-5.2,0-9.4-1.3-12.5-3.8c-3.1-2.5-5.4-6.2-6.8-11l4.8-1.6
          c1.8,5.6,6.4,8.6,13.8,8.8h9.2c6.4,0,10.8,2.5,13.1,7.5c2.3-5,6.7-7.5,13.1-7.5h8.4c7.8,0,12.7-2.9,14.6-8.7l4.8,1.6
          c-1.4,4.9-3.6,8.6-6.8,11.1c-3.1,2.5-7.3,3.7-12.4,3.8H63c-6.7,0-10,2.9-10,8.8"></path></g></svg><span class="self-center ml-2 text-sm">Made with MyST</span></a></div></div></div><main class="article-grid grid-gap"><article class="article-grid subgrid-gap col-screen article content"><div class="hidden"></div><div id="skip-to-frontmatter" aria-label="article frontmatter" class="mb-8 pt-9"><div class="flex items-center mb-5 h-6 text-sm font-light"><div class="flex-grow"></div><a href="https://github.com/UCB-stat-159-f25/proj02-group7" title="GitHub Repository: UCB-stat-159-f25/proj02-group7" target="_blank" rel="noopener noreferrer" class="text-inherit hover:text-inherit"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="inline-block mr-1 opacity-60 hover:opacity-100"><path d="M12 2.5c-5.4 0-9.8 4.4-9.8 9.7 0 4.3 2.8 8 6.7 9.2.5.1.7-.2.7-.5v-1.8c-2.4.5-3.1-.6-3.3-1.1-.1-.3-.6-1.1-1-1.4-.3-.2-.8-.6 0-.6s1.3.7 1.5 1c.9 1.5 2.3 1.1 2.8.8.1-.6.3-1.1.6-1.3-2.2-.2-4.4-1.1-4.4-4.8 0-1.1.4-1.9 1-2.6-.1-.2-.4-1.2.1-2.6 0 0 .8-.3 2.7 1 .8-.2 1.6-.3 2.4-.3.8 0 1.7.1 2.4.3 1.9-1.3 2.7-1 2.7-1 .5 1.3.2 2.3.1 2.6.6.7 1 1.5 1 2.6 0 3.7-2.3 4.6-4.4 4.8.4.3.7.9.7 1.8V21c0 .3.2.6.7.5 3.9-1.3 6.6-4.9 6.6-9.2 0-5.4-4.4-9.8-9.8-9.8z"></path></svg></a><a href="https://github.com/UCB-stat-159-f25/proj02-group7/edit/main/nlp-P02.ipynb" title="Edit This Page" target="_blank" rel="noopener noreferrer" class="text-inherit hover:text-inherit"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.25rem" height="1.25rem" class="inline-block mr-1 opacity-60 hover:opacity-100"><path stroke-linecap="round" stroke-linejoin="round" d="m16.862 4.487 1.687-1.688a1.875 1.875 0 1 1 2.652 2.652L10.582 16.07a4.5 4.5 0 0 1-1.897 1.13L6 18l.8-2.685a4.5 4.5 0 0 1 1.13-1.897l8.932-8.931Zm0 0L19.5 7.125M18 14v4.75A2.25 2.25 0 0 1 15.75 21H5.25A2.25 2.25 0 0 1 3 18.75V8.25A2.25 2.25 0 0 1 5.25 6H10"></path></svg></a><div class="relative flex inline-block mx-1 grow-0" data-headlessui-state=""><button class="relative ml-2 -mr-1" id="headlessui-menu-button-:Rs8top:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Downloads</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.25rem" height="1.25rem"><title>Download</title><path stroke-linecap="round" stroke-linejoin="round" d="M3 16.5v2.25A2.25 2.25 0 0 0 5.25 21h13.5A2.25 2.25 0 0 0 21 18.75V16.5M16.5 12 12 16.5m0 0L7.5 12m4.5 4.5V3"></path></svg></button></div></div><h1 class="mb-0">Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)</h1><header class="mt-4 not-prose"><div class="grid grid-cols-1 sm:grid-cols-2 gap-y-1"><div class="pb-2 text-xs font-thin uppercase">Authors</div><div class="pb-2 text-xs font-thin uppercase">Affiliations</div><div><span class="font-semibold text-sm"><button class="focus:shadow-[0_0_0_2px] focus:shadow-black outline-none hover:underline" aria-label="Author Details" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:Rad8top:" data-state="closed">Reily Fairchild</button></span></div><div class="text-sm"><div>UC Berkeley<!-- --> </div></div><div><span class="font-semibold text-sm"><button class="focus:shadow-[0_0_0_2px] focus:shadow-black outline-none hover:underline" aria-label="Author Details" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:Ral8top:" data-state="closed">Atiila Joselyn Birah Kharobo</button></span></div><div class="text-sm"><div>UC Berkeley<!-- --> </div></div><div><span class="font-semibold text-sm"><button class="focus:shadow-[0_0_0_2px] focus:shadow-black outline-none hover:underline" aria-label="Author Details" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:Rat8top:" data-state="closed">Jordan Elizabeth Collins</button></span></div><div class="text-sm"><div>UC Berkeley<!-- --> </div></div><div><span class="font-semibold text-sm"><button class="focus:shadow-[0_0_0_2px] focus:shadow-black outline-none hover:underline" aria-label="Author Details" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:Rb58top:" data-state="closed">Aditya Jagannadha Sai Mangalampalli</button></span></div><div class="text-sm"><div>UC Berkeley<!-- --> </div></div></div></header></div><div class="block my-10 lg:sticky lg:z-10 lg:h-0 lg:pt-0 lg:my-0 lg:ml-10 lg:col-margin-right" style="top:60px"><nav></nav></div><div id="skip-to-article"></div><div id="kzPN2nhyyU" class="relative group/block"><h1 id="part-2-simple-text-processing-tokenization-lemmatization-word-frequency-vectorization-20-pts" class="relative group"><span class="heading-text">Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#part-2-simple-text-processing-tokenization-lemmatization-word-frequency-vectorization-20-pts" title="Link to this Section" aria-label="Link to this Section">¶</a></h1><p>Now we will start working on simple text processing using the <code>SpaCy</code> package and the same dataset as Part 1. The package should already be included in the <code>environment.yml</code>. However, we will also need to download <code>en_core_web_sm</code>, an English language text processing model. To do this, while having your <code>sotu</code> environment activated, run the following:</p><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 bg-stone-200/10"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-text" style="white-space:pre">python -m spacy download en_core_web_sm</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><p>Now, you should be good to go!</p><p>Some important definitions:</p><ul><li><p><em>Token</em>: a single word or piece of a word</p></li><li><p><em>Lemma</em>: the core component of a word, e.g., “complete” is the lemma for “completed” and “completely”</p></li><li><p><em>Stop Word</em>: a common word that does not add semantic value, such as “a”, “and”, “the”, etc.</p></li><li><p><em>Vectorization</em>: representing a document as a vector where each index in the vector corresponds to a token or word and each entry is the count.</p></li></ul><p>In this section, we will explore the most common tokens and lemmas throughout different slices of the speech data. We will also develop vectorization representations of the speeches.</p><p>The core steps are:</p><ol start="1"><li><p>Process speeches using the SpaCy nlp module</p></li><li><p>Analyze Tokens vs Lemmas:</p></li></ol><ul><li><p>Create a list of all tokens across all speeches that are not stop words, punctuation, or spaces.</p></li><li><p>Create a second list of the lemmas for these same tokens.</p></li><li><p>Display the top 25 for each of these and compare.</p></li></ul><ol start="3"><li><p>Analyze common word distributions over different years:</p></li></ol><ul><li><p>Create a function that takes the dataset and a year as an input and outputs the top n lemmas for that year’s speeches</p></li><li><p>Compare the top 10 words for 2023 versus 2019</p></li></ul><ol start="4"><li><p>Document Vectorization:</p></li></ol><ul><li><p>Train a Term Frequency-Inverse Document Frequency (TF-IDF) vectorization model using your processed dataset and scikit learn</p></li><li><p>Output the feature vectors</p></li></ul><p><strong>Helpful Resources:</strong></p><ul><li><p><a target="_blank" rel="noreferrer" href="https://realpython.com/natural-language-processing-spacy-python/" class="">https://<wbr/>realpython<wbr/>.com<wbr/>/natural<wbr/>-language<wbr/>-processing<wbr/>-spacy<wbr/>-python/</a></p></li><li><p><a target="_blank" rel="noreferrer" href="https://www.statology.org/text-preprocessing-feature-engineering-spacy/" class="">https://<wbr/>www<wbr/>.statology<wbr/>.org<wbr/>/text<wbr/>-preprocessing<wbr/>-feature<wbr/>-engineering<wbr/>-spacy/</a></p></li><li><p><a target="_blank" rel="noreferrer" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#" class="">https://<wbr/>scikit<wbr/>-learn<wbr/>.org<wbr/>/stable<wbr/>/modules<wbr/>/generated<wbr/>/sklearn<wbr/>.feature<wbr/>_extraction<wbr/>.text<wbr/>.TfidfVectorizer<wbr/>.html#</a></p></li><li><p><a target="_blank" rel="noreferrer" href="https://www.geeksforgeeks.org/nlp/how-to-store-a-tfidfvectorizer-for-future-use-in-scikit-learn/" class="">https://<wbr/>www<wbr/>.geeksforgeeks<wbr/>.org<wbr/>/nlp<wbr/>/how<wbr/>-to<wbr/>-store<wbr/>-a<wbr/>-tfidfvectorizer<wbr/>-for<wbr/>-future<wbr/>-use<wbr/>-in<wbr/>-scikit<wbr/>-learn/</a></p></li></ul></div><div id="hAv4WWMK5m" class="relative group/block"><h2 id="processing-speeches-with-spacy" class="relative group"><span class="heading-text">Processing Speeches with SpaCy</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#processing-speeches-with-spacy" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>Lets study just speeches from 2000 and onwards to begin with. So, be sure to subset your DataFrame to just these speeches before continuing!</p></div><div id="UinbthcDEo" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">!pip install spacy</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="fWzqeZdU3actGolF2YiKM" class="max-w-full overflow-x-auto m-0 group not-prose relative overflow-y-visible text-left mb-5"><div><pre class="text-sm font-thin font-system"><code><span>Requirement already satisfied: spacy in /srv/conda/envs/notebook/lib/python3.12/site-packages (3.8.11)
Requirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.11 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (3.0.12)
Requirement already satisfied: spacy-loggers&lt;2.0.0,&gt;=1.0.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (1.0.5)
Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (1.0.15)
Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.0.13)
Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (3.0.12)
Requirement already satisfied: thinc&lt;8.4.0,&gt;=8.3.4 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (8.3.10)
Requirement already satisfied: wasabi&lt;1.2.0,&gt;=0.9.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (1.1.3)
Requirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.3 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.5.2)
Requirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.6 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.0.10)
Requirement already satisfied: weasel&lt;0.5.0,&gt;=0.4.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (0.4.3)
Requirement already satisfied: typer-slim&lt;1.0.0,&gt;=0.3.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (0.20.0)
Requirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (4.67.1)
Requirement already satisfied: numpy&gt;=1.19.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (1.26.4)
Requirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.32.3)
Requirement already satisfied: pydantic!=1.8,!=1.8.1,&lt;3.0.0,&gt;=1.7.4 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.12.3)
Requirement already satisfied: jinja2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (3.1.6)
Requirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (70.0.0)
Requirement already satisfied: packaging&gt;=20.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (24.0)
Requirement already satisfied: annotated-types&gt;=0.6.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,&lt;3.0.0,&gt;=1.7.4-&gt;spacy) (0.7.0)
Requirement already satisfied: pydantic-core==2.41.4 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,&lt;3.0.0,&gt;=1.7.4-&gt;spacy) (2.41.4)
Requirement already satisfied: typing-extensions&gt;=4.14.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,&lt;3.0.0,&gt;=1.7.4-&gt;spacy) (4.15.0)
Requirement already satisfied: typing-inspection&gt;=0.4.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,&lt;3.0.0,&gt;=1.7.4-&gt;spacy) (0.4.2)
Requirement already satisfied: charset_normalizer&lt;4,&gt;=2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (3.3.2)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (3.7)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (1.26.20)
Requirement already satisfied: certifi&gt;=2017.4.17 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (2025.10.5)
Requirement already satisfied: blis&lt;1.4.0,&gt;=1.3.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from thinc&lt;8.4.0,&gt;=8.3.4-&gt;spacy) (1.3.3)
Requirement already satisfied: confection&lt;1.0.0,&gt;=0.0.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from thinc&lt;8.4.0,&gt;=8.3.4-&gt;spacy) (0.1.5)
Requirement already satisfied: click&gt;=8.0.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from typer-slim&lt;1.0.0,&gt;=0.3.0-&gt;spacy) (8.3.0)
Requirement already satisfied: cloudpathlib&lt;1.0.0,&gt;=0.7.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from weasel&lt;0.5.0,&gt;=0.4.2-&gt;spacy) (0.23.0)
Requirement already satisfied: smart-open&lt;8.0.0,&gt;=5.2.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from weasel&lt;0.5.0,&gt;=0.4.2-&gt;spacy) (7.5.0)
Requirement already satisfied: wrapt in /srv/conda/envs/notebook/lib/python3.12/site-packages (from smart-open&lt;8.0.0,&gt;=5.2.1-&gt;weasel&lt;0.5.0,&gt;=0.4.2-&gt;spacy) (1.17.3)
Requirement already satisfied: MarkupSafe&gt;=2.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from jinja2-&gt;spacy) (3.0.2)
</span></code></pre></div></div></div><div id="cjue563K2g" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">import spacy
!python -m spacy download en_core_web_sm
spacy.cli.download(&quot;en_core_web_sm&quot;)

from tqdm import tqdm
from collections import Counter
import pandas as pd
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

sou = pd.read_csv(&#x27;data/SOTU.csv&#x27;)
nlp = spacy.load(&quot;en_core_web_sm&quot;)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="-4ozTqHXZ9Lum5TqQvjhJ" class="max-w-full overflow-x-auto m-0 group not-prose relative overflow-y-visible text-left mb-5"><div><pre class="text-sm font-thin font-system"><code><span>Collecting en-core-web-sm==3.8.0
  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)
</span><span>     </span><span style="color:rgb(85, 85, 85)">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span><span> </span><span style="color:rgb(0, 187, 0)">12.8/12.8 MB</span><span> </span><span style="color:rgb(187, 0, 0)">64.1 MB/s</span><span> eta </span><span style="color:rgb(0, 187, 187)">0:00:00</span><span>00:01</span><span>
</span><span style="color:rgb(0, 187, 0)">✔ Download and installation successful</span><span>
You can now load the package via spacy.load(&#x27;en_core_web_sm&#x27;)
Collecting en-core-web-sm==3.8.0
  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)
</span><span>     </span><span style="color:rgb(85, 85, 85)">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span><span> </span><span style="color:rgb(0, 187, 0)">12.8/12.8 MB</span><span> </span><span style="color:rgb(187, 0, 0)">70.8 MB/s</span><span> eta </span><span style="color:rgb(0, 187, 187)">0:00:00</span><span>00:01</span><span>
</span><span style="color:rgb(0, 187, 0)">✔ Download and installation successful</span><span>
You can now load the package via spacy.load(&#x27;en_core_web_sm&#x27;)
</span><span style="color:rgb(187, 187, 0)">⚠ Restart to reload dependencies</span><span>
If you are in a Jupyter or Colab notebook, you may need to restart Python in
order to load all the package&#x27;s dependencies. You can do this by selecting the
&#x27;Restart kernel&#x27; or &#x27;Restart runtime&#x27; option.
</span></code></pre></div></div></div><div id="GdffQC24GD" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">sou</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="HcOOzz0s_e12jOSNrJg_r" class="max-w-full overflow-x-auto m-0 group not-prose relative overflow-y-visible text-left mb-5"><div><div class="p-2.5">Loading...</div></div></div></div><div id="lkcPPgzD6y" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># subset the speech dataframe for speeches from 2000 and onwards
sou_new = sou[sou[&#x27;Year&#x27;] &gt;= 2000]
sou_new</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="m6LSaOHKeJ8iJGcAcdMSZ" class="max-w-full overflow-x-auto m-0 group not-prose relative overflow-y-visible text-left mb-5"><div><div class="p-2.5">Loading...</div></div></div></div><div id="RQMYGuJElA" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># Process each speeches using the &#x27;nlp&#x27; function
processed_speeches = []

for i in range(len(sou_new)):
    speech_text = sou_new.loc[i, &#x27;Text&#x27;] 
    processed = nlp(speech_text)    
    processed_speeches.append(processed)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="zZVwDprrIut9jKJM7LUVw" class="max-w-full overflow-x-auto m-0 group not-prose relative overflow-y-visible text-left"></div></div><div id="Kk3nkgQK0c" class="relative group/block"><h2 id="analyze-tokens-vs-lemmas" class="relative group"><span class="heading-text">Analyze Tokens vs Lemmas</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#analyze-tokens-vs-lemmas" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><h3 id="token-list" class="relative group"><span class="heading-text">Token List</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#token-list" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>Create a list of tokens across all speeches that are not spaces, stopwords, or punctuation. Make each token lowercase as well. <em>Hint: each element of the list we just created are themselves lists of tokens. Token objects have attributes <code>is_stop</code>, <code>is_punct</code>, and <code>is_space</code>.</em></p></div><div id="lTfLgCi0Md" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">tokens = []

for speech in processed_speeches:
    for token in speech:
        if not token.is_punct and not token.is_space and not token.is_stop:
            tokens.append(token.text)
    </code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="qQVk68j3cdAb3-V7rzZHp" class="max-w-full overflow-x-auto m-0 group not-prose relative overflow-y-visible text-left"></div></div><div id="q3kQWEed62" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># print top 20 tokens
# Hint - use Counter, and one of the Counter object&#x27;s methods to display the top 20
token_count = Counter(tokens).most_common(20)
print(token_count)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="soS0B21ZflnM44EZfXR8a" class="max-w-full overflow-x-auto m-0 group not-prose relative overflow-y-visible text-left mb-5"><div><pre class="text-sm font-thin font-system"><code><span>[(&#x27;America&#x27;, 816), (&#x27;people&#x27;, 622), (&#x27;American&#x27;, 582), (&#x27;new&#x27;, 495), (&#x27;years&#x27;, 439), (&#x27;Americans&#x27;, 437), (&#x27;world&#x27;, 409), (&#x27;year&#x27;, 401), (&#x27;country&#x27;, 369), (&#x27;jobs&#x27;, 325), (&#x27;work&#x27;, 324), (&#x27;know&#x27;, 323), (&#x27;Congress&#x27;, 317), (&#x27;time&#x27;, 297), (&#x27;help&#x27;, 278), (&#x27;need&#x27;, 266), (&#x27;tonight&#x27;, 253), (&#x27;President&#x27;, 246), (&#x27;economy&#x27;, 243), (&#x27;want&#x27;, 237)]
</span></code></pre></div></div></div><div id="z7p38PeDnP" class="relative group/block"><h3 id="lemma-list" class="relative group"><span class="heading-text">Lemma List</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#lemma-list" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>Do the same as above, but for lemmas. <em>Hint: recall lemmas are components of words. Each token should have an attribute to extract the lemma.</em></p></div><div id="UTwPR2d4Ea" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">lemmas = []

for token in tokens:
    word = nlp(token) # run the tokens through nlp to make sure it has the lemma_ attribute
    for token in word:
        lemma=token.lemma_
        lemmas.append(lemma.lower())</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="0VeGZkXGKoGgpT_huk6Y3" class="max-w-full overflow-x-auto m-0 group not-prose relative overflow-y-visible text-left"></div></div><div id="zbatsChg27" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># print top 20 lemmas
lemma_counts = Counter(lemmas).most_common(20)
lemma_counts</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="GMT6nsejCnxDvJpTC0mpf" class="max-w-full overflow-x-auto m-0 group not-prose relative overflow-y-visible text-left mb-5"><div class="font-mono text-sm whitespace-pre-wrap"><code><span>[(&#x27;american&#x27;, 1019),
 (&#x27;year&#x27;, 845),
 (&#x27;america&#x27;, 821),
 (&#x27;people&#x27;, 639),
 (&#x27;work&#x27;, 566),
 (&#x27;new&#x27;, 532),
 (&#x27;job&#x27;, 503),
 (&#x27;country&#x27;, 435),
 (&#x27;world&#x27;, 426),
 (&#x27;nation&#x27;, 402),
 (&#x27;know&#x27;, 396),
 (&#x27;help&#x27;, 387),
 (&#x27;need&#x27;, 353),
 (&#x27;time&#x27;, 351),
 (&#x27;tonight&#x27;, 344),
 (&#x27;child&#x27;, 334),
 (&#x27;state&#x27;, 326),
 (&#x27;let&#x27;, 326),
 (&#x27;congress&#x27;, 319),
 (&#x27;family&#x27;, 303)]</span></code></div></div></div><div id="xCytpTUp7j" class="relative group/block"><h3 id="token-versus-lemma-comparison" class="relative group"><span class="heading-text">Token versus Lemma Comparison</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#token-versus-lemma-comparison" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>What do you notice about the top tokens versus the top lemmas?
Consider two tokens - “year” and “years” - how do their counts compare to the lemma “year”?
What about the lemma “child”?</p></div><div id="bU9HFEen2v" class="relative group/block"><h2 id="common-words" class="relative group"><span class="heading-text">Common Words</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#common-words" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><h3 id="common-words-per-year-function" class="relative group"><span class="heading-text">Common Words per Year Function</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#common-words-per-year-function" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>Fill in the below function to obtain the n-most common words in speeches for a given year.</p><p>inputs:</p><ul><li><p>df raw unprocessed sou dataframe</p></li><li><p>year</p></li><li><p>n
outputs:</p></li><li><p>top n words for that years</p></li></ul><p>steps:</p><ul><li><p>subset the dataframe for the year of interest - note the years might not be in int type</p></li><li><p>process the subsetted dataframe with spacy</p></li><li><p>get the lemmas across all those speeches</p></li><li><p>count the top n lemmas</p></li></ul></div><div id="u8vmUNlSVJ" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">def get_most_common_words(df, year, n=25):
    &quot;&quot;&quot;
    Processes the SOTU speech for a given year and returns
    the most common non-stopword/punctuation lemmas.

    INPUTS
    - raw unprocessed sou data frame
    - year

    OUTPUTS
    - top n words for that year
    &quot;&quot;&quot;

    # Step 1: Subset df
    new_df = df[df[&#x27;Year&#x27;] == year]
    
    # Step 2: Process the text with spaCy
    new_df = df[df[&#x27;Year&#x27;] == year].reset_index(drop=True) 
    processed_speeches = []

    for i in range(len(new_df)):
        speech_text = new_df.loc[i, &#x27;Text&#x27;] 
        processed = nlp(speech_text)    
        processed_speeches.append(processed)
    
    # Step 3: Get lemmas
    lemmas = []

    for doc in processed_speeches:
        for token in doc:
            if not token.is_punct and not token.is_space and not token.is_stop:
                lemmas.append(token.lemma_.lower())
    
    lemma_counts = Counter(lemmas).most_common(n)
    
    return lemma_counts</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="26zhvxkWtEHbuuQ97dmp5" class="max-w-full overflow-x-auto m-0 group not-prose relative overflow-y-visible text-left"></div></div><div id="YxwtX0SKXu" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># test it on 2024
get_most_common_words(sou, 2024, n=20)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="xjSi7tPhzQDdIEFZ29JsP" class="max-w-full overflow-x-auto m-0 group not-prose relative overflow-y-visible text-left mb-5"><div class="font-mono text-sm whitespace-pre-wrap"><code><span>[(&#x27;president&#x27;, 58),
 (&#x27;year&#x27;, 45),
 (&#x27;america&#x27;, 44),
 (&#x27;american&#x27;, 34),
 (&#x27;people&#x27;, 33),
 (&#x27;$&#x27;, 33),
 (&#x27;member&#x27;, 32),
 (&#x27;want&#x27;, 29),
 (&#x27;audience&#x27;, 29),
 (&#x27;know&#x27;, 29),
 (&#x27;pay&#x27;, 29),
 (&#x27;come&#x27;, 26),
 (&#x27;home&#x27;, 25),
 (&#x27;family&#x27;, 24),
 (&#x27;future&#x27;, 23),
 (&#x27;million&#x27;, 23),
 (&#x27;like&#x27;, 21),
 (&#x27;build&#x27;, 21),
 (&#x27;laughter&#x27;, 20),
 (&#x27;americans&#x27;, 20)]</span></code></div></div></div><div id="FJu8UXPZiM" class="relative group/block"><h3 id="compare-2023-to-2017" class="relative group"><span class="heading-text">Compare 2023 to 2017</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#compare-2023-to-2017" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>Run your function from the previous step to get the top 20 words for 2017 and 2023. Plot the words and their frequencies in a barchart and replicate the figure below.</p></div><div id="rTQrQcgevo" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">words_2023 = get_most_common_words(sou, 2023, n=20)
words_2017 = get_most_common_words(sou, 2017, n=20)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="AUW7jJ69gctjge0HlRFtQ" class="max-w-full overflow-x-auto m-0 group not-prose relative overflow-y-visible text-left"></div></div><div id="tjQHaLfaTa" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">words_2023</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="4BrzrkW6DTY9dWJIjnPmb" class="max-w-full overflow-x-auto m-0 group not-prose relative overflow-y-visible text-left mb-5"><div class="font-mono text-sm whitespace-pre-wrap"><code><span>[(&#x27;year&#x27;, 58),
 (&#x27;go&#x27;, 56),
 (&#x27;let&#x27;, 45),
 (&#x27;know&#x27;, 40),
 (&#x27;people&#x27;, 39),
 (&#x27;job&#x27;, 38),
 (&#x27;america&#x27;, 36),
 (&#x27;come&#x27;, 33),
 (&#x27;law&#x27;, 33),
 (&#x27;pay&#x27;, 33),
 (&#x27;american&#x27;, 31),
 (&#x27;$&#x27;, 31),
 (&#x27;president&#x27;, 30),
 (&#x27;look&#x27;, 27),
 (&#x27;world&#x27;, 25),
 (&#x27;folk&#x27;, 24),
 (&#x27;nation&#x27;, 24),
 (&#x27;audience&#x27;, 23),
 (&#x27;work&#x27;, 23),
 (&#x27;right&#x27;, 23)]</span></code></div></div></div><div id="xN6O7KRfCm" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">words_2017</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="GpPpGSyLsjsGqTctf02OQ" class="max-w-full overflow-x-auto m-0 group not-prose relative overflow-y-visible text-left mb-5"><div class="font-mono text-sm whitespace-pre-wrap"><code><span>[(&#x27;american&#x27;, 34),
 (&#x27;america&#x27;, 29),
 (&#x27;country&#x27;, 26),
 (&#x27;nation&#x27;, 21),
 (&#x27;great&#x27;, 20),
 (&#x27;new&#x27;, 19),
 (&#x27;year&#x27;, 19),
 (&#x27;world&#x27;, 18),
 (&#x27;job&#x27;, 15),
 (&#x27;people&#x27;, 15),
 (&#x27;americans&#x27;, 14),
 (&#x27;united&#x27;, 13),
 (&#x27;tonight&#x27;, 13),
 (&#x27;states&#x27;, 12),
 (&#x27;work&#x27;, 12),
 (&#x27;child&#x27;, 12),
 (&#x27;want&#x27;, 12),
 (&#x27;time&#x27;, 12),
 (&#x27;citizen&#x27;, 11),
 (&#x27;right&#x27;, 11)]</span></code></div></div></div><div id="qTTFjBxdzH" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># Hint - put the words and counts into a pd Dataframe for better structure
# and to make plotting easier
df_2017 = pd.DataFrame(words_2017, columns=[&#x27;lemma&#x27;, &#x27;count&#x27;])
df_2023 = pd.DataFrame(words_2023, columns=[&#x27;lemma&#x27;, &#x27;count&#x27;])</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="vgC-hg7AOaLPcozrp6wTu" class="max-w-full overflow-x-auto m-0 group not-prose relative overflow-y-visible text-left"></div></div><div id="q57m5yXIH3" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">fig, axes = plt.subplots(2, 1, figsize=(12, 10)) 
# Plot 2017
sns.barplot(
    x = &#x27;lemma&#x27;,
    y = &#x27;count&#x27;,
    data = df_2017,
    ax = axes[0],
     color=&#x27;#2C7BA1&#x27;
    
)
axes[0].set_title(&#x27;2017 State of the Union Most Frequent Words&#x27;)
axes[0].set_xlabel(&#x27;Word&#x27;)
axes[0].set_ylabel(&#x27;Count&#x27;)
axes[0].tick_params(axis=&#x27;x&#x27;, rotation=45) 

# Plot 2023
sns.barplot(
    x=&#x27;lemma&#x27;,
    y=&#x27;count&#x27;,
    data=df_2023,
    ax=axes[1],
    color=&#x27;#2C7BA1&#x27;
)

axes[1].set_title(&#x27;2023 State of the Union Most Frequent Words&#x27;)
axes[1].set_xlabel(&#x27;Word&#x27;)
axes[1].set_ylabel(&#x27;Count&#x27;)
axes[1].tick_params(axis=&#x27;x&#x27;, rotation=45)

plt.tight_layout()

#saving figure
plt.savefig(&#x27;outputs/sotu_word_freq2017_2023.png&#x27;)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="wDsf817asb04eMT9LWEUl" class="max-w-full overflow-x-auto m-0 group not-prose relative overflow-y-visible text-left mb-5"><img src="/user/jcollins36855/myst-build/proj02-group7/build/7ac9af6b29839f506566862c5057bae1.png" alt="&lt;Figure size 1200x1000 with 2 Axes&gt;"/></div></div><div id="TtLUYMv8F2" class="relative group/block"><h2 id="tf-idf-vectorization" class="relative group"><span class="heading-text">TF-IDF Vectorization</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#tf-idf-vectorization" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>To use statsitical alorithms on documents, we need to transform them into vectors, where each element of the vector corresponds to a particular word in a document or corpus of documents. One common way is via TF-IDF embeddings. LLMs work similarly - they typically use transformer models to generate text embeddings before sending text through a deep neural network.</p><p>Here we will fit a TF-IDF vectorizer, plot all the speeches on a 2-D grid using PCA and also using a heatmap, and examine TF-IDF scores for the top 10 most common words in the first speech. This is a good resource here: <a target="_blank" rel="noreferrer" href="https://medium.com/GeoffreyGordonAshbrook/vector-visualization-2d-plot-your-tf-idf-with-pca-83fa9fccb1d" class="">https://<wbr/>medium<wbr/>.com<wbr/>/GeoffreyGordonAshbrook<wbr/>/vector<wbr/>-visualization<wbr/>-2d<wbr/>-plot<wbr/>-your<wbr/>-tf<wbr/>-idf<wbr/>-with<wbr/>-pca<wbr/>-83fa9fccb1d</a></p></div><div id="TN8foedyR4" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># Install nlk to current kernel only once, can do from terminal 
#(has been added to environment.yml too so if using this, do not have to install in notebook)

import sys, subprocess
subprocess.check_call([sys.executable, &quot;-m&quot;, &quot;pip&quot;, &quot;install&quot;, &quot;nltk&quot;])

# confirm the version that was just installed
import nltk
print(&quot;NLTK version:&quot;, nltk.__version__)
</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="Oo9LWB94zdzMA8ot5cdf7" class="max-w-full overflow-x-auto m-0 group not-prose relative overflow-y-visible text-left mb-5"><div><pre class="text-sm font-thin font-system"><code><span>Requirement already satisfied: nltk in /srv/conda/envs/notebook/lib/python3.12/site-packages (3.9.2)
Requirement already satisfied: click in /srv/conda/envs/notebook/lib/python3.12/site-packages (from nltk) (8.3.0)
Requirement already satisfied: joblib in /srv/conda/envs/notebook/lib/python3.12/site-packages (from nltk) (1.5.2)
Requirement already satisfied: regex&gt;=2021.8.3 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from nltk) (2025.10.23)
Requirement already satisfied: tqdm in /srv/conda/envs/notebook/lib/python3.12/site-packages (from nltk) (4.67.1)
NLTK version: 3.9.2
</span></code></pre></div></div></div><div id="C3PcM63Pvc" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from nltk.tokenize.casual import casual_tokenize</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="LLozNIUyoBhey0PgN-FFO" class="max-w-full overflow-x-auto m-0 group not-prose relative overflow-y-visible text-left"></div></div><div id="aeoQyEIHEd" class="relative group/block"><h3 id="train-the-vectorizer-and-transform-the-data" class="relative group"><span class="heading-text">Train the Vectorizer and Transform the Data</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#train-the-vectorizer-and-transform-the-data" title="Link to this Section" aria-label="Link to this Section">¶</a></h3></div><div id="RQSdO4P7eq" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># you may use this as input to fit the TF-IDF vectorizer
raw_docs = sou[&quot;Text&quot;].to_list()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="H2sZh3G-W08VMjwNQZSqU" class="max-w-full overflow-x-auto m-0 group not-prose relative overflow-y-visible text-left"></div></div><div id="YQA30QTa7C" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">##########################
# Minimal TF-IDF Vectors (Reference: Vector Visualization by Ashbrook)
##########################

# Select Model
tfidf_model = TfidfVectorizer()

# Fit Model
tfidf_vectors = tfidf_model.fit_transform(raw_docs).toarray()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="VTrylnQudr3GlqlsIvm8g" class="max-w-full overflow-x-auto m-0 group not-prose relative overflow-y-visible text-left"></div></div><div id="dzCLkLOeoj" class="relative group/block"><p>The output of <code>fit_transform()</code> will be a matrix where each row corresponds to a speech, each column corresponds to a word in the corpus of speeches, and the value is the TF-IDF score which measures the importance of that word in that speech, relative to the rest of the speeches.</p></div><div id="vsbsZ5Jlvb" class="relative group/block"><h3 id="plot-speeches" class="relative group"><span class="heading-text">Plot Speeches</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#plot-speeches" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><ul><li><p>First used PCA to generate the first chart</p></li><li><p>Second use seaborn heatmap with a log-scaled color axis to generate the second chart</p></li></ul></div><div id="BmDf0QnHPA" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">from sklearn.decomposition import PCA
from matplotlib import pyplot as plt
%matplotlib inline
# Step 1: Set PCA to find first 2 principal components
pca = PCA(n_components=2)

# Step 2: Create a new dataframe where each row is a speech, and each column is a projection onto
# one of the two principal components
speeches_array = tfidf_vectors
speeches_df2d = pd.DataFrame(pca.fit_transform(speeches_array), columns=list(&#x27;xy&#x27;))

# Plot Data Visualization (Matplotlib)

# Plot (matplotlib only)
plt.style.use(&#x27;seaborn-v0_8&#x27;)           # just a style; still matplotlibplt.figure(figsize=(6,4))
plt.figure(figsize=(6,4))
plt.scatter(speeches_df2d[&#x27;x&#x27;], speeches_df2d[&#x27;y&#x27;], s=20)
plt.xlabel(&#x27;Principle Component 1&#x27;); plt.ylabel(&#x27;Principle Component 2&#x27;); plt.title(&#x27;Plot of Vectorized Speeches Principle Components&#x27;)
plt.grid(False)
plt.tight_layout(); plt.show()
</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="2RFImmKxiDqGs3LKY7aIy" class="max-w-full overflow-x-auto m-0 group not-prose relative overflow-y-visible text-left mb-5"><img src="/user/jcollins36855/myst-build/proj02-group7/build/4ef77f4dbc6064c6055fb2e3eedba2c8.png" alt="&lt;Figure size 600x400 with 1 Axes&gt;"/></div></div><div id="YXhQ0rDhTQ" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">import numpy as np
import seaborn as sns
from matplotlib.colors import LogNorm
import matplotlib.ticker as mticker
from matplotlib.ticker import FixedLocator, FixedFormatter

# vectorized_docs: scipy sparse matrix (rows=speeches, cols=tokens)
A = tfidf_vectors.astype(np.float32)   # densify

# Log scale needs vmin &gt; 0; pick the smallest positive value
pos_min = A[A &gt; 0].min() if (A &gt; 0).any() else 1e-6

plt.style.use(&#x27;seaborn-v0_8&#x27;)
plt.figure(figsize=(5.8, 4.7))

ax = sns.heatmap(
    A,
    cmap=&quot;magma&quot;,
    norm=LogNorm(vmin=pos_min, vmax=A.max()),
    cbar_kws={&quot;format&quot;: mticker.LogFormatterMathtext()}
)

ax.set_title(&quot;Vectorized Speeches&quot;, pad=8)
ax.set_xlabel(&quot;Vector Index&quot;)
ax.set_ylabel(&quot;Speech Index&quot;)

n_cols = A.shape[1]
step = 928         
xticks = np.arange(0, n_cols, step)

ax.xaxis.set_major_locator(FixedLocator(xticks))
ax.xaxis.set_major_formatter(FixedFormatter([str(v) for v in xticks]))
plt.setp(ax.get_xticklabels(), rotation=90)
plt.grid(False)
plt.tight_layout()
plt.show()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="HifFgac6WqfB3s5Eci-FJ" class="max-w-full overflow-x-auto m-0 group not-prose relative overflow-y-visible text-left mb-5"><img src="/user/jcollins36855/myst-build/proj02-group7/build/6e1599813fa9e8786684dc59982f90ee.png" alt="&lt;Figure size 580x470 with 2 Axes&gt;"/></div></div><div id="kwJONA2afw" class="relative group/block"><h3 id="get-the-tf-idf-value-for-certain-words-and-documents" class="relative group"><span class="heading-text">Get the TF-IDF value for certain words and documents</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#get-the-tf-idf-value-for-certain-words-and-documents" title="Link to this Section" aria-label="Link to this Section">¶</a></h3></div><div id="qupCySkvaX" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">word_list = [&#x27;year&#x27;,
 &#x27;america&#x27;,
 &#x27;people&#x27;,
 &#x27;american&#x27;,
 &#x27;work&#x27;,
 &#x27;new&#x27;,
 &#x27;job&#x27;,
 &#x27;country&#x27;,
 &#x27;americans&#x27;,
 &#x27;world&#x27;] # top ten most common words through whole corpus</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="70THvtSSOfCgBTrFuzEg7" class="max-w-full overflow-x-auto m-0 group not-prose relative overflow-y-visible text-left"></div></div><div id="l2i9LqUEEY" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">word_nums = [tfidf_model.vocabulary_[w] for w in word_list] # get each word&#x27;s index number using the .vocabular_ attributed of vectorizer</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="rOyakbLmCuFMWTOD4jvdu" class="max-w-full overflow-x-auto m-0 group not-prose relative overflow-y-visible text-left"></div></div><div id="nPc3ljumz8" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">idf_score = tfidf_model.idf_[word_nums] # get their IDF score by using .idf_ at the indices from the previous step</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="6ufGtqflZegYrbllE-BbP" class="max-w-full overflow-x-auto m-0 group not-prose relative overflow-y-visible text-left"></div></div><div id="wbHI1obMB4" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">tf_idf = tfidf_vectors[0, word_nums] # get the tf_idf score for the first speech</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="lezjbfnB8er9QZiHChrxO" class="max-w-full overflow-x-auto m-0 group not-prose relative overflow-y-visible text-left"></div></div><div id="hSfGV54Tc4" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">pd.DataFrame({&quot;Word&quot;: word_list, &quot;IDF Score&quot;: idf_score, &quot;TF-IDF Score&quot;: tf_idf})</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="eeT-IF3jalhzpiiqbYZt6" class="max-w-full overflow-x-auto m-0 group not-prose relative overflow-y-visible text-left mb-5"><div><div class="p-2.5">Loading...</div></div></div></div><div></div><div class="flex pt-10 mb-10 space-x-4"><a class="flex-1 block p-4 font-normal text-gray-600 no-underline border border-gray-200 rounded shadow-sm group hover:border-blue-600 dark:hover:border-blue-400 hover:text-blue-600 dark:hover:text-blue-400 dark:text-gray-100 dark:border-gray-500 hover:shadow-lg dark:shadow-neutral-700" href="/user/jcollins36855/myst-build/proj02-group7/nlp-p01"><div class="flex h-full align-middle"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem" class="self-center transition-transform group-hover:-translate-x-1 shrink-0"><path stroke-linecap="round" stroke-linejoin="round" d="M10.5 19.5 3 12m0 0 7.5-7.5M3 12h18"></path></svg><div class="flex-grow text-right"><div class="text-xs text-gray-500 dark:text-gray-400">Stat159 Project 2 - Reproducibility in Natural Language Processing</div>Project 2: Reproducibility in Natural Language Processing</div></div></a><a class="flex-1 block p-4 font-normal text-gray-600 no-underline border border-gray-200 rounded shadow-sm group hover:border-blue-600 dark:hover:border-blue-400 hover:text-blue-600 dark:hover:text-blue-400 dark:text-gray-100 dark:border-gray-500 hover:shadow-lg dark:shadow-neutral-700" href="/user/jcollins36855/myst-build/proj02-group7/nlp-p03"><div class="flex h-full align-middle"><div class="flex-grow"><div class="text-xs text-gray-500 dark:text-gray-400">Stat159 Project 2 - Reproducibility in Natural Language Processing</div>Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)</div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem" class="self-center transition-transform group-hover:translate-x-1 shrink-0"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 4.5 21 12m0 0-7.5 7.5M21 12H3"></path></svg></div></a></div></article></main><script>((a,d)=>{if(!window.history.state||!window.history.state.key){let h=Math.random().toString(32).slice(2);window.history.replaceState({key:h},"")}try{let f=JSON.parse(sessionStorage.getItem(a)||"{}")[d||window.history.state.key];typeof f=="number"&&window.scrollTo(0,f)}catch(h){console.error(h),sessionStorage.removeItem(a)}})("positions", null)</script><link rel="modulepreload" href="/user/jcollins36855/myst-build/proj02-group7/build/entry.client-UNPC4GT3.js"/><link rel="modulepreload" href="/user/jcollins36855/myst-build/proj02-group7/build/_shared/chunk-OCTKKCIL.js"/><link rel="modulepreload" href="/user/jcollins36855/myst-build/proj02-group7/build/_shared/chunk-UAI5KRM7.js"/><link rel="modulepreload" href="/user/jcollins36855/myst-build/proj02-group7/build/_shared/chunk-2NH4LW52.js"/><link rel="modulepreload" href="/user/jcollins36855/myst-build/proj02-group7/build/_shared/chunk-F7G67JTZ.js"/><link rel="modulepreload" href="/user/jcollins36855/myst-build/proj02-group7/build/_shared/chunk-HBJK6BW3.js"/><link rel="modulepreload" href="/user/jcollins36855/myst-build/proj02-group7/build/_shared/chunk-HYMQ7M2K.js"/><link rel="modulepreload" href="/user/jcollins36855/myst-build/proj02-group7/build/_shared/chunk-OHOXABTA.js"/><link rel="modulepreload" href="/user/jcollins36855/myst-build/proj02-group7/build/_shared/chunk-OCWQY3HK.js"/><link rel="modulepreload" href="/user/jcollins36855/myst-build/proj02-group7/build/_shared/chunk-CPTH56EW.js"/><link rel="modulepreload" href="/user/jcollins36855/myst-build/proj02-group7/build/_shared/chunk-3CVK3PYF.js"/><link rel="modulepreload" href="/user/jcollins36855/myst-build/proj02-group7/build/_shared/chunk-J6FHCSRC.js"/><link rel="modulepreload" href="/user/jcollins36855/myst-build/proj02-group7/build/_shared/chunk-S4SWV34C.js"/><link rel="modulepreload" href="/user/jcollins36855/myst-build/proj02-group7/build/_shared/chunk-GUCIBHGO.js"/><link rel="modulepreload" href="/user/jcollins36855/myst-build/proj02-group7/build/root-7TUVC4ZT.js"/><link rel="modulepreload" href="/user/jcollins36855/myst-build/proj02-group7/build/_shared/chunk-INOWNUZ6.js"/><link rel="modulepreload" href="/user/jcollins36855/myst-build/proj02-group7/build/routes/$-P6PGXPYX.js"/><script>window.__remixContext = {"url":"/nlp-p02","state":{"loaderData":{"root":{"config":{"version":2,"myst":"1.6.3","options":{},"nav":[],"actions":[],"projects":[{"title":"Stat159 Project 2 - Reproducibility in Natural Language Processing","authors":[{"nameParsed":{"literal":"Reily Fairchild","given":"Reily","family":"Fairchild"},"name":"Reily Fairchild","affiliations":["UC Berkeley"],"id":"contributors-myst-generated-uid-0"},{"nameParsed":{"literal":"Atiila Joselyn Birah Kharobo","given":"Atiila Joselyn Birah","family":"Kharobo"},"name":"Atiila Joselyn Birah Kharobo","affiliations":["UC Berkeley"],"id":"contributors-myst-generated-uid-1"},{"nameParsed":{"literal":"Jordan Elizabeth Collins","given":"Jordan Elizabeth","family":"Collins"},"name":"Jordan Elizabeth Collins","affiliations":["UC Berkeley"],"id":"contributors-myst-generated-uid-2"},{"nameParsed":{"literal":"Aditya Jagannadha Sai Mangalampalli","given":"Aditya Jagannadha Sai","family":"Mangalampalli"},"name":"Aditya Jagannadha Sai Mangalampalli","affiliations":["UC Berkeley"],"id":"contributors-myst-generated-uid-3"}],"github":"https://github.com/UCB-stat-159-f25/proj02-group7","affiliations":[{"id":"UC Berkeley","name":"UC Berkeley"}],"id":"a74ecdef-c9eb-4683-b555-417faf7e975c","toc":[{"file":"index.md"},{"file":"contributions.md"},{"file":"nlp-P01.ipynb"},{"file":"nlp-P02.ipynb"},{"file":"nlp-P03.ipynb"},{"file":"nlp-P04.ipynb"},{"file":"proj02-nlp.ipynb"},{"file":"project-description.md"}],"thumbnail":"/user/jcollins36855/myst-build/proj02-group7/build/b77199e99a54e59b2e3c037c2cc90f21.svg","exports":[],"bibliography":[],"index":"index","pages":[{"slug":"contributions","title":"Contributions","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"nlp-p01","title":"Project 2: Reproducibility in Natural Language Processing","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"nlp-p02","title":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"nlp-p03","title":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"nlp-p04","title":"Part 4","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"proj02-nlp","title":"Project 2: Reproducibility in Natural Language Processing","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"project-description","title":"Project 2: Reproducibility in Natural Langauge Processing","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1}]}]},"CONTENT_CDN_PORT":"3100","MODE":"static","BASE_URL":"/user/jcollins36855/myst-build/proj02-group7"},"routes/$":{"config":{"version":2,"myst":"1.6.3","options":{},"nav":[],"actions":[],"projects":[{"title":"Stat159 Project 2 - Reproducibility in Natural Language Processing","authors":[{"nameParsed":{"literal":"Reily Fairchild","given":"Reily","family":"Fairchild"},"name":"Reily Fairchild","affiliations":["UC Berkeley"],"id":"contributors-myst-generated-uid-0"},{"nameParsed":{"literal":"Atiila Joselyn Birah Kharobo","given":"Atiila Joselyn Birah","family":"Kharobo"},"name":"Atiila Joselyn Birah Kharobo","affiliations":["UC Berkeley"],"id":"contributors-myst-generated-uid-1"},{"nameParsed":{"literal":"Jordan Elizabeth Collins","given":"Jordan Elizabeth","family":"Collins"},"name":"Jordan Elizabeth Collins","affiliations":["UC Berkeley"],"id":"contributors-myst-generated-uid-2"},{"nameParsed":{"literal":"Aditya Jagannadha Sai Mangalampalli","given":"Aditya Jagannadha Sai","family":"Mangalampalli"},"name":"Aditya Jagannadha Sai Mangalampalli","affiliations":["UC Berkeley"],"id":"contributors-myst-generated-uid-3"}],"github":"https://github.com/UCB-stat-159-f25/proj02-group7","affiliations":[{"id":"UC Berkeley","name":"UC Berkeley"}],"id":"a74ecdef-c9eb-4683-b555-417faf7e975c","toc":[{"file":"index.md"},{"file":"contributions.md"},{"file":"nlp-P01.ipynb"},{"file":"nlp-P02.ipynb"},{"file":"nlp-P03.ipynb"},{"file":"nlp-P04.ipynb"},{"file":"proj02-nlp.ipynb"},{"file":"project-description.md"}],"thumbnail":"/user/jcollins36855/myst-build/proj02-group7/build/b77199e99a54e59b2e3c037c2cc90f21.svg","exports":[],"bibliography":[],"index":"index","pages":[{"slug":"contributions","title":"Contributions","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"nlp-p01","title":"Project 2: Reproducibility in Natural Language Processing","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"nlp-p02","title":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"nlp-p03","title":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"nlp-p04","title":"Part 4","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"proj02-nlp","title":"Project 2: Reproducibility in Natural Language Processing","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"project-description","title":"Project 2: Reproducibility in Natural Langauge Processing","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1}]}]},"page":{"version":2,"kind":"Notebook","sha256":"e8b7221aac3e72348c43ec280bdefae25bddc58b52274ab183a7df0e9bab463c","slug":"nlp-p02","location":"/nlp-P02.ipynb","dependencies":[],"frontmatter":{"title":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","content_includes_title":true,"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"authors":[{"nameParsed":{"literal":"Reily Fairchild","given":"Reily","family":"Fairchild"},"name":"Reily Fairchild","affiliations":["UC Berkeley"],"id":"contributors-myst-generated-uid-0"},{"nameParsed":{"literal":"Atiila Joselyn Birah Kharobo","given":"Atiila Joselyn Birah","family":"Kharobo"},"name":"Atiila Joselyn Birah Kharobo","affiliations":["UC Berkeley"],"id":"contributors-myst-generated-uid-1"},{"nameParsed":{"literal":"Jordan Elizabeth Collins","given":"Jordan Elizabeth","family":"Collins"},"name":"Jordan Elizabeth Collins","affiliations":["UC Berkeley"],"id":"contributors-myst-generated-uid-2"},{"nameParsed":{"literal":"Aditya Jagannadha Sai Mangalampalli","given":"Aditya Jagannadha Sai","family":"Mangalampalli"},"name":"Aditya Jagannadha Sai Mangalampalli","affiliations":["UC Berkeley"],"id":"contributors-myst-generated-uid-3"}],"github":"https://github.com/UCB-stat-159-f25/proj02-group7","affiliations":[{"id":"UC Berkeley","name":"UC Berkeley"}],"source_url":"https://github.com/UCB-stat-159-f25/proj02-group7/blob/main/nlp-P02.ipynb","edit_url":"https://github.com/UCB-stat-159-f25/proj02-group7/edit/main/nlp-P02.ipynb","exports":[{"format":"ipynb","filename":"nlp-P02.ipynb","url":"/user/jcollins36855/myst-build/proj02-group7/build/nlp-P02-783a366036797526ca40cadaec3c9c8a.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":1,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zKYsFTXnQ6"}],"identifier":"part-2-simple-text-processing-tokenization-lemmatization-word-frequency-vectorization-20-pts","label":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","html_id":"part-2-simple-text-processing-tokenization-lemmatization-word-frequency-vectorization-20-pts","implicit":true,"key":"d3EXMUvghf"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Now we will start working on simple text processing using the ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"xs0Fv0VeZB"},{"type":"inlineCode","value":"SpaCy","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"TqFb00bpwx"},{"type":"text","value":" package and the same dataset as Part 1. The package should already be included in the ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"QTRL6DUVnT"},{"type":"inlineCode","value":"environment.yml","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"ADYrE1sFC1"},{"type":"text","value":". However, we will also need to download ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"HbchOZKhiE"},{"type":"inlineCode","value":"en_core_web_sm","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"VtLtkZKC9t"},{"type":"text","value":", an English language text processing model. To do this, while having your ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"iZeMlt0lTR"},{"type":"inlineCode","value":"sotu","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"qCSSi7wODA"},{"type":"text","value":" environment activated, run the following:","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"RaXrLTenJ3"}],"key":"ZTImklZalT"},{"type":"code","lang":"","value":"python -m spacy download en_core_web_sm","position":{"start":{"line":5,"column":1},"end":{"line":7,"column":1}},"key":"vih8L0wEJM"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Now, you should be good to go!","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"glCGiEHXpI"}],"key":"GZojzfDanp"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Some important definitions:","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"ZIL4SYDozk"}],"key":"fI2ESRHIOv"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":13,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"emphasis","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Token","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"mgbvCtmgWV"}],"key":"yauGCmgyoW"},{"type":"text","value":": a single word or piece of a word","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"F5bg9atgFm"}],"key":"L8Ki01Nq9C"}],"key":"cqyusXfVk4"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"emphasis","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"Lemma","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"QZYsLyqzTI"}],"key":"UFd8qIz87G"},{"type":"text","value":": the core component of a word, e.g., “complete” is the lemma for “completed” and “completely”","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"tyGGvWraeE"}],"key":"reIf4G301T"}],"key":"FtfpSSvK21"},{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","children":[{"type":"emphasis","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"Stop Word","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"zEdUPjAYbN"}],"key":"WoVMapvIhk"},{"type":"text","value":": a common word that does not add semantic value, such as “a”, “and”, “the”, etc.","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"XCI1JpBd9f"}],"key":"VVSmjysg3y"}],"key":"oT5E60KvsQ"},{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","children":[{"type":"emphasis","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"Vectorization","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"v7SbbuoBGn"}],"key":"FzApgxpKdZ"},{"type":"text","value":": representing a document as a vector where each index in the vector corresponds to a token or word and each entry is the count.","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"aIk8YxxcAU"}],"key":"H8ztBdB7fD"}],"key":"hEpCnwNKZv"}],"key":"lkLcKA8Zi9"},{"type":"paragraph","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"In this section, we will explore the most common tokens and lemmas throughout different slices of the speech data. We will also develop vectorization representations of the speeches.","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"TM9ikF0Dxx"}],"key":"Y1cH0m1MY6"},{"type":"paragraph","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"text","value":"The core steps are:","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"fUVE1Ez9VK"}],"key":"uni6PPMbRF"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":22,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Process speeches using the SpaCy nlp module","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"Vpm5zQj2qs"}],"key":"d4MF33J3HJ"}],"key":"cZnv1jQ4kE"},{"type":"listItem","spread":true,"position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Analyze Tokens vs Lemmas:","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"O37K9PEnPB"}],"key":"fpj8x44E6K"}],"key":"LHqJiAjM9Z"}],"key":"p75VVjN7Lz"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":24,"column":1},"end":{"line":26,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Create a list of all tokens across all speeches that are not stop words, punctuation, or spaces.","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"AFCSmHvE4Y"}],"key":"XQEsEboSFx"}],"key":"QQ6oCnAOVZ"},{"type":"listItem","spread":true,"position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Create a second list of the lemmas for these same tokens.","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"MGuKbGbQYh"}],"key":"w42Azrs7Db"}],"key":"JzobFw0PP1"},{"type":"listItem","spread":true,"position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Display the top 25 for each of these and compare.","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"key":"N0XzrmjKPO"}],"key":"jr79P3hz0u"}],"key":"KQlXAyGsca"}],"key":"ws4DCpOAde"},{"type":"list","ordered":true,"start":3,"spread":false,"position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Analyze common word distributions over different years:","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"lrE6LtHNt8"}],"key":"JGW2dSJX2B"}],"key":"uDKkGRlO1w"}],"key":"o4l1LQC5H4"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":28,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Create a function that takes the dataset and a year as an input and outputs the top n lemmas for that year’s speeches","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"oICoM7kEEd"}],"key":"SQ0PCbkZ7o"}],"key":"NIdChDClwt"},{"type":"listItem","spread":true,"position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Compare the top 10 words for 2023 versus 2019","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"De7tXAN0yD"}],"key":"QJBIIMRUr7"}],"key":"WCurwiomBt"}],"key":"tnRC8cXNex"},{"type":"list","ordered":true,"start":4,"spread":false,"position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Document Vectorization:","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"key":"RBNZuH2j3k"}],"key":"NabWClsxpG"}],"key":"iNsM4zVLse"}],"key":"ZySlrNrPoc"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":31,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Train a Term Frequency-Inverse Document Frequency (TF-IDF) vectorization model using your processed dataset and scikit learn","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"bzLxhJ1wQd"}],"key":"IHFM1gHBTY"}],"key":"ahWRZEVoJi"},{"type":"listItem","spread":true,"position":{"start":{"line":32,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Output the feature vectors","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"key":"U18Qs9wg9h"}],"key":"JaUe0oqL5g"}],"key":"m3fygY0VvQ"}],"key":"UyZ2pzjnu3"},{"type":"paragraph","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"strong","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"text","value":"Helpful Resources:","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"key":"oVAtECcfzA"}],"key":"ci9VPXBjcj"}],"key":"qdfN7TLhPg"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":35,"column":1},"end":{"line":39,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"paragraph","children":[{"type":"link","url":"https://realpython.com/natural-language-processing-spacy-python/","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"text","value":"https://​realpython​.com​/natural​-language​-processing​-spacy​-python/","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"y7ENpN8XEv"}],"urlSource":"https://realpython.com/natural-language-processing-spacy-python/","key":"z8VbdRGQhd"}],"key":"h4WKnJrvhM"}],"key":"wrg3ofhovN"},{"type":"listItem","spread":true,"position":{"start":{"line":36,"column":1},"end":{"line":36,"column":1}},"children":[{"type":"paragraph","children":[{"type":"link","url":"https://www.statology.org/text-preprocessing-feature-engineering-spacy/","position":{"start":{"line":36,"column":1},"end":{"line":36,"column":1}},"children":[{"type":"text","value":"https://​www​.statology​.org​/text​-preprocessing​-feature​-engineering​-spacy/","position":{"start":{"line":36,"column":1},"end":{"line":36,"column":1}},"key":"zyLEtNnA3R"}],"urlSource":"https://www.statology.org/text-preprocessing-feature-engineering-spacy/","key":"RYKzaDMi2P"}],"key":"ewk9jEr03W"}],"key":"q8S8k7oQRS"},{"type":"listItem","spread":true,"position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"paragraph","children":[{"type":"link","url":"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"https://​scikit​-learn​.org​/stable​/modules​/generated​/sklearn​.feature​_extraction​.text​.TfidfVectorizer​.html#","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"tqthksxakp"}],"urlSource":"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#","key":"ahmvNl4qrd"}],"key":"mQ4g4DdWHM"}],"key":"siFS2kX4vm"},{"type":"listItem","spread":true,"position":{"start":{"line":38,"column":1},"end":{"line":39,"column":1}},"children":[{"type":"paragraph","children":[{"type":"link","url":"https://www.geeksforgeeks.org/nlp/how-to-store-a-tfidfvectorizer-for-future-use-in-scikit-learn/","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"children":[{"type":"text","value":"https://​www​.geeksforgeeks​.org​/nlp​/how​-to​-store​-a​-tfidfvectorizer​-for​-future​-use​-in​-scikit​-learn/","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"key":"NZ9jyLnfQU"}],"urlSource":"https://www.geeksforgeeks.org/nlp/how-to-store-a-tfidfvectorizer-for-future-use-in-scikit-learn/","key":"TAdfLmRIZL"}],"key":"vnT3YWtCQb"}],"key":"TLkF2Uo7r8"}],"key":"iXCsBU4emB"}],"key":"kzPN2nhyyU"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Processing Speeches with SpaCy","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pOvW3zmwc7"}],"identifier":"processing-speeches-with-spacy","label":"Processing Speeches with SpaCy","html_id":"processing-speeches-with-spacy","implicit":true,"key":"xVrBvyneEX"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Lets study just speeches from 2000 and onwards to begin with. So, be sure to subset your DataFrame to just these speeches before continuing!","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"T82vNGFGnO"}],"key":"fgQOnCgdzN"}],"key":"hAv4WWMK5m"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"!pip install spacy","key":"ppcZ4sVOx4"},{"type":"output","id":"fWzqeZdU3actGolF2YiKM","data":[{"name":"stdout","output_type":"stream","text":"Requirement already satisfied: spacy in /srv/conda/envs/notebook/lib/python3.12/site-packages (3.8.11)\nRequirement already satisfied: spacy-legacy\u003c3.1.0,\u003e=3.0.11 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (3.0.12)\nRequirement already satisfied: spacy-loggers\u003c2.0.0,\u003e=1.0.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (1.0.5)\nRequirement already satisfied: murmurhash\u003c1.1.0,\u003e=0.28.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (1.0.15)\nRequirement already satisfied: cymem\u003c2.1.0,\u003e=2.0.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.0.13)\nRequirement already satisfied: preshed\u003c3.1.0,\u003e=3.0.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (3.0.12)\nRequirement already satisfied: thinc\u003c8.4.0,\u003e=8.3.4 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (8.3.10)\nRequirement already satisfied: wasabi\u003c1.2.0,\u003e=0.9.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (1.1.3)\nRequirement already satisfied: srsly\u003c3.0.0,\u003e=2.4.3 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.5.2)\nRequirement already satisfied: catalogue\u003c2.1.0,\u003e=2.0.6 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.0.10)\nRequirement already satisfied: weasel\u003c0.5.0,\u003e=0.4.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (0.4.3)\nRequirement already satisfied: typer-slim\u003c1.0.0,\u003e=0.3.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (0.20.0)\nRequirement already satisfied: tqdm\u003c5.0.0,\u003e=4.38.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (4.67.1)\nRequirement already satisfied: numpy\u003e=1.19.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (1.26.4)\nRequirement already satisfied: requests\u003c3.0.0,\u003e=2.13.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,\u003c3.0.0,\u003e=1.7.4 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.12.3)\nRequirement already satisfied: jinja2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (3.1.6)\nRequirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (70.0.0)\nRequirement already satisfied: packaging\u003e=20.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (24.0)\nRequirement already satisfied: annotated-types\u003e=0.6.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,\u003c3.0.0,\u003e=1.7.4-\u003espacy) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.4 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,\u003c3.0.0,\u003e=1.7.4-\u003espacy) (2.41.4)\nRequirement already satisfied: typing-extensions\u003e=4.14.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,\u003c3.0.0,\u003e=1.7.4-\u003espacy) (4.15.0)\nRequirement already satisfied: typing-inspection\u003e=0.4.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,\u003c3.0.0,\u003e=1.7.4-\u003espacy) (0.4.2)\nRequirement already satisfied: charset_normalizer\u003c4,\u003e=2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy) (3.3.2)\nRequirement already satisfied: idna\u003c4,\u003e=2.5 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy) (3.7)\nRequirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy) (1.26.20)\nRequirement already satisfied: certifi\u003e=2017.4.17 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy) (2025.10.5)\nRequirement already satisfied: blis\u003c1.4.0,\u003e=1.3.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from thinc\u003c8.4.0,\u003e=8.3.4-\u003espacy) (1.3.3)\nRequirement already satisfied: confection\u003c1.0.0,\u003e=0.0.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from thinc\u003c8.4.0,\u003e=8.3.4-\u003espacy) (0.1.5)\nRequirement already satisfied: click\u003e=8.0.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from typer-slim\u003c1.0.0,\u003e=0.3.0-\u003espacy) (8.3.0)\nRequirement already satisfied: cloudpathlib\u003c1.0.0,\u003e=0.7.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from weasel\u003c0.5.0,\u003e=0.4.2-\u003espacy) (0.23.0)\nRequirement already satisfied: smart-open\u003c8.0.0,\u003e=5.2.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from weasel\u003c0.5.0,\u003e=0.4.2-\u003espacy) (7.5.0)\nRequirement already satisfied: wrapt in /srv/conda/envs/notebook/lib/python3.12/site-packages (from smart-open\u003c8.0.0,\u003e=5.2.1-\u003eweasel\u003c0.5.0,\u003e=0.4.2-\u003espacy) (1.17.3)\nRequirement already satisfied: MarkupSafe\u003e=2.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from jinja2-\u003espacy) (3.0.2)\n"}],"key":"OhCsAmtjYC"}],"key":"UinbthcDEo"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import spacy\n!python -m spacy download en_core_web_sm\nspacy.cli.download(\"en_core_web_sm\")\n\nfrom tqdm import tqdm\nfrom collections import Counter\nimport pandas as pd\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsou = pd.read_csv('data/SOTU.csv')\nnlp = spacy.load(\"en_core_web_sm\")","key":"zXADYuYKqS"},{"type":"output","id":"-4ozTqHXZ9Lum5TqQvjhJ","data":[{"name":"stdout","output_type":"stream","text":"Collecting en-core-web-sm==3.8.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\nCollecting en-core-web-sm==3.8.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\n"}],"key":"yCRJfCSfSA"}],"key":"cjue563K2g"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"sou","key":"uA3amv1YDu"},{"type":"output","id":"HcOOzz0s_e12jOSNrJg_r","data":[{"output_type":"execute_result","execution_count":3,"metadata":{},"data":{"text/html":{"content":"\u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003ctable border=\"1\" class=\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style=\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003ePresident\u003c/th\u003e\n      \u003cth\u003eYear\u003c/th\u003e\n      \u003cth\u003eText\u003c/th\u003e\n      \u003cth\u003eWord Count\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e0\u003c/th\u003e\n      \u003ctd\u003eJoseph R. Biden\u003c/td\u003e\n      \u003ctd\u003e2024.0\u003c/td\u003e\n      \u003ctd\u003e\\n[Before speaking, the President presented hi...\u003c/td\u003e\n      \u003ctd\u003e8003\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e1\u003c/th\u003e\n      \u003ctd\u003eJoseph R. Biden\u003c/td\u003e\n      \u003ctd\u003e2023.0\u003c/td\u003e\n      \u003ctd\u003e\\nThe President. Mr. Speaker——\\n[At this point...\u003c/td\u003e\n      \u003ctd\u003e8978\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e2\u003c/th\u003e\n      \u003ctd\u003eJoseph R. Biden\u003c/td\u003e\n      \u003ctd\u003e2022.0\u003c/td\u003e\n      \u003ctd\u003e\\nThe President. Thank you all very, very much...\u003c/td\u003e\n      \u003ctd\u003e7539\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e3\u003c/th\u003e\n      \u003ctd\u003eJoseph R. Biden\u003c/td\u003e\n      \u003ctd\u003e2021.0\u003c/td\u003e\n      \u003ctd\u003e\\nThe President. Thank you. Thank you. Thank y...\u003c/td\u003e\n      \u003ctd\u003e7734\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e4\u003c/th\u003e\n      \u003ctd\u003eDonald J. Trump\u003c/td\u003e\n      \u003ctd\u003e2020.0\u003c/td\u003e\n      \u003ctd\u003e\\nThe President. Thank you very much. Thank yo...\u003c/td\u003e\n      \u003ctd\u003e6169\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e...\u003c/th\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e241\u003c/th\u003e\n      \u003ctd\u003eGeorge Washington\u003c/td\u003e\n      \u003ctd\u003e1791.0\u003c/td\u003e\n      \u003ctd\u003e\\nFellow-Citizens of the Senate and House of R...\u003c/td\u003e\n      \u003ctd\u003e2264\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e242\u003c/th\u003e\n      \u003ctd\u003eGeorge Washington\u003c/td\u003e\n      \u003ctd\u003e1790.0\u003c/td\u003e\n      \u003ctd\u003e\\nFellow-Citizens of the Senate and House of R...\u003c/td\u003e\n      \u003ctd\u003e1069\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e243\u003c/th\u003e\n      \u003ctd\u003eGeorge Washington\u003c/td\u003e\n      \u003ctd\u003e1790.0\u003c/td\u003e\n      \u003ctd\u003e\\nFellow-Citizens of the Senate and House of R...\u003c/td\u003e\n      \u003ctd\u003e1069\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e244\u003c/th\u003e\n      \u003ctd\u003eGeorge Washington\u003c/td\u003e\n      \u003ctd\u003e1790.0\u003c/td\u003e\n      \u003ctd\u003e\\nFellow-Citizens of the Senate and House of R...\u003c/td\u003e\n      \u003ctd\u003e1069\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e245\u003c/th\u003e\n      \u003ctd\u003eGeorge Washington\u003c/td\u003e\n      \u003ctd\u003e1790.0\u003c/td\u003e\n      \u003ctd\u003e\\nFellow-Citizens of the Senate and House of R...\u003c/td\u003e\n      \u003ctd\u003e1069\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e246 rows × 4 columns\u003c/p\u003e\n\u003c/div\u003e","content_type":"text/html"},"text/plain":{"content":"             President    Year  \\\n0      Joseph R. Biden  2024.0   \n1      Joseph R. Biden  2023.0   \n2      Joseph R. Biden  2022.0   \n3      Joseph R. Biden  2021.0   \n4      Donald J. Trump  2020.0   \n..                 ...     ...   \n241  George Washington  1791.0   \n242  George Washington  1790.0   \n243  George Washington  1790.0   \n244  George Washington  1790.0   \n245  George Washington  1790.0   \n\n                                                  Text  Word Count  \n0    \\n[Before speaking, the President presented hi...        8003  \n1    \\nThe President. Mr. Speaker——\\n[At this point...        8978  \n2    \\nThe President. Thank you all very, very much...        7539  \n3    \\nThe President. Thank you. Thank you. Thank y...        7734  \n4    \\nThe President. Thank you very much. Thank yo...        6169  \n..                                                 ...         ...  \n241  \\nFellow-Citizens of the Senate and House of R...        2264  \n242  \\nFellow-Citizens of the Senate and House of R...        1069  \n243  \\nFellow-Citizens of the Senate and House of R...        1069  \n244  \\nFellow-Citizens of the Senate and House of R...        1069  \n245  \\nFellow-Citizens of the Senate and House of R...        1069  \n\n[246 rows x 4 columns]","content_type":"text/plain"}}}],"key":"xIKP21CUlK"}],"key":"GdffQC24GD"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# subset the speech dataframe for speeches from 2000 and onwards\nsou_new = sou[sou['Year'] \u003e= 2000]\nsou_new","key":"fqGZLm7wQx"},{"type":"output","id":"m6LSaOHKeJ8iJGcAcdMSZ","data":[{"output_type":"execute_result","execution_count":4,"metadata":{},"data":{"text/html":{"content":"\u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003ctable border=\"1\" class=\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style=\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003ePresident\u003c/th\u003e\n      \u003cth\u003eYear\u003c/th\u003e\n      \u003cth\u003eText\u003c/th\u003e\n      \u003cth\u003eWord Count\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e0\u003c/th\u003e\n      \u003ctd\u003eJoseph R. Biden\u003c/td\u003e\n      \u003ctd\u003e2024.0\u003c/td\u003e\n      \u003ctd\u003e\\n[Before speaking, the President presented hi...\u003c/td\u003e\n      \u003ctd\u003e8003\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e1\u003c/th\u003e\n      \u003ctd\u003eJoseph R. Biden\u003c/td\u003e\n      \u003ctd\u003e2023.0\u003c/td\u003e\n      \u003ctd\u003e\\nThe President. Mr. Speaker——\\n[At this point...\u003c/td\u003e\n      \u003ctd\u003e8978\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e2\u003c/th\u003e\n      \u003ctd\u003eJoseph R. Biden\u003c/td\u003e\n      \u003ctd\u003e2022.0\u003c/td\u003e\n      \u003ctd\u003e\\nThe President. Thank you all very, very much...\u003c/td\u003e\n      \u003ctd\u003e7539\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e3\u003c/th\u003e\n      \u003ctd\u003eJoseph R. Biden\u003c/td\u003e\n      \u003ctd\u003e2021.0\u003c/td\u003e\n      \u003ctd\u003e\\nThe President. Thank you. Thank you. Thank y...\u003c/td\u003e\n      \u003ctd\u003e7734\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e4\u003c/th\u003e\n      \u003ctd\u003eDonald J. Trump\u003c/td\u003e\n      \u003ctd\u003e2020.0\u003c/td\u003e\n      \u003ctd\u003e\\nThe President. Thank you very much. Thank yo...\u003c/td\u003e\n      \u003ctd\u003e6169\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e5\u003c/th\u003e\n      \u003ctd\u003eDonald J. Trump\u003c/td\u003e\n      \u003ctd\u003e2019.0\u003c/td\u003e\n      \u003ctd\u003e\\nThe President. Madam Speaker, Mr. Vice Presi...\u003c/td\u003e\n      \u003ctd\u003e5519\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e6\u003c/th\u003e\n      \u003ctd\u003eDonald J. Trump\u003c/td\u003e\n      \u003ctd\u003e2018.0\u003c/td\u003e\n      \u003ctd\u003e\\nThe President. Mr. Speaker, Mr. Vice Preside...\u003c/td\u003e\n      \u003ctd\u003e5755\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e7\u003c/th\u003e\n      \u003ctd\u003eDonald J. Trump\u003c/td\u003e\n      \u003ctd\u003e2017.0\u003c/td\u003e\n      \u003ctd\u003e\\nThank you very much. Mr. Speaker, Mr. Vice P...\u003c/td\u003e\n      \u003ctd\u003e4903\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e8\u003c/th\u003e\n      \u003ctd\u003eBarack Obama\u003c/td\u003e\n      \u003ctd\u003e2016.0\u003c/td\u003e\n      \u003ctd\u003e\\nThank you. Mr. Speaker, Mr. Vice President, ...\u003c/td\u003e\n      \u003ctd\u003e5956\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e9\u003c/th\u003e\n      \u003ctd\u003eBarack Obama\u003c/td\u003e\n      \u003ctd\u003e2015.0\u003c/td\u003e\n      \u003ctd\u003e\\nThe President. Mr. Speaker, Mr. Vice Preside...\u003c/td\u003e\n      \u003ctd\u003e6659\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e10\u003c/th\u003e\n      \u003ctd\u003eBarack Obama\u003c/td\u003e\n      \u003ctd\u003e2014.0\u003c/td\u003e\n      \u003ctd\u003e\\nThe President. Mr. Speaker, Mr. Vice Preside...\u003c/td\u003e\n      \u003ctd\u003e6904\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e11\u003c/th\u003e\n      \u003ctd\u003eBarack Obama\u003c/td\u003e\n      \u003ctd\u003e2013.0\u003c/td\u003e\n      \u003ctd\u003e\\nPlease, everybody, have a seat. Mr. Speaker,...\u003c/td\u003e\n      \u003ctd\u003e6692\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e12\u003c/th\u003e\n      \u003ctd\u003eBarack Obama\u003c/td\u003e\n      \u003ctd\u003e2012.0\u003c/td\u003e\n      \u003ctd\u003e\\nMr. Speaker, Mr. Vice President, Members of ...\u003c/td\u003e\n      \u003ctd\u003e6915\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e13\u003c/th\u003e\n      \u003ctd\u003eBarack Obama\u003c/td\u003e\n      \u003ctd\u003e2011.0\u003c/td\u003e\n      \u003ctd\u003e\\nMr. Speaker, Mr. Vice President, Members of ...\u003c/td\u003e\n      \u003ctd\u003e6758\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e14\u003c/th\u003e\n      \u003ctd\u003eBarack Obama\u003c/td\u003e\n      \u003ctd\u003e2010.0\u003c/td\u003e\n      \u003ctd\u003e\\nMadam Speaker, Vice President Biden, Members...\u003c/td\u003e\n      \u003ctd\u003e7129\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e15\u003c/th\u003e\n      \u003ctd\u003eBarack Obama\u003c/td\u003e\n      \u003ctd\u003e2009.0\u003c/td\u003e\n      \u003ctd\u003e\\nMadam Speaker, Mr. Vice President, Members o...\u003c/td\u003e\n      \u003ctd\u003e5979\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e16\u003c/th\u003e\n      \u003ctd\u003eGeorge W. Bush\u003c/td\u003e\n      \u003ctd\u003e2008.0\u003c/td\u003e\n      \u003ctd\u003e\\nThank you all. Madam Speaker, Vice President...\u003c/td\u003e\n      \u003ctd\u003e5635\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e17\u003c/th\u003e\n      \u003ctd\u003eGeorge W. Bush\u003c/td\u003e\n      \u003ctd\u003e2007.0\u003c/td\u003e\n      \u003ctd\u003e\\nThank you very much. And tonight I have the ...\u003c/td\u003e\n      \u003ctd\u003e5491\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e18\u003c/th\u003e\n      \u003ctd\u003eGeorge W. Bush\u003c/td\u003e\n      \u003ctd\u003e2006.0\u003c/td\u003e\n      \u003ctd\u003e\\nThank you all. Mr. Speaker, Vice President C...\u003c/td\u003e\n      \u003ctd\u003e5218\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e19\u003c/th\u003e\n      \u003ctd\u003eGeorge W. Bush\u003c/td\u003e\n      \u003ctd\u003e2005.0\u003c/td\u003e\n      \u003ctd\u003e\\nMr. Speaker, Vice President Cheney, Members ...\u003c/td\u003e\n      \u003ctd\u003e4987\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e20\u003c/th\u003e\n      \u003ctd\u003eGeorge W. Bush\u003c/td\u003e\n      \u003ctd\u003e2004.0\u003c/td\u003e\n      \u003ctd\u003e\\nMr. Speaker, Vice President Cheney, Members ...\u003c/td\u003e\n      \u003ctd\u003e5100\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e21\u003c/th\u003e\n      \u003ctd\u003eGeorge W. Bush\u003c/td\u003e\n      \u003ctd\u003e2003.0\u003c/td\u003e\n      \u003ctd\u003e\\nMr. Speaker, Vice President Cheney, Members ...\u003c/td\u003e\n      \u003ctd\u003e5288\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e22\u003c/th\u003e\n      \u003ctd\u003eGeorge W. Bush\u003c/td\u003e\n      \u003ctd\u003e2002.0\u003c/td\u003e\n      \u003ctd\u003e\\nThank you very much. Mr. Speaker, Vice Presi...\u003c/td\u003e\n      \u003ctd\u003e3762\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e23\u003c/th\u003e\n      \u003ctd\u003eGeorge W. Bush\u003c/td\u003e\n      \u003ctd\u003e2001.0\u003c/td\u003e\n      \u003ctd\u003e\\nMr. Speaker, Mr. Vice President, Members of ...\u003c/td\u003e\n      \u003ctd\u003e4293\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e24\u003c/th\u003e\n      \u003ctd\u003eWilliam J. Clinton\u003c/td\u003e\n      \u003ctd\u003e2000.0\u003c/td\u003e\n      \u003ctd\u003e\\nMr. Speaker, Mr. Vice President, Members of ...\u003c/td\u003e\n      \u003ctd\u003e8925\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e","content_type":"text/html"},"text/plain":{"content":"             President    Year  \\\n0      Joseph R. Biden  2024.0   \n1      Joseph R. Biden  2023.0   \n2      Joseph R. Biden  2022.0   \n3      Joseph R. Biden  2021.0   \n4      Donald J. Trump  2020.0   \n5      Donald J. Trump  2019.0   \n6      Donald J. Trump  2018.0   \n7      Donald J. Trump  2017.0   \n8         Barack Obama  2016.0   \n9         Barack Obama  2015.0   \n10        Barack Obama  2014.0   \n11        Barack Obama  2013.0   \n12        Barack Obama  2012.0   \n13        Barack Obama  2011.0   \n14        Barack Obama  2010.0   \n15        Barack Obama  2009.0   \n16      George W. Bush  2008.0   \n17      George W. Bush  2007.0   \n18      George W. Bush  2006.0   \n19      George W. Bush  2005.0   \n20      George W. Bush  2004.0   \n21      George W. Bush  2003.0   \n22      George W. Bush  2002.0   \n23      George W. Bush  2001.0   \n24  William J. Clinton  2000.0   \n\n                                                 Text  Word Count  \n0   \\n[Before speaking, the President presented hi...        8003  \n1   \\nThe President. Mr. Speaker——\\n[At this point...        8978  \n2   \\nThe President. Thank you all very, very much...        7539  \n3   \\nThe President. Thank you. Thank you. Thank y...        7734  \n4   \\nThe President. Thank you very much. Thank yo...        6169  \n5   \\nThe President. Madam Speaker, Mr. Vice Presi...        5519  \n6   \\nThe President. Mr. Speaker, Mr. Vice Preside...        5755  \n7   \\nThank you very much. Mr. Speaker, Mr. Vice P...        4903  \n8   \\nThank you. Mr. Speaker, Mr. Vice President, ...        5956  \n9   \\nThe President. Mr. Speaker, Mr. Vice Preside...        6659  \n10  \\nThe President. Mr. Speaker, Mr. Vice Preside...        6904  \n11  \\nPlease, everybody, have a seat. Mr. Speaker,...        6692  \n12  \\nMr. Speaker, Mr. Vice President, Members of ...        6915  \n13  \\nMr. Speaker, Mr. Vice President, Members of ...        6758  \n14  \\nMadam Speaker, Vice President Biden, Members...        7129  \n15  \\nMadam Speaker, Mr. Vice President, Members o...        5979  \n16  \\nThank you all. Madam Speaker, Vice President...        5635  \n17  \\nThank you very much. And tonight I have the ...        5491  \n18  \\nThank you all. Mr. Speaker, Vice President C...        5218  \n19  \\nMr. Speaker, Vice President Cheney, Members ...        4987  \n20  \\nMr. Speaker, Vice President Cheney, Members ...        5100  \n21  \\nMr. Speaker, Vice President Cheney, Members ...        5288  \n22  \\nThank you very much. Mr. Speaker, Vice Presi...        3762  \n23  \\nMr. Speaker, Mr. Vice President, Members of ...        4293  \n24  \\nMr. Speaker, Mr. Vice President, Members of ...        8925  ","content_type":"text/plain"}}}],"key":"WBvvO8NI0n"}],"key":"lkcPPgzD6y"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Process each speeches using the 'nlp' function\nprocessed_speeches = []\n\nfor i in range(len(sou_new)):\n    speech_text = sou_new.loc[i, 'Text'] \n    processed = nlp(speech_text)    \n    processed_speeches.append(processed)","key":"YGb9kLepmB"},{"type":"output","id":"zZVwDprrIut9jKJM7LUVw","data":[],"key":"OrjjoL6eq5"}],"key":"RQMYGuJElA"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Analyze Tokens vs Lemmas","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gYEWF6qET7"}],"identifier":"analyze-tokens-vs-lemmas","label":"Analyze Tokens vs Lemmas","html_id":"analyze-tokens-vs-lemmas","implicit":true,"key":"MhiALEjpAd"},{"type":"heading","depth":3,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Token List","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"RX7r3YaAyY"}],"identifier":"token-list","label":"Token List","html_id":"token-list","implicit":true,"key":"UoNlDxsj5j"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Create a list of tokens across all speeches that are not spaces, stopwords, or punctuation. Make each token lowercase as well. ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"TkwK0G9s8i"},{"type":"emphasis","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Hint: each element of the list we just created are themselves lists of tokens. Token objects have attributes ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"QWwsKghty8"},{"type":"inlineCode","value":"is_stop","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"V8tCV94Mib"},{"type":"text","value":", ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"mvQtEsCK6t"},{"type":"inlineCode","value":"is_punct","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"LbCYkWfWhK"},{"type":"text","value":", and ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"u8QQbsBk87"},{"type":"inlineCode","value":"is_space","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"kb6mETOY7W"},{"type":"text","value":".","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"sgbAZytXSI"}],"key":"nb1dlh2YQf"}],"key":"ZhJ3sHckFX"}],"key":"Kk3nkgQK0c"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"tokens = []\n\nfor speech in processed_speeches:\n    for token in speech:\n        if not token.is_punct and not token.is_space and not token.is_stop:\n            tokens.append(token.text)\n    ","key":"f6ScpuyTmj"},{"type":"output","id":"qQVk68j3cdAb3-V7rzZHp","data":[],"key":"JwBzXU2E70"}],"key":"lTfLgCi0Md"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# print top 20 tokens\n# Hint - use Counter, and one of the Counter object's methods to display the top 20\ntoken_count = Counter(tokens).most_common(20)\nprint(token_count)","key":"hdBJeaCvNW"},{"type":"output","id":"soS0B21ZflnM44EZfXR8a","data":[{"name":"stdout","output_type":"stream","text":"[('America', 816), ('people', 622), ('American', 582), ('new', 495), ('years', 439), ('Americans', 437), ('world', 409), ('year', 401), ('country', 369), ('jobs', 325), ('work', 324), ('know', 323), ('Congress', 317), ('time', 297), ('help', 278), ('need', 266), ('tonight', 253), ('President', 246), ('economy', 243), ('want', 237)]\n"}],"key":"skOSXv3gSb"}],"key":"q3kQWEed62"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Lemma List","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"T70fTMhVax"}],"identifier":"lemma-list","label":"Lemma List","html_id":"lemma-list","implicit":true,"key":"fVtH99mrdf"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Do the same as above, but for lemmas. ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"kkr3pUimzq"},{"type":"emphasis","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Hint: recall lemmas are components of words. Each token should have an attribute to extract the lemma.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"NlUIEFEp2x"}],"key":"XUnfNHBz7Y"}],"key":"AEexronuoc"}],"key":"z7p38PeDnP"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"lemmas = []\n\nfor token in tokens:\n    word = nlp(token) # run the tokens through nlp to make sure it has the lemma_ attribute\n    for token in word:\n        lemma=token.lemma_\n        lemmas.append(lemma.lower())","key":"Qq6kjFJifZ"},{"type":"output","id":"0VeGZkXGKoGgpT_huk6Y3","data":[],"key":"N2GlDoV8bJ"}],"key":"UTwPR2d4Ea"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# print top 20 lemmas\nlemma_counts = Counter(lemmas).most_common(20)\nlemma_counts","key":"uRU5PcNCS9"},{"type":"output","id":"GMT6nsejCnxDvJpTC0mpf","data":[{"output_type":"execute_result","execution_count":9,"metadata":{},"data":{"text/plain":{"content":"[('american', 1019),\n ('year', 845),\n ('america', 821),\n ('people', 639),\n ('work', 566),\n ('new', 532),\n ('job', 503),\n ('country', 435),\n ('world', 426),\n ('nation', 402),\n ('know', 396),\n ('help', 387),\n ('need', 353),\n ('time', 351),\n ('tonight', 344),\n ('child', 334),\n ('state', 326),\n ('let', 326),\n ('congress', 319),\n ('family', 303)]","content_type":"text/plain"}}}],"key":"pOWlDhS3cU"}],"key":"zbatsChg27"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Token versus Lemma Comparison","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"J0omblV7SY"}],"identifier":"token-versus-lemma-comparison","label":"Token versus Lemma Comparison","html_id":"token-versus-lemma-comparison","implicit":true,"key":"tHaVrd8Bzi"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"What do you notice about the top tokens versus the top lemmas?\nConsider two tokens - “year” and “years” - how do their counts compare to the lemma “year”?\nWhat about the lemma “child”?","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"dS8yYirTIB"}],"key":"wQbFFklI0e"}],"key":"xCytpTUp7j"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Common Words","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ytzO1pQXni"}],"identifier":"common-words","label":"Common Words","html_id":"common-words","implicit":true,"key":"FvmBotdolj"},{"type":"heading","depth":3,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Common Words per Year Function","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"EA8zlPcIIy"}],"identifier":"common-words-per-year-function","label":"Common Words per Year Function","html_id":"common-words-per-year-function","implicit":true,"key":"DJJSeibi8F"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Fill in the below function to obtain the n-most common words in speeches for a given year.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"ju4d7RswWZ"}],"key":"bqJh5Kb4QR"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"inputs:","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"UYns46assT"}],"key":"qgmFBotK9D"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":8,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"df raw unprocessed sou dataframe","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"oISuBxOodY"}],"key":"nKc4Pe8L69"}],"key":"VWdeXjHHqk"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"year","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"PSTEKzQdO7"}],"key":"IBppJLtCXG"}],"key":"P8gIufDaQL"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"n\noutputs:","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"WrLC5fWa4E"}],"key":"ATm8VKVQnx"}],"key":"Tv5N5ynOlA"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"top n words for that years","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"U7jIkYscVV"}],"key":"u8SLgkwlDU"}],"key":"n61Ch6PfVT"}],"key":"qOUwfq9BUG"},{"type":"paragraph","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"steps:","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"RbBMMW4D9E"}],"key":"wPEeCKiugT"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":15,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"subset the dataframe for the year of interest - note the years might not be in int type","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"oQ2TQg8Fiq"}],"key":"j9sLxayg2G"}],"key":"oox3kH0DAa"},{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"process the subsetted dataframe with spacy","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"CjLhDRqraf"}],"key":"E2S4US5w6e"}],"key":"Gn8FDrhHtM"},{"type":"listItem","spread":true,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"get the lemmas across all those speeches","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"RK3lNjyte6"}],"key":"zIYLF5N3NZ"}],"key":"KlxRutsehw"},{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"count the top n lemmas","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"FddppAkMg6"}],"key":"qRClCUSZlW"}],"key":"amHmQRtw1w"}],"key":"gCYH1O4Lha"}],"key":"bU9HFEen2v"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def get_most_common_words(df, year, n=25):\n    \"\"\"\n    Processes the SOTU speech for a given year and returns\n    the most common non-stopword/punctuation lemmas.\n\n    INPUTS\n    - raw unprocessed sou data frame\n    - year\n\n    OUTPUTS\n    - top n words for that year\n    \"\"\"\n\n    # Step 1: Subset df\n    new_df = df[df['Year'] == year]\n    \n    # Step 2: Process the text with spaCy\n    new_df = df[df['Year'] == year].reset_index(drop=True) \n    processed_speeches = []\n\n    for i in range(len(new_df)):\n        speech_text = new_df.loc[i, 'Text'] \n        processed = nlp(speech_text)    \n        processed_speeches.append(processed)\n    \n    # Step 3: Get lemmas\n    lemmas = []\n\n    for doc in processed_speeches:\n        for token in doc:\n            if not token.is_punct and not token.is_space and not token.is_stop:\n                lemmas.append(token.lemma_.lower())\n    \n    lemma_counts = Counter(lemmas).most_common(n)\n    \n    return lemma_counts","key":"VgvcGXIRz3"},{"type":"output","id":"26zhvxkWtEHbuuQ97dmp5","data":[],"key":"keOwsREMqW"}],"key":"u8vmUNlSVJ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# test it on 2024\nget_most_common_words(sou, 2024, n=20)","key":"W8pa02mYCd"},{"type":"output","id":"xjSi7tPhzQDdIEFZ29JsP","data":[{"output_type":"execute_result","execution_count":11,"metadata":{},"data":{"text/plain":{"content":"[('president', 58),\n ('year', 45),\n ('america', 44),\n ('american', 34),\n ('people', 33),\n ('$', 33),\n ('member', 32),\n ('want', 29),\n ('audience', 29),\n ('know', 29),\n ('pay', 29),\n ('come', 26),\n ('home', 25),\n ('family', 24),\n ('future', 23),\n ('million', 23),\n ('like', 21),\n ('build', 21),\n ('laughter', 20),\n ('americans', 20)]","content_type":"text/plain"}}}],"key":"JyhpnSrqq2"}],"key":"YxwtX0SKXu"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Compare 2023 to 2017","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qH3llXqH46"}],"identifier":"compare-2023-to-2017","label":"Compare 2023 to 2017","html_id":"compare-2023-to-2017","implicit":true,"key":"uriTZvIwjQ"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Run your function from the previous step to get the top 20 words for 2017 and 2023. Plot the words and their frequencies in a barchart and replicate the figure below.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"IJCsGzx4BS"}],"key":"rrr8xPqnE6"}],"key":"FJu8UXPZiM"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"words_2023 = get_most_common_words(sou, 2023, n=20)\nwords_2017 = get_most_common_words(sou, 2017, n=20)","key":"ORvoHUXOL2"},{"type":"output","id":"AUW7jJ69gctjge0HlRFtQ","data":[],"key":"DuWH9FAQqd"}],"key":"rTQrQcgevo"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"words_2023","key":"XVE8yIYYxG"},{"type":"output","id":"4BrzrkW6DTY9dWJIjnPmb","data":[{"output_type":"execute_result","execution_count":13,"metadata":{},"data":{"text/plain":{"content":"[('year', 58),\n ('go', 56),\n ('let', 45),\n ('know', 40),\n ('people', 39),\n ('job', 38),\n ('america', 36),\n ('come', 33),\n ('law', 33),\n ('pay', 33),\n ('american', 31),\n ('$', 31),\n ('president', 30),\n ('look', 27),\n ('world', 25),\n ('folk', 24),\n ('nation', 24),\n ('audience', 23),\n ('work', 23),\n ('right', 23)]","content_type":"text/plain"}}}],"key":"CIDXv42Oh0"}],"key":"tjQHaLfaTa"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"words_2017","key":"uMx9MrYgMu"},{"type":"output","id":"GpPpGSyLsjsGqTctf02OQ","data":[{"output_type":"execute_result","execution_count":14,"metadata":{},"data":{"text/plain":{"content":"[('american', 34),\n ('america', 29),\n ('country', 26),\n ('nation', 21),\n ('great', 20),\n ('new', 19),\n ('year', 19),\n ('world', 18),\n ('job', 15),\n ('people', 15),\n ('americans', 14),\n ('united', 13),\n ('tonight', 13),\n ('states', 12),\n ('work', 12),\n ('child', 12),\n ('want', 12),\n ('time', 12),\n ('citizen', 11),\n ('right', 11)]","content_type":"text/plain"}}}],"key":"rmt9L2Rpgm"}],"key":"xN6O7KRfCm"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Hint - put the words and counts into a pd Dataframe for better structure\n# and to make plotting easier\ndf_2017 = pd.DataFrame(words_2017, columns=['lemma', 'count'])\ndf_2023 = pd.DataFrame(words_2023, columns=['lemma', 'count'])","key":"jxUysQ76sN"},{"type":"output","id":"vgC-hg7AOaLPcozrp6wTu","data":[],"key":"XWsNxDmEi0"}],"key":"qTTFjBxdzH"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"fig, axes = plt.subplots(2, 1, figsize=(12, 10)) \n# Plot 2017\nsns.barplot(\n    x = 'lemma',\n    y = 'count',\n    data = df_2017,\n    ax = axes[0],\n     color='#2C7BA1'\n    \n)\naxes[0].set_title('2017 State of the Union Most Frequent Words')\naxes[0].set_xlabel('Word')\naxes[0].set_ylabel('Count')\naxes[0].tick_params(axis='x', rotation=45) \n\n# Plot 2023\nsns.barplot(\n    x='lemma',\n    y='count',\n    data=df_2023,\n    ax=axes[1],\n    color='#2C7BA1'\n)\n\naxes[1].set_title('2023 State of the Union Most Frequent Words')\naxes[1].set_xlabel('Word')\naxes[1].set_ylabel('Count')\naxes[1].tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\n\n#saving figure\nplt.savefig('outputs/sotu_word_freq2017_2023.png')","key":"SeJFgDWwZU"},{"type":"output","id":"wDsf817asb04eMT9LWEUl","data":[{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"7ac9af6b29839f506566862c5057bae1","path":"/user/jcollins36855/myst-build/proj02-group7/build/7ac9af6b29839f506566862c5057bae1.png"},"text/plain":{"content":"\u003cFigure size 1200x1000 with 2 Axes\u003e","content_type":"text/plain"}}}],"key":"HzJ10Ar7r9"}],"key":"q57m5yXIH3"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"TF-IDF Vectorization","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Xfqyhg3PaG"}],"identifier":"tf-idf-vectorization","label":"TF-IDF Vectorization","html_id":"tf-idf-vectorization","implicit":true,"key":"YuUxc5ACAF"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"To use statsitical alorithms on documents, we need to transform them into vectors, where each element of the vector corresponds to a particular word in a document or corpus of documents. One common way is via TF-IDF embeddings. LLMs work similarly - they typically use transformer models to generate text embeddings before sending text through a deep neural network.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"wjc2QA7K1U"}],"key":"Kiz4hbwOah"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Here we will fit a TF-IDF vectorizer, plot all the speeches on a 2-D grid using PCA and also using a heatmap, and examine TF-IDF scores for the top 10 most common words in the first speech. This is a good resource here: ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"RulsyJGTCr"},{"type":"link","url":"https://medium.com/GeoffreyGordonAshbrook/vector-visualization-2d-plot-your-tf-idf-with-pca-83fa9fccb1d","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"https://​medium​.com​/GeoffreyGordonAshbrook​/vector​-visualization​-2d​-plot​-your​-tf​-idf​-with​-pca​-83fa9fccb1d","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"coCkG1mfWB"}],"urlSource":"https://medium.com/GeoffreyGordonAshbrook/vector-visualization-2d-plot-your-tf-idf-with-pca-83fa9fccb1d","key":"vViF4TLINk"}],"key":"Rs5pAb2PFI"}],"key":"TtLUYMv8F2"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Install nlk to current kernel only once, can do from terminal \n#(has been added to environment.yml too so if using this, do not have to install in notebook)\n\nimport sys, subprocess\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nltk\"])\n\n# confirm the version that was just installed\nimport nltk\nprint(\"NLTK version:\", nltk.__version__)\n","key":"afigjBpM3o"},{"type":"output","id":"Oo9LWB94zdzMA8ot5cdf7","data":[{"name":"stdout","output_type":"stream","text":"Requirement already satisfied: nltk in /srv/conda/envs/notebook/lib/python3.12/site-packages (3.9.2)\nRequirement already satisfied: click in /srv/conda/envs/notebook/lib/python3.12/site-packages (from nltk) (8.3.0)\nRequirement already satisfied: joblib in /srv/conda/envs/notebook/lib/python3.12/site-packages (from nltk) (1.5.2)\nRequirement already satisfied: regex\u003e=2021.8.3 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from nltk) (2025.10.23)\nRequirement already satisfied: tqdm in /srv/conda/envs/notebook/lib/python3.12/site-packages (from nltk) (4.67.1)\nNLTK version: 3.9.2\n"}],"key":"FtGkiLYvIU"}],"key":"TN8foedyR4"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom nltk.tokenize.casual import casual_tokenize","key":"vyD7jBVpK8"},{"type":"output","id":"LLozNIUyoBhey0PgN-FFO","data":[],"key":"qaVhfc2Tez"}],"key":"C3PcM63Pvc"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Train the Vectorizer and Transform the Data","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"leXJB2gRES"}],"identifier":"train-the-vectorizer-and-transform-the-data","label":"Train the Vectorizer and Transform the Data","html_id":"train-the-vectorizer-and-transform-the-data","implicit":true,"key":"bwJ23LvaeG"}],"key":"aeoQyEIHEd"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# you may use this as input to fit the TF-IDF vectorizer\nraw_docs = sou[\"Text\"].to_list()","key":"COrEkugnzL"},{"type":"output","id":"H2sZh3G-W08VMjwNQZSqU","data":[],"key":"XeEz1YDACB"}],"key":"RQSdO4P7eq"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"##########################\n# Minimal TF-IDF Vectors (Reference: Vector Visualization by Ashbrook)\n##########################\n\n# Select Model\ntfidf_model = TfidfVectorizer()\n\n# Fit Model\ntfidf_vectors = tfidf_model.fit_transform(raw_docs).toarray()","key":"iJ81s2HikY"},{"type":"output","id":"VTrylnQudr3GlqlsIvm8g","data":[],"key":"VDxEhZbdCm"}],"key":"YQA30QTa7C"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The output of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jbZebxdoCw"},{"type":"inlineCode","value":"fit_transform()","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xPmToXi9l1"},{"type":"text","value":" will be a matrix where each row corresponds to a speech, each column corresponds to a word in the corpus of speeches, and the value is the TF-IDF score which measures the importance of that word in that speech, relative to the rest of the speeches.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EwnwZeOfoJ"}],"key":"gDP9Em5wlI"}],"key":"dzCLkLOeoj"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Plot Speeches","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"No1xnpbb4C"}],"identifier":"plot-speeches","label":"Plot Speeches","html_id":"plot-speeches","implicit":true,"key":"YhqDoUEn31"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"First used PCA to generate the first chart","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"a4Wf5d4YQ3"}],"key":"ptZL7X6lBT"}],"key":"ixPTkvapbY"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Second use seaborn heatmap with a log-scaled color axis to generate the second chart","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"BVSLjDVP8h"}],"key":"PndDfpbi8N"}],"key":"y9HjG4EmMx"}],"key":"u4fsdar5AI"}],"key":"vsbsZ5Jlvb"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from sklearn.decomposition import PCA\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n# Step 1: Set PCA to find first 2 principal components\npca = PCA(n_components=2)\n\n# Step 2: Create a new dataframe where each row is a speech, and each column is a projection onto\n# one of the two principal components\nspeeches_array = tfidf_vectors\nspeeches_df2d = pd.DataFrame(pca.fit_transform(speeches_array), columns=list('xy'))\n\n# Plot Data Visualization (Matplotlib)\n\n# Plot (matplotlib only)\nplt.style.use('seaborn-v0_8')           # just a style; still matplotlibplt.figure(figsize=(6,4))\nplt.figure(figsize=(6,4))\nplt.scatter(speeches_df2d['x'], speeches_df2d['y'], s=20)\nplt.xlabel('Principle Component 1'); plt.ylabel('Principle Component 2'); plt.title('Plot of Vectorized Speeches Principle Components')\nplt.grid(False)\nplt.tight_layout(); plt.show()\n","key":"bOvxcIhKpV"},{"type":"output","id":"2RFImmKxiDqGs3LKY7aIy","data":[{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"4ef77f4dbc6064c6055fb2e3eedba2c8","path":"/user/jcollins36855/myst-build/proj02-group7/build/4ef77f4dbc6064c6055fb2e3eedba2c8.png"},"text/plain":{"content":"\u003cFigure size 600x400 with 1 Axes\u003e","content_type":"text/plain"}}}],"key":"k7OH9RskLN"}],"key":"BmDf0QnHPA"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import numpy as np\nimport seaborn as sns\nfrom matplotlib.colors import LogNorm\nimport matplotlib.ticker as mticker\nfrom matplotlib.ticker import FixedLocator, FixedFormatter\n\n# vectorized_docs: scipy sparse matrix (rows=speeches, cols=tokens)\nA = tfidf_vectors.astype(np.float32)   # densify\n\n# Log scale needs vmin \u003e 0; pick the smallest positive value\npos_min = A[A \u003e 0].min() if (A \u003e 0).any() else 1e-6\n\nplt.style.use('seaborn-v0_8')\nplt.figure(figsize=(5.8, 4.7))\n\nax = sns.heatmap(\n    A,\n    cmap=\"magma\",\n    norm=LogNorm(vmin=pos_min, vmax=A.max()),\n    cbar_kws={\"format\": mticker.LogFormatterMathtext()}\n)\n\nax.set_title(\"Vectorized Speeches\", pad=8)\nax.set_xlabel(\"Vector Index\")\nax.set_ylabel(\"Speech Index\")\n\nn_cols = A.shape[1]\nstep = 928         \nxticks = np.arange(0, n_cols, step)\n\nax.xaxis.set_major_locator(FixedLocator(xticks))\nax.xaxis.set_major_formatter(FixedFormatter([str(v) for v in xticks]))\nplt.setp(ax.get_xticklabels(), rotation=90)\nplt.grid(False)\nplt.tight_layout()\nplt.show()","key":"tqANiVYEiA"},{"type":"output","id":"HifFgac6WqfB3s5Eci-FJ","data":[{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"6e1599813fa9e8786684dc59982f90ee","path":"/user/jcollins36855/myst-build/proj02-group7/build/6e1599813fa9e8786684dc59982f90ee.png"},"text/plain":{"content":"\u003cFigure size 580x470 with 2 Axes\u003e","content_type":"text/plain"}}}],"key":"wLtNSMbgqE"}],"key":"YXhQ0rDhTQ"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Get the TF-IDF value for certain words and documents","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TNOS7Jrh2l"}],"identifier":"get-the-tf-idf-value-for-certain-words-and-documents","label":"Get the TF-IDF value for certain words and documents","html_id":"get-the-tf-idf-value-for-certain-words-and-documents","implicit":true,"key":"I8fkWUxw57"}],"key":"kwJONA2afw"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"word_list = ['year',\n 'america',\n 'people',\n 'american',\n 'work',\n 'new',\n 'job',\n 'country',\n 'americans',\n 'world'] # top ten most common words through whole corpus","key":"e31bazX7ia"},{"type":"output","id":"70THvtSSOfCgBTrFuzEg7","data":[],"key":"cwvfEwUNA8"}],"key":"qupCySkvaX"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"word_nums = [tfidf_model.vocabulary_[w] for w in word_list] # get each word's index number using the .vocabular_ attributed of vectorizer","key":"ggwz6u5Q5I"},{"type":"output","id":"rOyakbLmCuFMWTOD4jvdu","data":[],"key":"unPHsXb4GN"}],"key":"l2i9LqUEEY"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"idf_score = tfidf_model.idf_[word_nums] # get their IDF score by using .idf_ at the indices from the previous step","key":"UPlVXA8fTY"},{"type":"output","id":"6ufGtqflZegYrbllE-BbP","data":[],"key":"mM08ir2Yrg"}],"key":"nPc3ljumz8"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"tf_idf = tfidf_vectors[0, word_nums] # get the tf_idf score for the first speech","key":"BplUwgjUoZ"},{"type":"output","id":"lezjbfnB8er9QZiHChrxO","data":[],"key":"B2sJwRTC3j"}],"key":"wbHI1obMB4"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"pd.DataFrame({\"Word\": word_list, \"IDF Score\": idf_score, \"TF-IDF Score\": tf_idf})","key":"tz7L3vMrsd"},{"type":"output","id":"eeT-IF3jalhzpiiqbYZt6","data":[{"output_type":"execute_result","execution_count":29,"metadata":{},"data":{"text/html":{"content":"\u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003ctable border=\"1\" class=\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style=\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003eWord\u003c/th\u003e\n      \u003cth\u003eIDF Score\u003c/th\u003e\n      \u003cth\u003eTF-IDF Score\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e0\u003c/th\u003e\n      \u003ctd\u003eyear\u003c/td\u003e\n      \u003ctd\u003e1.032925\u003c/td\u003e\n      \u003ctd\u003e0.022719\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e1\u003c/th\u003e\n      \u003ctd\u003eamerica\u003c/td\u003e\n      \u003ctd\u003e1.272946\u003c/td\u003e\n      \u003ctd\u003e0.068439\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e2\u003c/th\u003e\n      \u003ctd\u003epeople\u003c/td\u003e\n      \u003ctd\u003e1.037118\u003c/td\u003e\n      \u003ctd\u003e0.043087\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e3\u003c/th\u003e\n      \u003ctd\u003eamerican\u003c/td\u003e\n      \u003ctd\u003e1.102217\u003c/td\u003e\n      \u003ctd\u003e0.045792\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e4\u003c/th\u003e\n      \u003ctd\u003ework\u003c/td\u003e\n      \u003ctd\u003e1.162281\u003c/td\u003e\n      \u003ctd\u003e0.005681\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e5\u003c/th\u003e\n      \u003ctd\u003enew\u003c/td\u003e\n      \u003ctd\u003e1.024591\u003c/td\u003e\n      \u003ctd\u003e0.016275\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e6\u003c/th\u003e\n      \u003ctd\u003ejob\u003c/td\u003e\n      \u003ctd\u003e2.043480\u003c/td\u003e\n      \u003ctd\u003e0.009988\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e7\u003c/th\u003e\n      \u003ctd\u003ecountry\u003c/td\u003e\n      \u003ctd\u003e1.008130\u003c/td\u003e\n      \u003ctd\u003e0.013550\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e8\u003c/th\u003e\n      \u003ctd\u003eamericans\u003c/td\u003e\n      \u003ctd\u003e1.713598\u003c/td\u003e\n      \u003ctd\u003e0.041877\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e9\u003c/th\u003e\n      \u003ctd\u003eworld\u003c/td\u003e\n      \u003ctd\u003e1.138750\u003c/td\u003e\n      \u003ctd\u003e0.026438\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e","content_type":"text/html"},"text/plain":{"content":"        Word  IDF Score  TF-IDF Score\n0       year   1.032925      0.022719\n1    america   1.272946      0.068439\n2     people   1.037118      0.043087\n3   american   1.102217      0.045792\n4       work   1.162281      0.005681\n5        new   1.024591      0.016275\n6        job   2.043480      0.009988\n7    country   1.008130      0.013550\n8  americans   1.713598      0.041877\n9      world   1.138750      0.026438","content_type":"text/plain"}}}],"key":"xpf2CDtW0w"}],"key":"hSfGV54Tc4"}],"key":"uPLBAyxBK5"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"Project 2: Reproducibility in Natural Language Processing","url":"/nlp-p01","group":"Stat159 Project 2 - Reproducibility in Natural Language Processing"},"next":{"title":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","url":"/nlp-p03","group":"Stat159 Project 2 - Reproducibility in Natural Language Processing"}}},"domain":"http://localhost:3000"},"project":{"title":"Stat159 Project 2 - Reproducibility in Natural Language Processing","authors":[{"nameParsed":{"literal":"Reily Fairchild","given":"Reily","family":"Fairchild"},"name":"Reily Fairchild","affiliations":["UC Berkeley"],"id":"contributors-myst-generated-uid-0"},{"nameParsed":{"literal":"Atiila Joselyn Birah Kharobo","given":"Atiila Joselyn Birah","family":"Kharobo"},"name":"Atiila Joselyn Birah Kharobo","affiliations":["UC Berkeley"],"id":"contributors-myst-generated-uid-1"},{"nameParsed":{"literal":"Jordan Elizabeth Collins","given":"Jordan Elizabeth","family":"Collins"},"name":"Jordan Elizabeth Collins","affiliations":["UC Berkeley"],"id":"contributors-myst-generated-uid-2"},{"nameParsed":{"literal":"Aditya Jagannadha Sai Mangalampalli","given":"Aditya Jagannadha Sai","family":"Mangalampalli"},"name":"Aditya Jagannadha Sai Mangalampalli","affiliations":["UC Berkeley"],"id":"contributors-myst-generated-uid-3"}],"github":"https://github.com/UCB-stat-159-f25/proj02-group7","affiliations":[{"id":"UC Berkeley","name":"UC Berkeley"}],"id":"a74ecdef-c9eb-4683-b555-417faf7e975c","toc":[{"file":"index.md"},{"file":"contributions.md"},{"file":"nlp-P01.ipynb"},{"file":"nlp-P02.ipynb"},{"file":"nlp-P03.ipynb"},{"file":"nlp-P04.ipynb"},{"file":"proj02-nlp.ipynb"},{"file":"project-description.md"}],"thumbnail":"/user/jcollins36855/myst-build/proj02-group7/build/b77199e99a54e59b2e3c037c2cc90f21.svg","exports":[],"bibliography":[],"index":"index","pages":[{"slug":"contributions","title":"Contributions","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"nlp-p01","title":"Project 2: Reproducibility in Natural Language Processing","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"nlp-p02","title":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"nlp-p03","title":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"nlp-p04","title":"Part 4","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"proj02-nlp","title":"Project 2: Reproducibility in Natural Language Processing","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"project-description","title":"Project 2: Reproducibility in Natural Langauge Processing","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1}]}}},"actionData":null,"errors":null},"future":{"unstable_dev":false,"unstable_postcss":false,"unstable_tailwind":false,"v2_errorBoundary":true,"v2_headers":true,"v2_meta":true,"v2_normalizeFormMethod":true,"v2_routeConvention":true}};</script><script type="module" async="">import "/user/jcollins36855/myst-build/proj02-group7/build/manifest-3481E987.js";
import * as route0 from "/user/jcollins36855/myst-build/proj02-group7/build/root-7TUVC4ZT.js";
import * as route1 from "/user/jcollins36855/myst-build/proj02-group7/build/routes/$-P6PGXPYX.js";
window.__remixRouteModules = {"root":route0,"routes/$":route1};

import("/user/jcollins36855/myst-build/proj02-group7/build/entry.client-UNPC4GT3.js");</script></body></html>