{"version":"1","records":[{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/contributions","position":0},{"hierarchy":{"lvl1":""},"content":"Each member contributed to a specific part of the project:\n\nPart 1 and Part 4 - Aditya\n\nPart 2 - Jordan and Atiila\n\nPart 3 & Myst Site & Binder - Reily","type":"content","url":"/contributions","position":1},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing"},"content":"Project 2, Stat 159/259 Fall 2025\nTeam Members:\nAtiila Joselyn Birah Kharobo \n\natiila‚Äã_kharobo@berkeley‚Äã.edu,\nReily Fairchild \n\nreilyjean@berkeley‚Äã.edu\nAditya Jagannadha Sai Mangalampalli \n\namangalampalli@berkeley‚Äã.edu\nJordan Elizabeth Collins \n\njcollins36855@berkeley‚Äã.edu\n\nThis is a project that conduct EDA, leverages NLP packages, and topic modeling frameworks LDA and BERTopic to analyze topic speeches given by presidents.\n\n\n\nGithub Myst Site","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing"},"type":"lvl1","url":"/nlp-p01","position":0},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing"},"content":"","type":"content","url":"/nlp-p01","position":1},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl2":"Read Data"},"type":"lvl2","url":"/nlp-p01#read-data","position":2},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl2":"Read Data"},"content":"\n\n# imports\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('seaborn-v0_8-dark') \n\n# read in SOTU.csv using pandas, name the variable `sou` for simplicity\n# the below cell is what the output should look like\nsou = pd.read_csv('data/SOTU.csv')\n\nsou\n\n","type":"content","url":"/nlp-p01#read-data","position":3},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"Number of Speeches per President","lvl2":"Read Data"},"type":"lvl3","url":"/nlp-p01#number-of-speeches-per-president","position":4},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"Number of Speeches per President","lvl2":"Read Data"},"content":"\n\n# Hint - use value_counts() on the President column\n# Hint - sort in order of dataframe\ncounts = sou[\"President\"].value_counts(sort=False)\n\n# Plot \n# Hint - use the .plot() method for Pandas Series, make sure all presidents show up on x-axis\n\ncounts.plot(kind=\"bar\", figsize=(20, 10))\nplt.title(\"Number of Speeches per President\", fontsize=24)\nplt.xlabel(\"President\", fontsize=18)\nplt.ylabel(\"Count\", fontsize=18)\nplt.xticks(rotation=90, fontsize=13)\nplt.yticks(fontsize=13)\n\nplt.show()\nplt.savefig(\"outputs/num_speeches.png\")\n\nIt seems pretty interesting that each president seems to give more speeches each as we get closer and closer to the present. Maybe this might be a function of the party system and increasing tension between parties so more speeches are necessary for communication.\n\n","type":"content","url":"/nlp-p01#number-of-speeches-per-president","position":5},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"Number of Speeches per Year","lvl2":"Read Data"},"type":"lvl3","url":"/nlp-p01#number-of-speeches-per-year","position":6},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"Number of Speeches per Year","lvl2":"Read Data"},"content":"\n\n# Hint - Use value counts and sort by years\ncounts_by_year = sou[\"Year\"].value_counts().sort_index()\n\nplt.figure(figsize=(5.75, 5))\ncounts_by_year.plot(kind=\"line\")\n\nplt.title(\"Number of State of the Union Speeches per Year\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Count\")\n\nplt.tight_layout()\nplt.show()\nplt.savefig(\"outputs/num_speeches_year.png\")\n\nIt seems like there were many more SOU speeches done in the beginning when the US was first founded. This makes sense as I would assume there were lots of speeches and presentations to create the laws to run the nation.\n\n","type":"content","url":"/nlp-p01#number-of-speeches-per-year","position":7},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"Word Count Distribution","lvl2":"Read Data"},"type":"lvl3","url":"/nlp-p01#word-count-distribution","position":8},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"Word Count Distribution","lvl2":"Read Data"},"content":"\n\n# Hint - try seaborn.histplot()\n\nplt.figure(figsize=(5.75, 5))\n\nsns.histplot(\n    sou[\"Word Count\"],\n    bins=18,\n    kde=False,\n    color=\"steelblue\"\n)\n\nplt.title(\"Distribution of State of the Union Speech\\nWord Counts\")\nplt.xlabel(\"Word Count\")\nplt.ylabel(\"Count\")\n\nplt.tight_layout()\nplt.show()\nplt.savefig(\"outputs/dist_word_counts.png\")\n\nIt‚Äôs quite interesting how many of the speeches are around 5000 words in length which is quite short for a speech. I would have imagined most of the speeches are closer to 15000. However, this makes sense. I would assume the speeches would be longer in situations where the nation is under threat or actively involved in something which is a threat to national security.\n\n","type":"content","url":"/nlp-p01#word-count-distribution","position":9},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"Word Count Distribution over Year","lvl2":"Read Data"},"type":"lvl3","url":"/nlp-p01#word-count-distribution-over-year","position":10},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"Word Count Distribution over Year","lvl2":"Read Data"},"content":"\n\n# Hint: try seaborn.rugplot()\nplt.figure(figsize=(6, 5))\n\nsns.scatterplot(\n    data=sou,\n    x=\"Word Count\",\n    y=\"Year\",\n    s=40\n)\n\nsns.rugplot(\n    data=sou,\n    x=\"Word Count\",\n    height=0.02,\n    alpha=0.6,\n)\n\nsns.rugplot(\n    data=sou,\n    y=\"Year\",\n    height=0.02,\n    alpha=1,\n    color=(59/255, 117/255, 174/255)\n)\n\nplt.title(\"Speech Year Versus Word Count\")\nplt.xlabel(\"Word Count\")\nplt.ylabel(\"Year\")\n\nplt.tight_layout()\nplt.show()\nplt.savefig(\"outputs/speech_vs_word_count.png\")\n\nIt seems that as we move towards modern times, the word count really starts to shrink. This also makes it appear as 1820-1900s had the US in many disagreements and things which might explain why the majority of the speeches during that time period have word counts greater than 5000.\n\n","type":"content","url":"/nlp-p01#word-count-distribution-over-year","position":11},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"Word Count Distribution per President","lvl2":"Read Data"},"type":"lvl3","url":"/nlp-p01#word-count-distribution-per-president","position":12},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"Word Count Distribution per President","lvl2":"Read Data"},"content":"\n\n# Hint: use pandas groupby to get mean word count per president then sort by order\n\navg_words = sou.groupby(\"President\")[\"Word Count\"].mean()\n\navg_words = avg_words.loc[sou[\"President\"].unique()]\nplt.figure(figsize=(10, 6))\navg_words.plot(kind=\"bar\")\n\nplt.title(\"Average State of the Union Word Count\\nper President\")\nplt.xlabel(\"President\")\nplt.ylabel(\"Average Word Count\")\n\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.show()\nplt.savefig(\"outputs/avg_sou_wc_president.png\")\n\nIt seems like there are like two main discontinuities where from Biden to Wilson, the majority of the speeches are short but Taft to Washington has significantly more words. This could be an explanation as to how the nation‚Äôs priorities have shifted and hence some topics needed more explanations than others.","type":"content","url":"/nlp-p01#word-count-distribution-per-president","position":13},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl1","url":"/nlp-p02","position":0},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"","type":"content","url":"/nlp-p02","position":1},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl1","url":"/nlp-p02#part-2-simple-text-processing-tokenization-lemmatization-word-frequency-vectorization-20-pts","position":2},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"Now we will start working on simple text processing using the SpaCy package and the same dataset as Part 1. The package should already be included in the environment.yml. However, we will also need to download en_core_web_sm, an English language text processing model. To do this, while having your sotu environment activated, run the following:python -m spacy download en_core_web_sm\n\nNow, you should be good to go!\n\nSome important definitions:\n\nToken: a single word or piece of a word\n\nLemma: the core component of a word, e.g., ‚Äúcomplete‚Äù is the lemma for ‚Äúcompleted‚Äù and ‚Äúcompletely‚Äù\n\nStop Word: a common word that does not add semantic value, such as ‚Äúa‚Äù, ‚Äúand‚Äù, ‚Äúthe‚Äù, etc.\n\nVectorization: representing a document as a vector where each index in the vector corresponds to a token or word and each entry is the count.\n\nIn this section, we will explore the most common tokens and lemmas throughout different slices of the speech data. We will also develop vectorization representations of the speeches.\n\nThe core steps are:\n\nProcess speeches using the SpaCy nlp module\n\nAnalyze Tokens vs Lemmas:\n\nCreate a list of all tokens across all speeches that are not stop words, punctuation, or spaces.\n\nCreate a second list of the lemmas for these same tokens.\n\nDisplay the top 25 for each of these and compare.\n\nAnalyze common word distributions over different years:\n\nCreate a function that takes the dataset and a year as an input and outputs the top n lemmas for that year‚Äôs speeches\n\nCompare the top 10 words for 2023 versus 2019\n\nDocument Vectorization:\n\nTrain a Term Frequency-Inverse Document Frequency (TF-IDF) vectorization model using your processed dataset and scikit learn\n\nOutput the feature vectors\n\nHelpful Resources:\n\nhttps://‚Äãrealpython‚Äã.com‚Äã/natural‚Äã-language‚Äã-processing‚Äã-spacy‚Äã-python/\n\nhttps://‚Äãwww‚Äã.statology‚Äã.org‚Äã/text‚Äã-preprocessing‚Äã-feature‚Äã-engineering‚Äã-spacy/\n\nhttps://‚Äãscikit‚Äã-learn‚Äã.org‚Äã/stable‚Äã/modules‚Äã/generated‚Äã/sklearn‚Äã.feature‚Äã_extraction‚Äã.text‚Äã.TfidfVectorizer‚Äã.html#\n\nhttps://‚Äãwww‚Äã.geeksforgeeks‚Äã.org‚Äã/nlp‚Äã/how‚Äã-to‚Äã-store‚Äã-a‚Äã-tfidfvectorizer‚Äã-for‚Äã-future‚Äã-use‚Äã-in‚Äã-scikit‚Äã-learn/\n\n","type":"content","url":"/nlp-p02#part-2-simple-text-processing-tokenization-lemmatization-word-frequency-vectorization-20-pts","position":3},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl2":"Processing Speeches with SpaCy"},"type":"lvl2","url":"/nlp-p02#processing-speeches-with-spacy","position":4},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl2":"Processing Speeches with SpaCy"},"content":"Lets study just speeches from 2000 and onwards to begin with. So, be sure to subset your DataFrame to just these speeches before continuing!\n\n!pip install spacy\n\nimport spacy\n!python -m spacy download en_core_web_sm\nspacy.cli.download(\"en_core_web_sm\")\n\nfrom tqdm import tqdm\nfrom collections import Counter\nimport pandas as pd\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsou = pd.read_csv('data/SOTU.csv')\nnlp = spacy.load(\"en_core_web_sm\")\n\nsou\n\n# subset the speech dataframe for speeches from 2000 and onwards\nsou_new = sou[sou['Year'] >= 2000]\nsou_new\n\n# Process each speeches using the 'nlp' function\nprocessed_speeches = []\n\nfor i in range(len(sou_new)):\n    speech_text = sou_new.loc[i, 'Text'] \n    processed = nlp(speech_text)    \n    processed_speeches.append(processed)\n\n","type":"content","url":"/nlp-p02#processing-speeches-with-spacy","position":5},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl2":"Analyze Tokens vs Lemmas"},"type":"lvl2","url":"/nlp-p02#analyze-tokens-vs-lemmas","position":6},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl2":"Analyze Tokens vs Lemmas"},"content":"","type":"content","url":"/nlp-p02#analyze-tokens-vs-lemmas","position":7},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"Token List","lvl2":"Analyze Tokens vs Lemmas"},"type":"lvl3","url":"/nlp-p02#token-list","position":8},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"Token List","lvl2":"Analyze Tokens vs Lemmas"},"content":"Create a list of tokens across all speeches that are not spaces, stopwords, or punctuation. Make each token lowercase as well. Hint: each element of the list we just created are themselves lists of tokens. Token objects have attributes is_stop, is_punct, and is_space.\n\ntokens = []\n\nfor speech in processed_speeches:\n    for token in speech:\n        if not token.is_punct and not token.is_space and not token.is_stop:\n            tokens.append(token.text)\n    \n\n# print top 20 tokens\n# Hint - use Counter, and one of the Counter object's methods to display the top 20\ntoken_count = Counter(tokens).most_common(20)\nprint(token_count)\n\n","type":"content","url":"/nlp-p02#token-list","position":9},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"Lemma List","lvl2":"Analyze Tokens vs Lemmas"},"type":"lvl3","url":"/nlp-p02#lemma-list","position":10},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"Lemma List","lvl2":"Analyze Tokens vs Lemmas"},"content":"Do the same as above, but for lemmas. Hint: recall lemmas are components of words. Each token should have an attribute to extract the lemma.\n\nlemmas = []\n\nfor token in tokens:\n    word = nlp(token) # run the tokens through nlp to make sure it has the lemma_ attribute\n    for token in word:\n        lemma=token.lemma_\n        lemmas.append(lemma.lower())\n\n# print top 20 lemmas\nlemma_counts = Counter(lemmas).most_common(20)\nlemma_counts\n\n","type":"content","url":"/nlp-p02#lemma-list","position":11},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"Token versus Lemma Comparison","lvl2":"Analyze Tokens vs Lemmas"},"type":"lvl3","url":"/nlp-p02#token-versus-lemma-comparison","position":12},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"Token versus Lemma Comparison","lvl2":"Analyze Tokens vs Lemmas"},"content":"What do you notice about the top tokens versus the top lemmas?\nConsider two tokens - ‚Äúyear‚Äù and ‚Äúyears‚Äù - how do their counts compare to the lemma ‚Äúyear‚Äù?\nWhat about the lemma ‚Äúchild‚Äù?\n\n","type":"content","url":"/nlp-p02#token-versus-lemma-comparison","position":13},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl2":"Common Words"},"type":"lvl2","url":"/nlp-p02#common-words","position":14},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl2":"Common Words"},"content":"","type":"content","url":"/nlp-p02#common-words","position":15},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"Common Words per Year Function","lvl2":"Common Words"},"type":"lvl3","url":"/nlp-p02#common-words-per-year-function","position":16},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"Common Words per Year Function","lvl2":"Common Words"},"content":"Fill in the below function to obtain the n-most common words in speeches for a given year.\n\ninputs:\n\ndf raw unprocessed sou dataframe\n\nyear\n\nn\noutputs:\n\ntop n words for that years\n\nsteps:\n\nsubset the dataframe for the year of interest - note the years might not be in int type\n\nprocess the subsetted dataframe with spacy\n\nget the lemmas across all those speeches\n\ncount the top n lemmas\n\ndef get_most_common_words(df, year, n=25):\n    \"\"\"\n    Processes the SOTU speech for a given year and returns\n    the most common non-stopword/punctuation lemmas.\n\n    INPUTS\n    - raw unprocessed sou data frame\n    - year\n\n    OUTPUTS\n    - top n words for that year\n    \"\"\"\n\n    # Step 1: Subset df\n    new_df = df[df['Year'] == year]\n    \n    # Step 2: Process the text with spaCy\n    new_df = df[df['Year'] == year].reset_index(drop=True) \n    processed_speeches = []\n\n    for i in range(len(new_df)):\n        speech_text = new_df.loc[i, 'Text'] \n        processed = nlp(speech_text)    \n        processed_speeches.append(processed)\n    \n    # Step 3: Get lemmas\n    lemmas = []\n\n    for doc in processed_speeches:\n        for token in doc:\n            if not token.is_punct and not token.is_space and not token.is_stop:\n                lemmas.append(token.lemma_.lower())\n    \n    lemma_counts = Counter(lemmas).most_common(n)\n    \n    return lemma_counts\n\n# test it on 2024\nget_most_common_words(sou, 2024, n=20)\n\n","type":"content","url":"/nlp-p02#common-words-per-year-function","position":17},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"Compare 2023 to 2017","lvl2":"Common Words"},"type":"lvl3","url":"/nlp-p02#compare-2023-to-2017","position":18},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"Compare 2023 to 2017","lvl2":"Common Words"},"content":"Run your function from the previous step to get the top 20 words for 2017 and 2023. Plot the words and their frequencies in a barchart and replicate the figure below.\n\nwords_2023 = get_most_common_words(sou, 2023, n=20)\nwords_2017 = get_most_common_words(sou, 2017, n=20)\n\nwords_2023\n\nwords_2017\n\n# Hint - put the words and counts into a pd Dataframe for better structure\n# and to make plotting easier\ndf_2017 = pd.DataFrame(words_2017, columns=['lemma', 'count'])\ndf_2023 = pd.DataFrame(words_2023, columns=['lemma', 'count'])\n\nfig, axes = plt.subplots(2, 1, figsize=(12, 10)) \n# Plot 2017\nsns.barplot(\n    x = 'lemma',\n    y = 'count',\n    data = df_2017,\n    ax = axes[0],\n     color='#2C7BA1'\n    \n)\naxes[0].set_title('2017 State of the Union Most Frequent Words')\naxes[0].set_xlabel('Word')\naxes[0].set_ylabel('Count')\naxes[0].tick_params(axis='x', rotation=45) \n\n# Plot 2023\nsns.barplot(\n    x='lemma',\n    y='count',\n    data=df_2023,\n    ax=axes[1],\n    color='#2C7BA1'\n)\n\naxes[1].set_title('2023 State of the Union Most Frequent Words')\naxes[1].set_xlabel('Word')\naxes[1].set_ylabel('Count')\naxes[1].tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\n\n#saving figure\nplt.savefig('outputs/sotu_word_freq2017_2023.png')\n\n","type":"content","url":"/nlp-p02#compare-2023-to-2017","position":19},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl2":"TF-IDF Vectorization"},"type":"lvl2","url":"/nlp-p02#tf-idf-vectorization","position":20},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl2":"TF-IDF Vectorization"},"content":"To use statsitical alorithms on documents, we need to transform them into vectors, where each element of the vector corresponds to a particular word in a document or corpus of documents. One common way is via TF-IDF embeddings. LLMs work similarly - they typically use transformer models to generate text embeddings before sending text through a deep neural network.\n\nHere we will fit a TF-IDF vectorizer, plot all the speeches on a 2-D grid using PCA and also using a heatmap, and examine TF-IDF scores for the top 10 most common words in the first speech. This is a good resource here: \n\nhttps://‚Äãmedium‚Äã.com‚Äã/GeoffreyGordonAshbrook‚Äã/vector‚Äã-visualization‚Äã-2d‚Äã-plot‚Äã-your‚Äã-tf‚Äã-idf‚Äã-with‚Äã-pca‚Äã-83fa9fccb1d\n\n# Install nlk to current kernel only once, can do from terminal \n#(has been added to environment.yml too so if using this, do not have to install in notebook)\n\nimport sys, subprocess\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nltk\"])\n\n# confirm the version that was just installed\nimport nltk\nprint(\"NLTK version:\", nltk.__version__)\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom nltk.tokenize.casual import casual_tokenize\n\n","type":"content","url":"/nlp-p02#tf-idf-vectorization","position":21},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"Train the Vectorizer and Transform the Data","lvl2":"TF-IDF Vectorization"},"type":"lvl3","url":"/nlp-p02#train-the-vectorizer-and-transform-the-data","position":22},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"Train the Vectorizer and Transform the Data","lvl2":"TF-IDF Vectorization"},"content":"\n\n# you may use this as input to fit the TF-IDF vectorizer\nraw_docs = sou[\"Text\"].to_list()\n\n##########################\n# Minimal TF-IDF Vectors (Reference: Vector Visualization by Ashbrook)\n##########################\n\n# Select Model\ntfidf_model = TfidfVectorizer()\n\n# Fit Model\ntfidf_vectors = tfidf_model.fit_transform(raw_docs).toarray()\n\nThe output of fit_transform() will be a matrix where each row corresponds to a speech, each column corresponds to a word in the corpus of speeches, and the value is the TF-IDF score which measures the importance of that word in that speech, relative to the rest of the speeches.\n\n","type":"content","url":"/nlp-p02#train-the-vectorizer-and-transform-the-data","position":23},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"Plot Speeches","lvl2":"TF-IDF Vectorization"},"type":"lvl3","url":"/nlp-p02#plot-speeches","position":24},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"Plot Speeches","lvl2":"TF-IDF Vectorization"},"content":"First used PCA to generate the first chart\n\nSecond use seaborn heatmap with a log-scaled color axis to generate the second chart\n\nfrom sklearn.decomposition import PCA\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n# Step 1: Set PCA to find first 2 principal components\npca = PCA(n_components=2)\n\n# Step 2: Create a new dataframe where each row is a speech, and each column is a projection onto\n# one of the two principal components\nspeeches_array = tfidf_vectors\nspeeches_df2d = pd.DataFrame(pca.fit_transform(speeches_array), columns=list('xy'))\n\n# Plot Data Visualization (Matplotlib)\n\n# Plot (matplotlib only)\nplt.style.use('seaborn-v0_8')           # just a style; still matplotlibplt.figure(figsize=(6,4))\nplt.figure(figsize=(6,4))\nplt.scatter(speeches_df2d['x'], speeches_df2d['y'], s=20)\nplt.xlabel('Principle Component 1'); plt.ylabel('Principle Component 2'); plt.title('Plot of Vectorized Speeches Principle Components')\nplt.grid(False)\nplt.tight_layout(); plt.show()\n\n\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib.colors import LogNorm\nimport matplotlib.ticker as mticker\nfrom matplotlib.ticker import FixedLocator, FixedFormatter\n\n# vectorized_docs: scipy sparse matrix (rows=speeches, cols=tokens)\nA = tfidf_vectors.astype(np.float32)   # densify\n\n# Log scale needs vmin > 0; pick the smallest positive value\npos_min = A[A > 0].min() if (A > 0).any() else 1e-6\n\nplt.style.use('seaborn-v0_8')\nplt.figure(figsize=(5.8, 4.7))\n\nax = sns.heatmap(\n    A,\n    cmap=\"magma\",\n    norm=LogNorm(vmin=pos_min, vmax=A.max()),\n    cbar_kws={\"format\": mticker.LogFormatterMathtext()}\n)\n\nax.set_title(\"Vectorized Speeches\", pad=8)\nax.set_xlabel(\"Vector Index\")\nax.set_ylabel(\"Speech Index\")\n\nn_cols = A.shape[1]\nstep = 928         \nxticks = np.arange(0, n_cols, step)\n\nax.xaxis.set_major_locator(FixedLocator(xticks))\nax.xaxis.set_major_formatter(FixedFormatter([str(v) for v in xticks]))\nplt.setp(ax.get_xticklabels(), rotation=90)\nplt.grid(False)\nplt.tight_layout()\nplt.show()\n\n","type":"content","url":"/nlp-p02#plot-speeches","position":25},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"Get the TF-IDF value for certain words and documents","lvl2":"TF-IDF Vectorization"},"type":"lvl3","url":"/nlp-p02#get-the-tf-idf-value-for-certain-words-and-documents","position":26},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"Get the TF-IDF value for certain words and documents","lvl2":"TF-IDF Vectorization"},"content":"\n\nword_list = ['year',\n 'america',\n 'people',\n 'american',\n 'work',\n 'new',\n 'job',\n 'country',\n 'americans',\n 'world'] # top ten most common words through whole corpus\n\nword_nums = [tfidf_model.vocabulary_[w] for w in word_list] # get each word's index number using the .vocabular_ attributed of vectorizer\n\nidf_score = tfidf_model.idf_[word_nums] # get their IDF score by using .idf_ at the indices from the previous step\n\ntf_idf = tfidf_vectors[0, word_nums] # get the tf_idf score for the first speech\n\npd.DataFrame({\"Word\": word_list, \"IDF Score\": idf_score, \"TF-IDF Score\": tf_idf})","type":"content","url":"/nlp-p02#get-the-tf-idf-value-for-certain-words-and-documents","position":27},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)"},"type":"lvl1","url":"/nlp-p03","position":0},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)"},"content":"","type":"content","url":"/nlp-p03","position":1},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl3":"References Used:"},"type":"lvl3","url":"/nlp-p03#references-used","position":2},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl3":"References Used:"},"content":"LDA:\n\nhttps://‚Äãmedium‚Äã.com‚Äã/sayahfares19‚Äã/text‚Äã-analysis‚Äã-topic‚Äã-modelling‚Äã-with‚Äã-spacy‚Äã-gensim‚Äã-4cd92ef06e06\n\nhttps://‚Äãwww‚Äã.kaggle‚Äã.com‚Äã/code‚Äã/faressayah‚Äã/text‚Äã-analysis‚Äã-topic‚Äã-modeling‚Äã-with‚Äã-spacy‚Äã-gensim‚Äã#üìö‚Äã-Topic‚Äã-Modeling (code for previous post)\n\nhttps://‚Äãtowardsdatascience‚Äã.com‚Äã/topic‚Äã-modelling‚Äã-in‚Äã-python‚Äã-with‚Äã-spacy‚Äã-and‚Äã-gensim‚Äã-dc8f7748bdbf/\n\nBERTopic:\n\nhttps://‚Äãmaartengr‚Äã.github‚Äã.io‚Äã/BERTopic‚Äã/getting‚Äã_started‚Äã/visualization‚Äã/visualize‚Äã_documents‚Äã.html‚Äã#visualize‚Äã-documents‚Äã-with‚Äã-plotly\n\nhttps://‚Äãmaartengr‚Äã.github‚Äã.io‚Äã/BERTopic‚Äã/getting‚Äã_started‚Äã/visualization‚Äã/visualize‚Äã_topics‚Äã.html\n\nhttps://‚Äãmaartengr‚Äã.github‚Äã.io‚Äã/BERTopic‚Äã/getting‚Äã_started‚Äã/distribution‚Äã/distribution‚Äã.html‚Äã#example\n\nhttps://‚Äãmaartengr‚Äã.github‚Äã.io‚Äã/BERTopic‚Äã/getting‚Äã_started‚Äã/topicrepresentation‚Äã/topicrepresentation‚Äã.html‚Äã#update‚Äã-topic‚Äã-representation‚Äã-after‚Äã-training\n\nimport spacy\nfrom tqdm import tqdm\nfrom collections import Counter\nimport pandas as pd\n\n# imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('seaborn-v0_8-dark') \n\nsou = pd.read_csv('data/SOTU.csv')\nnlp = spacy.load(\"en_core_web_sm\")\n\nfrom spacy import displacy\nfrom bertopic import BERTopic\nfrom gensim.corpora import Dictionary\nfrom gensim.models import LdaModel\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pyLDAvis\nimport pyLDAvis.gensim_models\n\n","type":"content","url":"/nlp-p03#references-used","position":3},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl3":"LDA"},"type":"lvl3","url":"/nlp-p03#lda","position":4},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl3":"LDA"},"content":"To create and analyze potential topics associated with the speeches, we will first use the LDA method and package.\n\nTrain an LDA model with 18 topics\n\nOutput the top 10 words for each topic.\n\nOutput the topic distribution for the first speech\n\nMake a visualization\n\ndef preprocess_text(text): \n    doc = nlp(text) \n    return [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and not token.is_space and len(token.lemma_) > 3]\n\n# Process all texts - note this takes ~ 5 minutes to run\nprocessed_docs = sou['Text'].apply(preprocess_text)\n\nprocessed_docs\n\n# Build dictionary from processed_docs, which is a list of tokens extracted from our speeches\nsou['tokens'] = processed_docs\n#Gensim Dictionary object maps each word to their unique ID:\ndictionary = Dictionary(sou['tokens'])\n#print(dictionary.token2id)\n#dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=1000)\n\n#create sparse vector (i, j) where i is dictionary id and j is number of occurences of that distinct word (?)\ncorpus = [dictionary.doc2bow(doc) for doc in sou['tokens']]\n\n# train LDA model with 18 topics\nlda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=18, random_state=42, passes=10)\n\n# print the top 10 words for each topic\nlda_model.print_topics(-1)\n\n# print the topic distribution for the first speech\nsou['Text'][0]\nlda_model[corpus][0]\n\nThe first speech is 99% belonging to topic 2!\n\n# make a visualization using pyLDAvis\npyLDAvis.enable_notebook()\n\nlda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\npyLDAvis.display(lda_display)\n\n\n#save to outputs\npyLDAvis.save_html(lda_display, 'outputs/lda_topics.html')\n\n","type":"content","url":"/nlp-p03#lda","position":5},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl3":"BERTopic"},"type":"lvl3","url":"/nlp-p03#bertopic","position":6},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl3":"BERTopic"},"content":"We will also conduct topic analysis using the BERTopic method and package. We will run through the following steps:\n\nTrain a BERTopic model with a min_topic_size of 3\n\nOutput the top 10 words for each topic.\n\nOutput the topic distribution for the first speech\n\nMake a visualization of the topics\n\nfrom bertopic import BERTopic\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndocs = sou['Text'].to_list()\n\n# train the model - this takes about 30 seconds\ntopic_model = BERTopic(min_topic_size=3)\ntopics, probs = topic_model.fit_transform(docs)\n\n\n# remove stop words from the topics (Hint: use CountVectorizer and then .update_topics on topic_model)\nvectorizer_model = CountVectorizer(stop_words=\"english\")\ntopic_model.update_topics(docs, vectorizer_model=vectorizer_model) \n\n# output the top 10 words for each topic - hint see get_topic_info\ntopic_model.get_topic_info()['Representation']\n\n# output the topic distribution for the first speech\ntopic_distr, _ = topic_model.approximate_distribution(docs)\nfirst_speech_viz = topic_model.visualize_distribution(topic_distr[1])\n\n#save first speech topic distribution to outputs\nfirst_speech_viz.write_html(\"outputs/BERTopic_first_speech_viz.html\")\nfirst_speech_viz\n\n# run this cell to visualize the topics\nviz_topics = topic_model.visualize_topics()\n\n#save topic visualizations to output\nviz_topics.write_html(\"outputs/BERTopic_topics_viz.html\")\nviz_topics\n\n","type":"content","url":"/nlp-p03#bertopic","position":7},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl2":"Discussion and Reflections"},"type":"lvl2","url":"/nlp-p03#discussion-and-reflections","position":8},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl2":"Discussion and Reflections"},"content":"\n\nThe topic distribution across the two dimensional PCA is notably different for the LDA (bag of words) and BERTopic (semantic similarity) approaches. The LDA distribution appears to have larger clusters on the right quadrant of the analyses, with significantly smaller clusters on the left quadrant. On the other hand, the BERTopic distributions land in each quadrant of the PCA grid, with more even distribution between each in terms of cluster size. This demonstrates how the two approaches use different attributes of the speeches and different algorithms to conclude topic summaries and distributions.","type":"content","url":"/nlp-p03#discussion-and-reflections","position":9},{"hierarchy":{"lvl1":"Part 4"},"type":"lvl1","url":"/nlp-p04","position":0},{"hierarchy":{"lvl1":"Part 4"},"content":"For this, we will be exploring word frequency over time to answer the question: does the frequency of certain words change over time?\n\nWe will be starting with four common words that have been referenced in almost every single debate or speech I‚Äôve ever listened to, which I am sure you have heard as well: [\"freedom\", \"economy\", \"security\", \"health\"]\n\n# Define words we think are important in this case\nimport pandas as pd\nimport re\nfrom matplotlib import pyplot as plt\n\nsou = pd.read_csv('data/SOTU.csv')\nwords = [\"freedom\", \"economy\", \"security\", \"health\"]\n\ndef count_word(text, word):\n    \"\"\"\n    RegEx Method to find variations of the word (case insensitive)\n    \"\"\"\n    pattern = r'\\b' + re.escape(word) + r'\\b'\n    return len(re.findall(pattern, text, flags=re.IGNORECASE))\n\n\n# Iterate through every word and get the raw count per year and then scale it\nfor w in words:\n    raw_col = f\"freq_{w}\"\n    rel_col = f\"rel_freq_{w}\"\n    sou[raw_col] = sou[\"Text\"].apply(lambda x: count_word(x, w))\n    sou[rel_col] = sou[raw_col] / sou[\"Word Count\"] * 1000\n\n\n# Sort the values every year\nsou_sorted = sou.sort_values(\"Year\").reset_index(drop=True)\n\n# Iterate through the words and fit a rolling average\nfor w in words:\n    col = f\"rel_freq_{w}\"\n    sou_sorted[col + \"_smooth\"] = (\n        sou_sorted[col].rolling(window=10, min_periods=3).mean()\n    )\n\n\nplt.figure(figsize=(12, 6))\n\n# Plot for the respective years\nfor w in words:\n    plt.plot(\n        sou_sorted[\"Year\"],\n        sou_sorted[f\"rel_freq_{w}_smooth\"],\n        label=w\n    )\n\nplt.title(\"Word Frequency Over Time (10-year rolling average)\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Relative Frequency (per 1000 words)\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\nplt.savefig(\"outputs/word_freq_over_time.png\")\n\n","type":"content","url":"/nlp-p04","position":1},{"hierarchy":{"lvl1":"Part 4","lvl2":"Freedom"},"type":"lvl2","url":"/nlp-p04#freedom","position":2},{"hierarchy":{"lvl1":"Part 4","lvl2":"Freedom"},"content":"Mentions of freedom stay low throughout the 1800s, then gradually increase in the early 20th century. The term spikes sharply during the Cold War (1970s‚Äì1990s), when U.S. presidents frequently framed politics in ideological terms. Another noticeable peak appears in the early 2000s during the post-9/11 era, when ‚Äúfreedom‚Äù became central to national messaging. Through time, ‚Äúfreedom‚Äù becomes a major rhetorical theme primarily in the modern era.","type":"content","url":"/nlp-p04#freedom","position":3},{"hierarchy":{"lvl1":"Part 4","lvl2":"Economy"},"type":"lvl2","url":"/nlp-p04#economy","position":4},{"hierarchy":{"lvl1":"Part 4","lvl2":"Economy"},"content":"The word economy remains rarely used before 1900, but rises significantly during major economic crises. There are clear peaks during the Great Depression (1930s), post-WWII recovery, the stagflation era (1970s), and again around the Great Recession (2008‚Äì2010). The pattern reflects how presidents address economic instability directly in their State of the Union speeches.","type":"content","url":"/nlp-p04#economy","position":5},{"hierarchy":{"lvl1":"Part 4","lvl2":"Security"},"type":"lvl2","url":"/nlp-p04#security","position":6},{"hierarchy":{"lvl1":"Part 4","lvl2":"Security"},"content":"Security shows the strongest spikes of any word. Usage jumps dramatically during World War II, peaks again throughout the Cold War, and rises once more after 2001 in response to terrorism and national security concerns. This term closely tracks periods when the nation faces real or perceived threats, making it the most crisis-driven word in the group.","type":"content","url":"/nlp-p04#security","position":7},{"hierarchy":{"lvl1":"Part 4","lvl2":"Health"},"type":"lvl2","url":"/nlp-p04#health","position":8},{"hierarchy":{"lvl1":"Part 4","lvl2":"Health"},"content":"Mentions of health are almost nonexistent before the 20th century. Use rises steadily as the federal government becomes more involved in public health policy‚Äîespecially around the creation of Medicare and Medicaid (1960s), health reform debates in the 1990s, the Affordable Care Act period (2009‚Äì2015), and again around 2020 during the COVID-19 pandemic. ‚ÄúHealth‚Äù is the newest major theme in modern SOTU speeches.","type":"content","url":"/nlp-p04#health","position":9},{"hierarchy":{"lvl1":"Part 4","lvl2":"Overall Summary"},"type":"lvl2","url":"/nlp-p04#overall-summary","position":10},{"hierarchy":{"lvl1":"Part 4","lvl2":"Overall Summary"},"content":"Across all four words, usage stays low before 1900 and rises sharply in the modern era as speeches become more policy-focused. The trends reveal how presidential priorities evolve: ‚Äúsecurity‚Äù peaks during wars and threats, ‚Äúeconomy‚Äù during financial crises, ‚Äúfreedom‚Äù during ideological conflicts, and ‚Äúhealth‚Äù during healthcare policy shifts and pandemics. Together, these patterns highlight how State of the Union language reflects broad historical changes in national concerns.","type":"content","url":"/nlp-p04#overall-summary","position":11},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing"},"type":"lvl1","url":"/proj02-nlp","position":0},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing"},"content":"","type":"content","url":"/proj02-nlp","position":1},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"type":"lvl2","url":"/proj02-nlp#part-1-data-loading-and-initial-exploration-15-pts","position":2},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"content":"The data for this project is stored in the data folder in your repositories, in the SOTU.csv file. The data file is structured as a CSV with columns for president name, speech text, year, and word count in the speech.\n\nIn this section you will:\n\nImport the data into a pandas dataframe\n\nPerform exploratory data analysis (EDA) including specifically:\n\nAnalyze the number of speeches per president\n\nAnalyze the number of speeches per year\n\nAnalyze the word count distribution\n\nAnalyze the word count distribution accross years using a rug plot\n\nAnalyze the average word count per president\n\nWrite commentary on your findings\n\nFirst, create the conda environment with the provided yaml file. Note, it‚Äôs not unusual for it to take ~15 minutes for the environment to fully install.\n\n","type":"content","url":"/proj02-nlp#part-1-data-loading-and-initial-exploration-15-pts","position":3},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"Read Data","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"type":"lvl3","url":"/proj02-nlp#read-data","position":4},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"Read Data","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"content":"\n\n# imports\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('seaborn-v0_8-dark') \n\n# read in SOTU.csv using pandas, name the variable `sou` for simplicity\n# the below cell is what the output should look like\nsou = pd.read_csv('data/SOTU.csv')\n\nsou\n\n","type":"content","url":"/proj02-nlp#read-data","position":5},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"Exploratory Data Analysis","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"type":"lvl3","url":"/proj02-nlp#exploratory-data-analysis","position":6},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"Exploratory Data Analysis","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"content":"Replicate the plots below using the hints specified. For each plot, provide some commentary describing the results/anything interesting you might see.\n\n","type":"content","url":"/proj02-nlp#exploratory-data-analysis","position":7},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Number of Speeches per President","lvl3":"Exploratory Data Analysis","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"type":"lvl4","url":"/proj02-nlp#number-of-speeches-per-president","position":8},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Number of Speeches per President","lvl3":"Exploratory Data Analysis","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"content":"\n\n# Hint - use value_counts() on the President column\n# Hint - sort in order of dataframe\ncounts = sou[\"President\"].value_counts(sort=False)\n\n# Plot \n# Hint - use the .plot() method for Pandas Series, make sure all presidents show up on x-axis\n\ncounts.plot(kind=\"bar\", figsize=(20, 10))\nplt.title(\"Number of Speeches per President\", fontsize=24)\nplt.xlabel(\"President\", fontsize=18)\nplt.ylabel(\"Count\", fontsize=18)\nplt.xticks(rotation=90, fontsize=13)\nplt.yticks(fontsize=13)\n\nplt.show()\n\n","type":"content","url":"/proj02-nlp#number-of-speeches-per-president","position":9},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Number of Speeches per Year","lvl3":"Exploratory Data Analysis","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"type":"lvl4","url":"/proj02-nlp#number-of-speeches-per-year","position":10},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Number of Speeches per Year","lvl3":"Exploratory Data Analysis","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"content":"\n\n# Hint - Use value counts and sort by years\ncounts_by_year = sou[\"Year\"].value_counts().sort_index()\n\nplt.figure(figsize=(5.75, 5))\ncounts_by_year.plot(kind=\"line\")\n\nplt.title(\"Number of State of the Union Speeches per Year\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Count\")\n\nplt.tight_layout()\nplt.show()\n\n","type":"content","url":"/proj02-nlp#number-of-speeches-per-year","position":11},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Word Count Distribution","lvl3":"Exploratory Data Analysis","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"type":"lvl4","url":"/proj02-nlp#word-count-distribution","position":12},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Word Count Distribution","lvl3":"Exploratory Data Analysis","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"content":"\n\n# Hint - try seaborn.histplot()\n\nplt.figure(figsize=(5.75, 5))\n\nsns.histplot(\n    sou[\"Word Count\"],\n    bins=18,\n    kde=False,\n    color=\"steelblue\"\n)\n\nplt.title(\"Distribution of State of the Union Speech\\nWord Counts\")\nplt.xlabel(\"Word Count\")\nplt.ylabel(\"Count\")\n\nplt.tight_layout()\nplt.show()\n\n","type":"content","url":"/proj02-nlp#word-count-distribution","position":13},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Word Count Distribution over Year","lvl3":"Exploratory Data Analysis","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"type":"lvl4","url":"/proj02-nlp#word-count-distribution-over-year","position":14},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Word Count Distribution over Year","lvl3":"Exploratory Data Analysis","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"content":"\n\n# Hint: try seaborn.rugplot()\nplt.figure(figsize=(6, 5))\n\nsns.scatterplot(\n    data=sou,\n    x=\"Word Count\",\n    y=\"Year\",\n    s=40\n)\n\nsns.rugplot(\n    data=sou,\n    x=\"Word Count\",\n    height=0.02,\n    alpha=0.6,\n)\n\nsns.rugplot(\n    data=sou,\n    y=\"Year\",\n    height=0.02,\n    alpha=1,\n    color=(59/255, 117/255, 174/255)\n)\n\nplt.title(\"Speech Year Versus Word Count\")\nplt.xlabel(\"Word Count\")\nplt.ylabel(\"Year\")\n\nplt.tight_layout()\nplt.show()\n\n","type":"content","url":"/proj02-nlp#word-count-distribution-over-year","position":15},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Word Count Distribution per President","lvl3":"Exploratory Data Analysis","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"type":"lvl4","url":"/proj02-nlp#word-count-distribution-per-president","position":16},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Word Count Distribution per President","lvl3":"Exploratory Data Analysis","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"content":"\n\n# Hint: use pandas groupby to get mean word count per president then sort by order\n\navg_words = sou.groupby(\"President\")[\"Word Count\"].mean()\n\navg_words = avg_words.loc[sou[\"President\"].unique()]\nplt.figure(figsize=(10, 6))\navg_words.plot(kind=\"bar\")\n\nplt.title(\"Average State of the Union Word Count\\nper President\")\nplt.xlabel(\"President\")\nplt.ylabel(\"Average Word Count\")\n\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.show()\n\n","type":"content","url":"/proj02-nlp#word-count-distribution-per-president","position":17},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl2","url":"/proj02-nlp#part-2-simple-text-processing-tokenization-lemmatization-word-frequency-vectorization-20-pts","position":18},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"Now we will start working on simple text processing using the SpaCy package and the same dataset as Part 1. The package should already be included in the environment.yml. However, we will also need to download en_core_web_sm, an English language text processing model. To do this, while having your sotu environment activated, run the following:python -m spacy download en_core_web_sm\n\nNow, you should be good to go!\n\nSome important definitions:\n\nToken: a single word or piece of a word\n\nLemma: the core component of a word, e.g., ‚Äúcomplete‚Äù is the lemma for ‚Äúcompleted‚Äù and ‚Äúcompletely‚Äù\n\nStop Word: a common word that does not add semantic value, such as ‚Äúa‚Äù, ‚Äúand‚Äù, ‚Äúthe‚Äù, etc.\n\nVectorization: representing a document as a vector where each index in the vector corresponds to a token or word and each entry is the count.\n\nIn this section, we will explore the most common tokens and lemmas throughout different slices of the speech data. We will also develop vectorization representations of the speeches.\n\nThe core steps are:\n\nProcess speeches using the SpaCy nlp module\n\nAnalyze Tokens vs Lemmas:\n\nCreate a list of all tokens across all speeches that are not stop words, punctuation, or spaces.\n\nCreate a second list of the lemmas for these same tokens.\n\nDisplay the top 25 for each of these and compare.\n\nAnalyze common word distributions over different years:\n\nCreate a function that takes the dataset and a year as an input and outputs the top n lemmas for that year‚Äôs speeches\n\nCompare the top 10 words for 2023 versus 2019\n\nDocument Vectorization:\n\nTrain a Term Frequency-Inverse Document Frequency (TF-IDF) vectorization model using your processed dataset and scikit learn\n\nOutput the feature vectors\n\nHelpful Resources:\n\nhttps://‚Äãrealpython‚Äã.com‚Äã/natural‚Äã-language‚Äã-processing‚Äã-spacy‚Äã-python/\n\nhttps://‚Äãwww‚Äã.statology‚Äã.org‚Äã/text‚Äã-preprocessing‚Äã-feature‚Äã-engineering‚Äã-spacy/\n\nhttps://‚Äãscikit‚Äã-learn‚Äã.org‚Äã/stable‚Äã/modules‚Äã/generated‚Äã/sklearn‚Äã.feature‚Äã_extraction‚Äã.text‚Äã.TfidfVectorizer‚Äã.html#\n\nhttps://‚Äãwww‚Äã.geeksforgeeks‚Äã.org‚Äã/nlp‚Äã/how‚Äã-to‚Äã-store‚Äã-a‚Äã-tfidfvectorizer‚Äã-for‚Äã-future‚Äã-use‚Äã-in‚Äã-scikit‚Äã-learn/\n\n","type":"content","url":"/proj02-nlp#part-2-simple-text-processing-tokenization-lemmatization-word-frequency-vectorization-20-pts","position":19},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"Processing Speeches with SpaCy","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl3","url":"/proj02-nlp#processing-speeches-with-spacy","position":20},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"Processing Speeches with SpaCy","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"Lets study just speeches from 2000 and onwards to begin with. So, be sure to subset your DataFrame to just these speeches before continuing!\n\nimport spacy\nfrom tqdm import tqdm\nfrom collections import Counter\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nsou_new = sou[sou['Year'] >= 2000]\nsou_new\n\nprocessed_speeches = []\n\nfor i in range(len(sou_new)):\n    speech_text = sou_new.loc[i, 'Text'] \n    processed = nlp(speech_text)    \n    processed_speeches.append(processed)\n\n\n\n\n","type":"content","url":"/proj02-nlp#processing-speeches-with-spacy","position":21},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"Analyze Tokens vs Lemmas","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl3","url":"/proj02-nlp#analyze-tokens-vs-lemmas","position":22},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"Analyze Tokens vs Lemmas","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"","type":"content","url":"/proj02-nlp#analyze-tokens-vs-lemmas","position":23},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Token List","lvl3":"Analyze Tokens vs Lemmas","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl4","url":"/proj02-nlp#token-list","position":24},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Token List","lvl3":"Analyze Tokens vs Lemmas","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"Create a list of tokens across all speeches that are not spaces, stopwords, or punctuation. Make each token lowercase as well. Hint: each element of the list we just created are themselves lists of tokens. Token objects have attributes is_stop, is_punct, and is_space.\n\ntokens = []\n\nfor speech in processed_speeches:\n    for token in speech:\n        if not token.is_punct and not token.is_space and not token.is_stop:\n            tokens.append(token.text.lower())\n\n\n\n# print top 20 tokens\ntoken_counts = Counter(tokens).most_common(20)\ntoken_counts\n\n\n","type":"content","url":"/proj02-nlp#token-list","position":25},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Lemma List","lvl3":"Analyze Tokens vs Lemmas","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl4","url":"/proj02-nlp#lemma-list","position":26},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Lemma List","lvl3":"Analyze Tokens vs Lemmas","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"Do the same as above, but for lemmas. Hint: recall lemmas are components of words. Each token should have an attribute to extract the lemma.\n\nlemmas = []\n\nfor word in tokens:\n    doc = nlp(word)  \n    for token in doc:\n        lemmas.append(token.lemma_.lower())\n\n\n# print top 20 lemmas\nlemma_counts = Counter(lemmas).most_common(20)\nlemma_counts\n\n","type":"content","url":"/proj02-nlp#lemma-list","position":27},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Token versus Lemma Comparison","lvl3":"Analyze Tokens vs Lemmas","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl4","url":"/proj02-nlp#token-versus-lemma-comparison","position":28},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Token versus Lemma Comparison","lvl3":"Analyze Tokens vs Lemmas","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"What do you notice about the top tokens versus the top lemmas?\nConsider two tokens - ‚Äúyear‚Äù and ‚Äúyears‚Äù - how do their counts compare to the lemma ‚Äúyear‚Äù?\nWhat about the lemma ‚Äúchild‚Äù?\n\nTokens ‚Äòyears‚Äô and ‚Äòyear‚Äô are common tokens, appearing in the 5th and 7th most common slots, respectively. The lemma ‚Äòyear‚Äô\nis the most popular lemma and its occurance is the sum of the occurences of its similar tokens. Although ‚Äòchild‚Äô does not appear in the top 20 tokens, it does appear as the 17th most common lemma. This could be becuase there are many tokens related to the lemma ‚Äòchild‚Äô such as ‚Äòchild,‚Äô ‚Äòchildren,‚Äô ‚Äòchildish,‚Äô and ‚Äòchildrens‚Äô.‚Äô This is twice the amount of tokens related to the same lemma of ‚Äòchild‚Äô as there is for the lemma of ‚Äòyear.‚Äô\n\n","type":"content","url":"/proj02-nlp#token-versus-lemma-comparison","position":29},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"Common Words","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl3","url":"/proj02-nlp#common-words","position":30},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"Common Words","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"","type":"content","url":"/proj02-nlp#common-words","position":31},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Common Words per Year Function","lvl3":"Common Words","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl4","url":"/proj02-nlp#common-words-per-year-function","position":32},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Common Words per Year Function","lvl3":"Common Words","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"Fill in the below function to obtain the n-most common words in speeches for a given year.\n\ninputs:\n\ndf raw unprocessed sou dataframe\n\nyear\n\nn\noutputs:\n\ntop n words for that years\n\nsteps:\n\nsubset the dataframe for the year of interest - note the years might not be in int type\n\nprocess the subsetted dataframe with spacy\n\nget the lemmas across all those speeches\n\ncount the top n lemmas\n\ndef get_most_common_words(df, year, n=25):\n    \"\"\"\n    Processes the SOTU speech for a given year and returns\n    the most common non-stopword/punctuation lemmas.\n\n    INPUTS\n    - raw unprocessed sou data frame\n    - year\n\n    OUTPUTS\n    - top n words for that year\n    \"\"\"\n\n    # Step 1: Subset df\n    new_df = df[df['Year'] == year]\n    \n    # Step 2: Process the text with spaCy\n    new_df = df[df['Year'] == year].reset_index(drop=True) \n    processed_speeches = []\n\n    for i in range(len(new_df)):\n        speech_text = new_df.loc[i, 'Text'] \n        processed = nlp(speech_text)    \n        processed_speeches.append(processed)\n    \n    # Step 3: Get lemmas\n    lemmas = []\n\n    for doc in processed_speeches:\n        for token in doc:\n            if not token.is_punct and not token.is_space and not token.is_stop:\n                lemmas.append(token.lemma_.lower())\n    \n    lemma_counts = Counter(lemmas).most_common(n)\n    \n    return lemma_counts\n\n# test it on 2024\nget_most_common_words(sou, 2024, n=20)\n\n","type":"content","url":"/proj02-nlp#common-words-per-year-function","position":33},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Compare 2023 to 2017","lvl3":"Common Words","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl4","url":"/proj02-nlp#compare-2023-to-2017","position":34},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Compare 2023 to 2017","lvl3":"Common Words","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"Run your function from the previous step to get the top 20 words for 2017 and 2023. Plot the words and their frequencies in a barchart and replicate the figure below.\n\nwords_2023 = get_most_common_words(sou, 2023, n=20)\nwords_2017 = get_most_common_words(sou, 2017, n=20)\n\nwords_2023\n\nwords_2017\n\n# Hint - put the words and counts into a pd Dataframe for better structure\n# and to make plotting easier\ndf_2017 = pd.DataFrame(words_2017, columns=['lemma', 'count'])\ndf_2023 = pd.DataFrame(words_2023, columns=['lemma', 'count'])\n\n# Hint - use seaborn, subplots, and rotate tick labels\n\nfig, axes = plt.subplots(2, 1, figsize=(12, 10)) \n# Plot 2017\nsns.barplot(\n    x = 'lemma',\n    y = 'count',\n    data = df_2017,\n    ax = axes[0],\n     color='#2C7BA1'\n    \n)\naxes[0].set_title('2017 State of the Union Most Frequent Words')\naxes[0].set_xlabel('Word')\naxes[0].set_ylabel('Count')\naxes[0].tick_params(axis='x', rotation=45) \n\n# Plot 2023\nsns.barplot(\n    x='lemma',\n    y='count',\n    data=df_2023,\n    ax=axes[1],\n    color='#2C7BA1'\n)\n\naxes[1].set_title('2023 State of the Union Most Frequent Words')\naxes[1].set_xlabel('Word')\naxes[1].set_ylabel('Count')\naxes[1].tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\n\n","type":"content","url":"/proj02-nlp#compare-2023-to-2017","position":35},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"TF-IDF Vectorization","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl3","url":"/proj02-nlp#tf-idf-vectorization","position":36},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"TF-IDF Vectorization","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"To use statsitical alorithms on documents, we need to transform them into vectors, where each element of the vector corresponds to a particular word in a document or corpus of documents. One common way is via TF-IDF embeddings. LLMs work similarly - they typically use transformer models to generate text embeddings before sending text through a deep neural network.\n\nHere we will fit a TF-IDF vectorizer, plot all the speeches on a 2-D grid using PCA and also using a heatmap, and examine TF-IDF scores for the top 10 most common words in the first speech. This is a good resource here: \n\nhttps://‚Äãmedium‚Äã.com‚Äã/GeoffreyGordonAshbrook‚Äã/vector‚Äã-visualization‚Äã-2d‚Äã-plot‚Äã-your‚Äã-tf‚Äã-idf‚Äã-with‚Äã-pca‚Äã-83fa9fccb1d\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\n\n","type":"content","url":"/proj02-nlp#tf-idf-vectorization","position":37},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Train the Vectorizer and Transform the Data","lvl3":"TF-IDF Vectorization","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl4","url":"/proj02-nlp#train-the-vectorizer-and-transform-the-data","position":38},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Train the Vectorizer and Transform the Data","lvl3":"TF-IDF Vectorization","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"\n\n# you may use this as input to fit the TF-IDF vectorizer\nraw_docs = sou[\"Text\"].to_list()\n\n# Hint - use fit_transform for vectorizer and PCA\n\nThe output of fit_transform() will be a matrix where each row corresponds to a speech, each column corresponds to a word in the corpus of speeches, and the value is the TF-IDF score which measures the importance of that word in that speech, relative to the rest of the speeches.\n\n","type":"content","url":"/proj02-nlp#train-the-vectorizer-and-transform-the-data","position":39},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Plot Speeches","lvl3":"TF-IDF Vectorization","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl4","url":"/proj02-nlp#plot-speeches","position":40},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Plot Speeches","lvl3":"TF-IDF Vectorization","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"First used PCA to generate the first chart\n\nSecond use seaborn heatmap with a log-scaled color axis to generate the second chart\n\n# Step 1: Set PCA to find first 2 principal components\n\n# Step 2: Create a new dataframe where each row is a speech, and each column is a projection onto\n# one of the two principal components\n\n# Plot Data Visualization (Matplotlib)\n\n# Hint - vectorized_docs is a sparse matrix whose rows are speeches and columns are tokens, with each\n# value being a TF-IDF score. Densify this array first, and then plot using seaborn.\n\n","type":"content","url":"/proj02-nlp#plot-speeches","position":41},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Get the TF-IDF value for certain words and documents","lvl3":"TF-IDF Vectorization","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl4","url":"/proj02-nlp#get-the-tf-idf-value-for-certain-words-and-documents","position":42},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Get the TF-IDF value for certain words and documents","lvl3":"TF-IDF Vectorization","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"\n\nword_list = ['year',\n 'america',\n 'people',\n 'american',\n 'work',\n 'new',\n 'job',\n 'country',\n 'americans',\n 'world'] # top ten most common words through whole corpus\n\nword_nums = ... # get each word's index number using the .vocabular_ attributed of vectorizer\n\nidf_score = ... # get their IDF score by using .idf_ at the indices from the previous step\n\ntf_idf = ... # get the tf_idf score for the first speech\n\npd.DataFrame({\"Word\": word_list, \"IDF Score\": idf_score, \"TF-IDF Score\": tf_idf})\n\n","type":"content","url":"/proj02-nlp#get-the-tf-idf-value-for-certain-words-and-documents","position":43},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl2":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)"},"type":"lvl2","url":"/proj02-nlp#part-3-advanced-text-processing-lda-and-bertopic-topic-modeling-20-pts","position":44},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl2":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)"},"content":"Resources:\n\nLDA:\n\nhttps://‚Äãmedium‚Äã.com‚Äã/sayahfares19‚Äã/text‚Äã-analysis‚Äã-topic‚Äã-modelling‚Äã-with‚Äã-spacy‚Äã-gensim‚Äã-4cd92ef06e06\n\nhttps://‚Äãwww‚Äã.kaggle‚Äã.com‚Äã/code‚Äã/faressayah‚Äã/text‚Äã-analysis‚Äã-topic‚Äã-modeling‚Äã-with‚Äã-spacy‚Äã-gensim‚Äã#üìö‚Äã-Topic‚Äã-Modeling (code for previous post)\n\nhttps://‚Äãtowardsdatascience‚Äã.com‚Äã/topic‚Äã-modelling‚Äã-in‚Äã-python‚Äã-with‚Äã-spacy‚Äã-and‚Äã-gensim‚Äã-dc8f7748bdbf/\n\nBERTopic:\n\nhttps://‚Äãmaartengr‚Äã.github‚Äã.io‚Äã/BERTopic‚Äã/getting‚Äã_started‚Äã/visualization‚Äã/visualize‚Äã_documents‚Äã.html‚Äã#visualize‚Äã-documents‚Äã-with‚Äã-plotly\n\nhttps://‚Äãmaartengr‚Äã.github‚Äã.io‚Äã/BERTopic‚Äã/getting‚Äã_started‚Äã/visualization‚Äã/visualize‚Äã_topics‚Äã.html\n\nimport spacy\nfrom tqdm import tqdm\nfrom collections import Counter\nimport pandas as pd\n\n# imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('seaborn-v0_8-dark') \n\nsou = pd.read_csv('data/SOTU.csv')\nnlp = spacy.load(\"en_core_web_sm\")\n\nfrom spacy import displacy\nfrom bertopic import BERTopic\nfrom gensim.corpora import Dictionary\nfrom gensim.models import LdaModel\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pyLDAvis\nimport pyLDAvis.gensim_models\n\n","type":"content","url":"/proj02-nlp#part-3-advanced-text-processing-lda-and-bertopic-topic-modeling-20-pts","position":45},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"LDA","lvl2":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)"},"type":"lvl3","url":"/proj02-nlp#lda","position":46},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"LDA","lvl2":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)"},"content":"Train an LDA model with 18 topics\n\nOutput the top 10 words for each topic.\n\nOutput the topic distribution for the first speech\n\nMake a visualization\n\nYou may use the next two cells to process the data.\n\ndef preprocess_text(text): \n    doc = nlp(text) \n    return [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and not token.is_space and len(token.lemma_) > 3]\n\n# Process all texts - note this takes ~ 5 minutes to run\nprocessed_docs = sou['Text'].apply(preprocess_text)\n\nTo train an LDA model, use the LdaModel function that we imported a couple of cells back. The last resource linked under the LDA section is especially useful for walking through the steps we have below. Note: one of the arguments to the LdaModel function is random_state which specifies the random seed for reproducibility. Please set yours to 42. Further, the last resource provided uses LdaMulticore which is essentially a parallelizable version of our function LdaModel. Use LdaModel instead, but the usage will be similar, except you can ignore the iterations and workers arguments...\n\n# Build dictionary from processed_docs, which is a list of tokens extracted from our speeches\n\nsou['tokens'] = processed_docs\n#Gensim Dictionary object maps each word to their unique ID:\ndictionary = Dictionary(sou['tokens'])\n#print(dictionary.token2id)\n#dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=1000)\n\n#create sparse vector (i, j) where i is dictionary id and j is number of occurences of that distinct word (?)\ncorpus = [dictionary.doc2bow(doc) for doc in sou['tokens']]\n\n# train LDA model with 18 topics\nlda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=18, random_state=42, passes=10)\n\n# print the top 10 words for each topic\nlda_model.print_topics(-1)\n\n# print the topic distribution for the first speech\nsou['Text'][0]\nlda_model[corpus][0]\n\nThe first speech is 99% belonging to topic 2!\n\n# make a visualization using pyLDAvis\npyLDAvis.enable_notebook()\n\nlda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\npyLDAvis.display(lda_display)\n\n\n#save to outputs\npyLDAvis.save_html(lda_display, 'outputs/lda_topics.html')\n\n","type":"content","url":"/proj02-nlp#lda","position":47},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"BERTopic","lvl2":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)"},"type":"lvl3","url":"/proj02-nlp#bertopic","position":48},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"BERTopic","lvl2":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)"},"content":"Train a BERTopic model with a min_topic_size of 3 Hint: use BERTopic to instantiate the model and specify min_topic_size in here. Actually fit the model using fit_transform, which docs passed into this.\n\nOutput the top 10 words for each topic.\n\nOutput the topic distribution for the first speech\n\nMake a visualization of the topics (see topic_model.visualize_topics())\n\nfrom bertopic import BERTopic\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndocs = sou['Text'].to_list()\n\n# train the model - this takes about 30 seconds\ntopic_model = BERTopic(min_topic_size=3)\ntopics, probs = topic_model.fit_transform(docs)\n\n\n# remove stop words from the topics (Hint: use CountVectorizer and then .update_topics on topic_model)\nvectorizer_model = CountVectorizer(stop_words=\"english\")\ntopic_model.update_topics(docs, vectorizer_model=vectorizer_model) \n\n# output the top 10 words for each topic - hint see get_topic_info\ntopic_model.get_topic_info()['Representation']\n\n# output the topic distribution for the first speech\ntopic_distr, _ = topic_model.approximate_distribution(docs)\nfirst_speech_viz = topic_model.visualize_distribution(topic_distr[1])\n\n#save first speech topic distribution to outputs\nfirst_speech_viz.write_html(\"outputs/BERTopic_first_speech_viz.html\")\nfirst_speech_viz\n\n# run this cell to visualize the topics\nviz_topics = topic_model.visualize_topics()\n\n#save topic visualizations to output\nviz_topics.write_html(\"outputs/BERTopic_topics_viz.html\")\nviz_topics\n\n","type":"content","url":"/proj02-nlp#bertopic","position":49},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl2":"Discussion and Reflections"},"type":"lvl2","url":"/proj02-nlp#discussion-and-reflections","position":50},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl2":"Discussion and Reflections"},"content":"\n\nThe topic distribution across the two dimensional PCA is notably different for the LDA (bag of words) and BERTopic (semantic similarity) approaches. The LDA distribution appears to have larger clusters on the right quadrant of the analyses, with significantly smaller clusters on the left quadrant. On the other hand, the BERTopic distributions land in each quadrant of the PCA grid, with more even distribution between each in terms of cluster size. This demonstrates how the two approaches use different attributes of the speeches and different algorithms to conclude topic summaries and distributions.\n\n","type":"content","url":"/proj02-nlp#discussion-and-reflections","position":51},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl2":"Part 4: Choose your own advecnture! (7 Points; Optional for Extra Credit)"},"type":"lvl2","url":"/proj02-nlp#part-4-choose-your-own-advecnture-7-points-optional-for-extra-credit","position":52},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl2":"Part 4: Choose your own advecnture! (7 Points; Optional for Extra Credit)"},"content":"This section is open ended and your chance to explare any advanced analysis. Please perform any additional analysis you find interesting! Suggested analyses (only do one max):\n\nTopic evolution over time - see \n\nhttps://‚Äãmaartengr‚Äã.github‚Äã.io‚Äã/BERTopic‚Äã/getting‚Äã_started‚Äã/topicsovertime‚Äã/topicsovertime‚Äã.html‚Äã#visualization\n\nWord frequency over time - does the frequency of certain words change over time\n\nSemantic similarity - investigate similarity within and between presidents or time periods. For example, similarity between one presidents speeches, e.g. are all of Biden‚Äôs speeches similar to each other? How similar are they to Trump‚Äôs speeches? Are speeches from the 2000s more similar to each other than speeches in the 1800s? Which two presidents have the most similar speeches? See \n\nhttps://‚Äãspacy‚Äã.io‚Äã/usage‚Äã/linguistic‚Äã-features‚Äã#vectors‚Äã-similarity\n\nNamed Entity Recognition - which entity types are most common in speeches? What are the most common words for each entity type - see \n\nhttps://‚Äãspacy‚Äã.io‚Äã/usage‚Äã/linguistic‚Äã-features‚Äã#named‚Äã-entities\n\nClassification - can you build a classifier to detect democratic versus republican state of the union speeches from 1980-2024 - see \n\nhttps://‚Äãscikit‚Äã-learn‚Äã.org‚Äã/stable‚Äã/auto‚Äã_examples‚Äã/text‚Äã/plot‚Äã_document‚Äã_classification‚Äã_20newsgroups‚Äã.html‚Äã#sphx‚Äã-glr‚Äã-auto‚Äã-examples‚Äã-text‚Äã-plot‚Äã-document‚Äã-classification‚Äã-20newsgroups‚Äã-py\n\n","type":"content","url":"/proj02-nlp#part-4-choose-your-own-advecnture-7-points-optional-for-extra-credit","position":53},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"Word frequency over time - does the frequency of certain words change over time","lvl2":"Part 4: Choose your own advecnture! (7 Points; Optional for Extra Credit)"},"type":"lvl3","url":"/proj02-nlp#word-frequency-over-time-does-the-frequency-of-certain-words-change-over-time","position":54},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"Word frequency over time - does the frequency of certain words change over time","lvl2":"Part 4: Choose your own advecnture! (7 Points; Optional for Extra Credit)"},"content":"\n\nimport re\n\n# Define words we think are important in this case\nwords = [\"freedom\", \"economy\", \"security\", \"health\"]\n\n\ndef count_word(text, word):\n    \"\"\"\n    RegEx Method to find variations of the word (case insensitive)\n    \"\"\"\n    pattern = r'\\b' + re.escape(word) + r'\\b'\n    return len(re.findall(pattern, text, flags=re.IGNORECASE))\n\n\n# Iterate through every word and get the raw count per year and then scale it\nfor w in words:\n    raw_col = f\"freq_{w}\"\n    rel_col = f\"rel_freq_{w}\"\n    sou[raw_col] = sou[\"Text\"].apply(lambda x: count_word(x, w))\n    sou[rel_col] = sou[raw_col] / sou[\"Word Count\"] * 1000\n\n\n# Sort the values every year\nsou_sorted = sou.sort_values(\"Year\").reset_index(drop=True)\n\n# Iterate through the words and fit a rolling average\nfor w in words:\n    col = f\"rel_freq_{w}\"\n    sou_sorted[col + \"_smooth\"] = (\n        sou_sorted[col].rolling(window=10, min_periods=3).mean()\n    )\n\nplt.figure(figsize=(12, 6))\n\n# Plot for the respective years\nfor w in words:\n    plt.plot(\n        sou_sorted[\"Year\"],\n        sou_sorted[f\"rel_freq_{w}_smooth\"],\n        label=w\n    )\n\nplt.title(\"Word Frequency Over Time (10-year rolling average)\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Relative Frequency (per 1000 words)\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n","type":"content","url":"/proj02-nlp#word-frequency-over-time-does-the-frequency-of-certain-words-change-over-time","position":55},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Freedom","lvl3":"Word frequency over time - does the frequency of certain words change over time","lvl2":"Part 4: Choose your own advecnture! (7 Points; Optional for Extra Credit)"},"type":"lvl4","url":"/proj02-nlp#freedom","position":56},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Freedom","lvl3":"Word frequency over time - does the frequency of certain words change over time","lvl2":"Part 4: Choose your own advecnture! (7 Points; Optional for Extra Credit)"},"content":"Mentions of freedom stay low throughout the 1800s, then gradually increase in the early 20th century. The term spikes sharply during the Cold War (1970s‚Äì1990s), when U.S. presidents frequently framed politics in ideological terms. Another noticeable peak appears in the early 2000s during the post-9/11 era, when ‚Äúfreedom‚Äù became central to national messaging. Through time, ‚Äúfreedom‚Äù becomes a major rhetorical theme primarily in the modern era.","type":"content","url":"/proj02-nlp#freedom","position":57},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Economy","lvl3":"Word frequency over time - does the frequency of certain words change over time","lvl2":"Part 4: Choose your own advecnture! (7 Points; Optional for Extra Credit)"},"type":"lvl4","url":"/proj02-nlp#economy","position":58},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Economy","lvl3":"Word frequency over time - does the frequency of certain words change over time","lvl2":"Part 4: Choose your own advecnture! (7 Points; Optional for Extra Credit)"},"content":"The word economy remains rarely used before 1900, but rises significantly during major economic crises. There are clear peaks during the Great Depression (1930s), post-WWII recovery, the stagflation era (1970s), and again around the Great Recession (2008‚Äì2010). The pattern reflects how presidents address economic instability directly in their State of the Union speeches.","type":"content","url":"/proj02-nlp#economy","position":59},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Security","lvl3":"Word frequency over time - does the frequency of certain words change over time","lvl2":"Part 4: Choose your own advecnture! (7 Points; Optional for Extra Credit)"},"type":"lvl4","url":"/proj02-nlp#security","position":60},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Security","lvl3":"Word frequency over time - does the frequency of certain words change over time","lvl2":"Part 4: Choose your own advecnture! (7 Points; Optional for Extra Credit)"},"content":"Security shows the strongest spikes of any word. Usage jumps dramatically during World War II, peaks again throughout the Cold War, and rises once more after 2001 in response to terrorism and national security concerns. This term closely tracks periods when the nation faces real or perceived threats, making it the most crisis-driven word in the group.","type":"content","url":"/proj02-nlp#security","position":61},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Health","lvl3":"Word frequency over time - does the frequency of certain words change over time","lvl2":"Part 4: Choose your own advecnture! (7 Points; Optional for Extra Credit)"},"type":"lvl4","url":"/proj02-nlp#health","position":62},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Health","lvl3":"Word frequency over time - does the frequency of certain words change over time","lvl2":"Part 4: Choose your own advecnture! (7 Points; Optional for Extra Credit)"},"content":"Mentions of health are almost nonexistent before the 20th century. Use rises steadily as the federal government becomes more involved in public health policy‚Äîespecially around the creation of Medicare and Medicaid (1960s), health reform debates in the 1990s, the Affordable Care Act period (2009‚Äì2015), and again around 2020 during the COVID-19 pandemic. ‚ÄúHealth‚Äù is the newest major theme in modern SOTU speeches.","type":"content","url":"/proj02-nlp#health","position":63},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Overall Summary","lvl3":"Word frequency over time - does the frequency of certain words change over time","lvl2":"Part 4: Choose your own advecnture! (7 Points; Optional for Extra Credit)"},"type":"lvl4","url":"/proj02-nlp#overall-summary","position":64},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Overall Summary","lvl3":"Word frequency over time - does the frequency of certain words change over time","lvl2":"Part 4: Choose your own advecnture! (7 Points; Optional for Extra Credit)"},"content":"Across all four words, usage stays low before 1900 and rises sharply in the modern era as speeches become more policy-focused. The trends reveal how presidential priorities evolve: ‚Äúsecurity‚Äù peaks during wars and threats, ‚Äúeconomy‚Äù during financial crises, ‚Äúfreedom‚Äù during ideological conflicts, and ‚Äúhealth‚Äù during healthcare policy shifts and pandemics. Together, these patterns highlight how State of the Union language reflects broad historical changes in national concerns.","type":"content","url":"/proj02-nlp#overall-summary","position":65},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Langauge Processing"},"type":"lvl1","url":"/project-description","position":0},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Langauge Processing"},"content":"Statistics 159/259, Fall 2025\n\nDue 11/21/2025, 11:59PM PT\n\nProf. F. P√©rez, GSI J. Butler, and GSI S. Andrade, Department of Statistics, UC Berkeley.\n\nYour score for this assignment is out of 70 points (with a potential of 7 points extra credit). This project as a whole accounts for 20% of the final course grade.\n\nAssignment type: group project assignment\n\nThe assignment content is located in proj02-nlp.ipynb.\n\nLearning Objectives:\n\nImplement Modern NLP Pipelines:\n\nLoad and Prepare Data: Ingest and clean a real-world text dataset using pandas.\n\nProcess Text Efficiently: Implement a text pre-processing pipeline using spaCy, a production-grade NLP library.\n\nUnderstand Core Concepts: Understand the diffence between a token, a lemma, a stop word, and punctuation, and understand why lemmatization was preferred for text analysis prior to LLMs.\n\nConduct Core Text Analysis:\n\nExtract Linguistic Features: Use spaCy to efficiently extract lemmas, parts-of-speech, and named entities from raw text.\n\nPerform Frequency Analysis: Use spaCy outputs to compare the most frequent words in different documents.\n\nAnalyze Text Over Time: Compare the language of modern presidents to historical ones, drawing initial conclusions about how political communication has evolved.\n\nBuild and Compare Topic Models:\n\nVectorize Text: Understand and apply the TF-IDF (Term Frequency-Inverse Document Frequency) vectorization method to prepare text for machine learning.\n\nImplement Traditional Topic Modeling: Build, train, and interpret a Latent Dirichlet Allocation (LDA) topic model using gensim to find topics based on word co-occurrence.\n\nImplement Modern Topic Modeling: Use BERTopic to leverage transformer embeddings (BERT), clustering, and semantic similarity to discover conceptually coherent topics.\n\nCritically Evaluate Models: Compare the topics generated by LDA and BERTopic, analyzing the trade-offs and advantages of each approach (i.e., ‚Äúbag-of-words‚Äù vs. ‚Äúsemantic similarity‚Äù).\n\nReproducibility and Collaboration:\n\nWork on more open-ended questions that involve self learning, utilization of external resources, and exploration\n\nCollaboration on Complex Projects: Gain experience coordinating on teams to complete tasks with dependencies on each other\n\nReproducibility: Gain experience describing your work in accessibile formats and making it reproducible.\n\nDeliverables: For this assignment, you will have a single GitHub repository for your group. Your repository should contain the following:\n\nOne notebook for each of Parts 1, 2, and 3 in the assignment notebook proj02-nlp.ipynb (and Part 4 if you wish to do it) that includes code to create the plots and simulations the question asks for, along with any written responses, commentary, discussion, and documentation where applicable. Please name each notebook using the convention nlp-PXX.ipynb, with XX corresponding to the number of the part. Please remember to use markdown headings for each section/subsection so the entire notebook document is readable. All figures should be both rendered in the notebook and saved in a separate folder called outputs. Please also make sure to structure your notebooks as if you were conducting this as a clean and nicely presented data analysis report. Do not include our prompts/problem statements in the final report notebooks.\n\nComplete the contribution statement in contribution_statement.md, briefly and qualitatively detailing each group member‚Äôs contributions to the assignment.\n\nAn ai_documentation.txt file where your group will put any prompts and output from AI companions.\n\nA README describing the aims of the project, with a Binder link to the repo so your notebooks can be run.\n\nA MyST Site for your project, deployed to GitHub Pages. Each notebook should have its own tab on the website.\n\nThere are two core components to this homework:\n\nWorking with text data building from simple to complex methods (Parts 1-4).\n\nBuilding a reproducible and structured repository (Part 5).\n\nWe have provided you with detailed questions with hints and will cover similar material in the lab.\n\nThe total grade for this homework is divided between:\n\n[15 Points] Part 1: Data loading and Initial Exploration.\n\n[20 Points] Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization.\n\n[20 Points] Part 3:  Advanced Text Processing - LDA and BERTopic Topic Modeling\n\n[Extra Credit: 7 Points] Part 4: Choose your own adventure!\n\n[15 Points] Part 5: Project Structure\n\nParts 1-4 will each be graded from their respective notebooks. Part 5 will be graded by assessing whether all deliverables mentioned above are present, as well as your team‚Äôs use of git (e.g. effective commit messages, use of .gitignore for handling extraneous/unnecessary files, repo organization, etc.)\n\nWhen running your analyses and making plots, it may be useful to save intermediate steps, such as vectorized data files and then read them in order to make just the plots.\n\nNote: The parts successively build on each other, so you will need to strategize how to divde and conquer in your groups. For example, some topic models from part 3 require the vectorization used in part 2.\n\nAcknowledgment: This homework assignment has been revamped from the previous Spring 2017 assignment created by Prof Fernando Perez and Eli Ben-Michael, adapted from Prof Deb Nolan previous version of this project. The data from the project is from a Kaggle dataset here: \n\nhttps://‚Äãwww‚Äã.kaggle‚Äã.com‚Äã/datasets‚Äã/nicholasheyerdahl‚Äã/state‚Äã-of‚Äã-the‚Äã-union‚Äã-address‚Äã-texts‚Äã-1790‚Äã-2024‚Äã?resource‚Äã=‚Äãdownload","type":"content","url":"/project-description","position":1}]}