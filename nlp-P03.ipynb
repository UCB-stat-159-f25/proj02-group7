{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fa36b17-d0bd-405c-9332-fda4c1cfe2f9",
   "metadata": {},
   "source": [
    "# Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)\n",
    "\n",
    "### **References Used:**\n",
    "- LDA:\n",
    "    - https://medium.com/sayahfares19/text-analysis-topic-modelling-with-spacy-gensim-4cd92ef06e06 \n",
    "    - https://www.kaggle.com/code/faressayah/text-analysis-topic-modeling-with-spacy-gensim#%F0%9F%93%9A-Topic-Modeling (code for previous post)\n",
    "    - https://towardsdatascience.com/topic-modelling-in-python-with-spacy-and-gensim-dc8f7748bdbf/ \n",
    "- BERTopic:\n",
    "    - https://maartengr.github.io/BERTopic/getting_started/visualization/visualize_documents.html#visualize-documents-with-plotly \n",
    "    - https://maartengr.github.io/BERTopic/getting_started/visualization/visualize_topics.html\n",
    "    - https://maartengr.github.io/BERTopic/getting_started/distribution/distribution.html#example\n",
    "    - https://maartengr.github.io/BERTopic/getting_started/topicrepresentation/topicrepresentation.html#update-topic-representation-after-training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bffb1462-5e55-496e-8b13-1bb95556005c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /srv/conda/envs/notebook/lib/python3.12/site-packages (3.8.11)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (0.20.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.12.3)\n",
      "Requirement already satisfied: jinja2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (70.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (24.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /srv/conda/envs/notebook/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (1.17.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "import spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcf2c0f7-30e8-44b0-91f3-e44911cec07b",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m plt.style.use(\u001b[33m'\u001b[39m\u001b[33mseaborn-v0_8-dark\u001b[39m\u001b[33m'\u001b[39m) \n\u001b[32m     11\u001b[39m sou = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mdata/SOTU.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m nlp = \u001b[43mspacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43men_core_web_sm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/sotu/lib/python3.13/site-packages/spacy/__init__.py:52\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     29\u001b[39m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[32m     30\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] = util.SimpleFrozenDict(),\n\u001b[32m     36\u001b[39m ) -> Language:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m \u001b[33;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m \u001b[33;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/sotu/lib/python3.13/site-packages/spacy/util.py:484\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[32m    483\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E941.format(name=name, full=OLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E050.format(name=name))\n",
      "\u001b[31mOSError\u001b[39m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-v0_8-dark') \n",
    "\n",
    "sou = pd.read_csv('data/SOTU.csv')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04d7c3b-8c92-4b9b-9933-00761331c10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "from bertopic import BERTopic\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075de05e-3dcb-4d11-aa81-ab324a1726d7",
   "metadata": {},
   "source": [
    "### LDA\n",
    "\n",
    "To create and analyze potential topics associated with the speeches, we will first use the LDA method and package.\n",
    "- Train an LDA model with 18 topics\n",
    "- Output the top 10 words for each topic. \n",
    "- Output the topic distribution for the first speech\n",
    "- Make a visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360e6b3e-0bb5-42da-b13d-3f620c75ede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text): \n",
    "    doc = nlp(text) \n",
    "    return [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and not token.is_space and len(token.lemma_) > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f14be97-3424-4b65-90e1-766033d7d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all texts - note this takes ~ 5 minutes to run\n",
    "processed_docs = sou['Text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d8eae2-4ff2-4875-a50c-3d44b077f044",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1878b0f-b822-4f81-ae62-0281bc70c2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dictionary from processed_docs, which is a list of tokens extracted from our speeches\n",
    "sou['tokens'] = processed_docs\n",
    "#Gensim Dictionary object maps each word to their unique ID:\n",
    "dictionary = Dictionary(sou['tokens'])\n",
    "#print(dictionary.token2id)\n",
    "#dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=1000)\n",
    "\n",
    "#create sparse vector (i, j) where i is dictionary id and j is number of occurences of that distinct word (?)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in sou['tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9d7821-301b-4bcb-ab54-52b93b16e387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train LDA model with 18 topics\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=18, random_state=42, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b36255d-b05a-480d-9d8b-cfad31c439ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the top 10 words for each topic\n",
    "lda_model.print_topics(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451be912-5f2d-43f0-84fb-224aff203819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the topic distribution for the first speech\n",
    "sou['Text'][0]\n",
    "lda_model[corpus][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b611b4dd-d3d4-4141-9f23-f4e4d1cb04ca",
   "metadata": {},
   "source": [
    "The first speech is 99% belonging to topic 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d417c0-f0a4-46aa-abdf-af0ac234bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a visualization using pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.display(lda_display)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323b344c-79ce-468e-aae8-f6050b8b0e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to outputs\n",
    "pyLDAvis.save_html(lda_display, 'outputs/lda_topics.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b12991f-55c9-42a0-9c97-972e9341bd5b",
   "metadata": {},
   "source": [
    "### BERTopic\n",
    "We will also conduct topic analysis using the BERTopic method and package. We will run through the following steps:\n",
    "- Train a BERTopic model with a `min_topic_size` of 3 \n",
    "- Output the top 10 words for each topic. \n",
    "- Output the topic distribution for the first speech\n",
    "- Make a visualization of the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8249d87-9abf-4861-bd2a-4655a7fdef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3265c2-9cb1-4606-bebe-5fa4cfd84010",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = sou['Text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266c5358-b1a9-49b4-b80c-350d2202d84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model - this takes about 30 seconds\n",
    "topic_model = BERTopic(min_topic_size=3)\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "\n",
    "# remove stop words from the topics (Hint: use CountVectorizer and then .update_topics on topic_model)\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "topic_model.update_topics(docs, vectorizer_model=vectorizer_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212822ca-056d-4894-960e-8474ee2c2d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the top 10 words for each topic - hint see get_topic_info\n",
    "topic_model.get_topic_info()['Representation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f141af7-3af4-4c56-895d-45ab1ba5113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the topic distribution for the first speech\n",
    "topic_distr, _ = topic_model.approximate_distribution(docs)\n",
    "first_speech_viz = topic_model.visualize_distribution(topic_distr[1])\n",
    "\n",
    "#save first speech topic distribution to outputs\n",
    "first_speech_viz.write_html(\"outputs/BERTopic_first_speech_viz.html\")\n",
    "first_speech_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3453ba00-f377-43c1-9cad-8badd2f9285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to visualize the topics\n",
    "viz_topics = topic_model.visualize_topics()\n",
    "\n",
    "#save topic visualizations to output\n",
    "viz_topics.write_html(\"outputs/BERTopic_topics_viz.html\")\n",
    "viz_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f6c93f-60de-4b9e-b27b-87d277117324",
   "metadata": {},
   "source": [
    "## Discussion and Reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96f478b-779f-4153-8991-47668e7d3378",
   "metadata": {},
   "source": [
    "The topic distribution across the two dimensional PCA is notably different for the LDA (bag of words) and BERTopic (semantic similarity) approaches. The LDA distribution appears to have larger clusters on the right quadrant of the analyses, with significantly smaller clusters on the left quadrant. On the other hand, the BERTopic distributions land in each quadrant of the PCA grid, with more even distribution between each in terms of cluster size. This demonstrates how the two approaches use different attributes of the speeches and different algorithms to conclude topic summaries and distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c900ce8b-daf3-4b0f-bdfc-97b06a78edfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sotu",
   "language": "python",
   "name": "sotu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
