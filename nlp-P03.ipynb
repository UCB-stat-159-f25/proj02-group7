{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fa36b17-d0bd-405c-9332-fda4c1cfe2f9",
   "metadata": {},
   "source": [
    "# Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)\n",
    "\n",
    "### **References Used:**\n",
    "- LDA:\n",
    "    - https://medium.com/sayahfares19/text-analysis-topic-modelling-with-spacy-gensim-4cd92ef06e06 \n",
    "    - https://www.kaggle.com/code/faressayah/text-analysis-topic-modeling-with-spacy-gensim#%F0%9F%93%9A-Topic-Modeling (code for previous post)\n",
    "    - https://towardsdatascience.com/topic-modelling-in-python-with-spacy-and-gensim-dc8f7748bdbf/ \n",
    "- BERTopic:\n",
    "    - https://maartengr.github.io/BERTopic/getting_started/visualization/visualize_documents.html#visualize-documents-with-plotly \n",
    "    - https://maartengr.github.io/BERTopic/getting_started/visualization/visualize_topics.html\n",
    "    - https://maartengr.github.io/BERTopic/getting_started/distribution/distribution.html#example\n",
    "    - https://maartengr.github.io/BERTopic/getting_started/topicrepresentation/topicrepresentation.html#update-topic-representation-after-training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bffb1462-5e55-496e-8b13-1bb95556005c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /srv/conda/envs/notebook/lib/python3.12/site-packages (3.8.11)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (0.20.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.12.3)\n",
      "Requirement already satisfied: jinja2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (70.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (24.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /srv/conda/envs/notebook/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (1.17.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "import spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcf2c0f7-30e8-44b0-91f3-e44911cec07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-v0_8-dark') \n",
    "\n",
    "sou = pd.read_csv('data/SOTU.csv')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c04d7c3b-8c92-4b9b-9933-00761331c10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/envs/sotu/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "from bertopic import BERTopic\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075de05e-3dcb-4d11-aa81-ab324a1726d7",
   "metadata": {},
   "source": [
    "### LDA\n",
    "\n",
    "To create and analyze potential topics associated with the speeches, we will first use the LDA method and package.\n",
    "- Train an LDA model with 18 topics\n",
    "- Output the top 10 words for each topic. \n",
    "- Output the topic distribution for the first speech\n",
    "- Make a visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "360e6b3e-0bb5-42da-b13d-3f620c75ede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text): \n",
    "    doc = nlp(text) \n",
    "    return [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and not token.is_space and len(token.lemma_) > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f14be97-3424-4b65-90e1-766033d7d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all texts - note this takes ~ 5 minutes to run\n",
    "processed_docs = sou['Text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d8eae2-4ff2-4875-a50c-3d44b077f044",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1878b0f-b822-4f81-ae62-0281bc70c2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dictionary from processed_docs, which is a list of tokens extracted from our speeches\n",
    "sou['tokens'] = processed_docs\n",
    "#Gensim Dictionary object maps each word to their unique ID:\n",
    "dictionary = Dictionary(sou['tokens'])\n",
    "#print(dictionary.token2id)\n",
    "#dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=1000)\n",
    "\n",
    "#create sparse vector (i, j) where i is dictionary id and j is number of occurences of that distinct word (?)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in sou['tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9d7821-301b-4bcb-ab54-52b93b16e387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train LDA model with 18 topics\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=18, random_state=42, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b36255d-b05a-480d-9d8b-cfad31c439ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the top 10 words for each topic\n",
    "lda_model.print_topics(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451be912-5f2d-43f0-84fb-224aff203819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the topic distribution for the first speech\n",
    "sou['Text'][0]\n",
    "lda_model[corpus][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b611b4dd-d3d4-4141-9f23-f4e4d1cb04ca",
   "metadata": {},
   "source": [
    "The first speech is 99% belonging to topic 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d417c0-f0a4-46aa-abdf-af0ac234bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a visualization using pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.display(lda_display)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323b344c-79ce-468e-aae8-f6050b8b0e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to outputs\n",
    "pyLDAvis.save_html(lda_display, 'outputs/lda_topics.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b12991f-55c9-42a0-9c97-972e9341bd5b",
   "metadata": {},
   "source": [
    "### BERTopic\n",
    "We will also conduct topic analysis using the BERTopic method and package. We will run through the following steps:\n",
    "- Train a BERTopic model with a `min_topic_size` of 3 \n",
    "- Output the top 10 words for each topic. \n",
    "- Output the topic distribution for the first speech\n",
    "- Make a visualization of the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8249d87-9abf-4861-bd2a-4655a7fdef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3265c2-9cb1-4606-bebe-5fa4cfd84010",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = sou['Text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266c5358-b1a9-49b4-b80c-350d2202d84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model - this takes about 30 seconds\n",
    "topic_model = BERTopic(min_topic_size=3)\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "\n",
    "# remove stop words from the topics (Hint: use CountVectorizer and then .update_topics on topic_model)\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "topic_model.update_topics(docs, vectorizer_model=vectorizer_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212822ca-056d-4894-960e-8474ee2c2d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the top 10 words for each topic - hint see get_topic_info\n",
    "topic_model.get_topic_info()['Representation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f141af7-3af4-4c56-895d-45ab1ba5113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the topic distribution for the first speech\n",
    "topic_distr, _ = topic_model.approximate_distribution(docs)\n",
    "first_speech_viz = topic_model.visualize_distribution(topic_distr[1])\n",
    "\n",
    "#save first speech topic distribution to outputs\n",
    "first_speech_viz.write_html(\"outputs/BERTopic_first_speech_viz.html\")\n",
    "first_speech_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3453ba00-f377-43c1-9cad-8badd2f9285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to visualize the topics\n",
    "viz_topics = topic_model.visualize_topics()\n",
    "\n",
    "#save topic visualizations to output\n",
    "viz_topics.write_html(\"outputs/BERTopic_topics_viz.html\")\n",
    "viz_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f6c93f-60de-4b9e-b27b-87d277117324",
   "metadata": {},
   "source": [
    "## Discussion and Reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96f478b-779f-4153-8991-47668e7d3378",
   "metadata": {},
   "source": [
    "The topic distribution across the two dimensional PCA is notably different for the LDA (bag of words) and BERTopic (semantic similarity) approaches. The LDA distribution appears to have larger clusters on the right quadrant of the analyses, with significantly smaller clusters on the left quadrant. On the other hand, the BERTopic distributions land in each quadrant of the PCA grid, with more even distribution between each in terms of cluster size. This demonstrates how the two approaches use different attributes of the speeches and different algorithms to conclude topic summaries and distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c900ce8b-daf3-4b0f-bdfc-97b06a78edfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sotu",
   "language": "python",
   "name": "sotu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
